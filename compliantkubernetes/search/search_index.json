{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Elastisys Compliant Kubernetes","text":"<p> Like vanilla Kubernetes - but with security and observability built in. </p> <p> Elastisys Compliant Kubernetes is an open source, Certified Kubernetes distribution designed according to the ISO27001 controls: providing you with security tooling and observability from day one. </p> <p></p>"},{"location":"#components-of-elastisys-compliant-kubernetes","title":"Components of Elastisys Compliant Kubernetes","text":""},{"location":"#mapping-of-iso-27001-controls-to-implementation","title":"Mapping of ISO 27001 Controls to Implementation","text":""},{"location":"#how-do-i-get-started","title":"How do I get started?","text":"<p>Getting started guides:</p> <ul> <li>for application developers</li> <li>for Kubernetes administrators</li> <li>for CISOs (Chief Information Security Officers)</li> </ul>"},{"location":"#would-you-like-to-contribute","title":"Would you like to contribute?","text":"<p>We want to build the next generation of cloud native technology where data security and privacy is the default setting.</p> <p>Join us on our mission as a contributor? Go to the guide for contributors.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Below we present the architecture of Compliant Kubernetes, using the C4 model.</p> <p>For the nitty-gritty details, see Architectural Decision Records.</p>","tags":["MSBFS 2020:7 2 kap. 4 \u00a7"]},{"location":"architecture/#level-1-system-context","title":"Level 1: System Context","text":"<p>Let us start with the system context.</p> <p></p> <p>Compliance imposes restrictions on all levels of the tech stack. Your compliance focus should mostly lie on your application. Compliant Kubernetes ensures that the platform hosting your application is compliant. Finally, you need the whole software stack on a hardware that is managed in a compliant way, either via an ISO 27001-certified cloud provider or using on-prem hardware.</p>","tags":["MSBFS 2020:7 2 kap. 4 \u00a7"]},{"location":"architecture/#level-2-clusters","title":"Level 2: Clusters","text":"<p>Most regulations require logging to a tamper-proof environment. This is usually interpreted as an attacker gaining access to your application should not be able to delete logs showing their attack and the harm caused by their attack.</p> <p>To achieve this, Compliant Kubernetes is implemented as two Kubernetes clusters</p> <ul> <li>A workload cluster, which hosts your application, and</li> <li>A service cluster, which hosts services for monitoring, logging and vulnerability management.</li> </ul> <p></p>","tags":["MSBFS 2020:7 2 kap. 4 \u00a7"]},{"location":"architecture/#level-3-individual-components","title":"Level 3: Individual Components","text":"<p>Click on the diagram below to see the nuts-and-bolts of Compliant Kubernetes.</p> <p></p> <p>Note</p> <p>Due to technical limitations, some compliance-related components still need to run in the workload cluster. These are visible when inspecting the workload cluster, for example, via the Kubernetes API. Currently, these components are:</p> <ul> <li>Falco, for intrusion detection;</li> <li>Prometheus, for collecting metrics;</li> <li>Fluentd, for collecting logs;</li> <li>OpenPolicyAgent, for enforcing Kubernetes API policies.</li> </ul> <p>Note that, the logs, metrics and alerts produced by these components are immediately pushed into the tamper-proof logging environment, hence this technical limitation does not weaken compliance.</p>","tags":["MSBFS 2020:7 2 kap. 4 \u00a7"]},{"location":"architecture/#level-3-authentication","title":"Level 3: Authentication","text":"<p>Click on the diagram below to see the nuts-and-bolts of Compliant Kubernetes authentication.</p> <p></p>","tags":["MSBFS 2020:7 2 kap. 4 \u00a7"]},{"location":"architecture/#level-3-backup","title":"Level 3: Backup","text":"<p>Click on the diagram below to see the nuts-and-bolts of Compliant Kubernetes backup.</p> <p></p>","tags":["MSBFS 2020:7 2 kap. 4 \u00a7"]},{"location":"architecture/#level-3-metrics-and-metrics-based-alerting","title":"Level 3: Metrics and Metrics-based Alerting","text":"","tags":["MSBFS 2020:7 2 kap. 4 \u00a7"]},{"location":"architecture/#level-3-logs-and-log-based-alerting","title":"Level 3: Logs and Log-based Alerting","text":"","tags":["MSBFS 2020:7 2 kap. 4 \u00a7"]},{"location":"architecture/#level-3-access-control","title":"Level 3: Access Control","text":"","tags":["MSBFS 2020:7 2 kap. 4 \u00a7"]},{"location":"compliance/","title":"Compliance Basics","text":"<p>Compliance will vary widely depending on:</p> <ul> <li>Jurisdiction (e.g., US vs. EU);</li> <li>Industry regulation (e.g., MedTech vs. FinTech);</li> <li>Company policies (e.g., log retention based on cost-risk analysis).</li> </ul> <p>The following is meant to offer an overview of compliance focusing on information security, and how Compliant Kubernetes reduces compliance burden.</p> <p>Click on the revelant blue text to find out more:</p> <p></p>"},{"location":"compliance/#compliance-the-societal-perspective","title":"Compliance: The Societal Perspective","text":"<p>Organizations in certain sectors, such as BioTech, FinTech, MedTech, and those processing personal data, need public trust to operate. Such companies are allowed to handle sensitive data, create and destroy money, etc., in exchange for being compliant with certain regulations \u2014 in devtalk put, sticking to some rules set by regulators. For example:</p> <ul> <li>Any organization dealing with personal data is scrutinized by the Swedish Data Protection Authority (Datainspektionen) and needs to comply with GDPR.</li> <li>Any organization handling patient data needs to comply with Patientdatalagen (PDL) in Sweden.</li> </ul> <p>Such regulation is not only aspirational, but is actually checked as often as yearly by an external auditor. If an organization is found to be non-compliant it may pay heavy fines or even lose its license to operate.</p>"},{"location":"compliance/#compliance-the-engineering-perspective","title":"Compliance: The Engineering Perspective","text":"<p>Translating legalese into code involves several steps. First a Compliance Officer will identify what regulations apply to the company. Based on those regulations, they will draft policies to clarify how the company should operate \u2014 i.e., run its daily business \u2014 in the most efficient manner while complying with regulations. To ensure the policies do not have gaps, are non-overlapping and consistent, they will generally follow an information security standard, such as ISO/IEC 27001. Such information security standards list a set of controls, i.e., \"points\" in the organization where a process and a check needs to be put in place.</p> <p>The resulting policies need to be interpreted and implemented by each department. Some of these can be supported by, or entirely implemented by, technology. Compliant Kubernetes includes software to do just that, and thus, Compliant Kubernetes addresses the needs of the infrastructure team.</p> <p>In essence, Compliant Kubernetes are carefully configured Kubernetes clusters together with other open-source components. They reduce compliance burden by allowing an organization to focus on making their processes and application compliant, knowing that the underlying platform is compliant.</p> <p>As far as getting certification, a key aspect is the ability to point to documentation that clearly states that your tech stack fulfils all stipulated requirements. By relying on Compliant Kubernetes, the majority of this work is already done for you.</p>"},{"location":"release-notes/","title":"Release Notes","text":""},{"location":"release-notes/#compliant-kubernetes","title":"Compliant Kubernetes","text":"<ul> <li>v0.29.0 - 2023-03-16</li> <li>v0.28.1 - 2023-03-02</li> <li>v0.28.0 - 2023-01-30</li> <li>v0.27.0 - 2022-11-17</li> <li>v0.26.0 - 2022-09-19</li> <li>v0.25.0 - 2022-08-25</li> <li>v0.24.1 - 2022-08-01</li> <li>v0.24.0 - 2022-07-25</li> <li>v0.23.0 - 2022-07-06</li> <li>v0.22.0 - 2022-06-01</li> <li>v0.21.0 - 2022-05-04</li> <li>v0.20.0 - 2022-03-21</li> <li>v0.19.1 - 2022-03-01</li> <li>v0.19.0 - 2022-02-01</li> <li>v0.18.2 - 2021-12-16</li> <li>v0.17.2 - 2021-12-16</li> <li>v0.18.1 - 2021-12-08</li> <li>v0.17.1 - 2021-12-08</li> <li>v0.18.0 - 2021-11-04</li> <li>v0.17.0 - 2021-06-29</li> <li>v0.16.0 - 2021-05-27</li> </ul> <p>Note</p> <p>For a more detailed look check out the full changelog.</p>"},{"location":"release-notes/#v0290","title":"v0.29.0","text":"<p>Released 2023-03-16</p>"},{"location":"release-notes/#added","title":"Added","text":"<ul> <li>Static users can now be added in OpenSearch.</li> </ul>"},{"location":"release-notes/#changed","title":"Changed","text":"<ul> <li>The Fluentd deplyoment has changed considerably and users must ensure that their custom filters continue to work as expected.</li> </ul>"},{"location":"release-notes/#updated","title":"Updated","text":"<ul> <li>Cert-manager updated to <code>v1.11.0</code></li> <li>The containers in pods created by cert-manager have been renamed to better reflect what they do. This can be breaking for automation that relies on these names being static.</li> <li>The cert-manager Gateway API integration now uses the v1beta1 API version. ExperimentalGatewayAPISupport alpha feature users must upgrade to v1beta of Gateway API.</li> </ul>"},{"location":"release-notes/#v0281","title":"v0.28.1","text":"<p>Released 2023-03-02</p>"},{"location":"release-notes/#added_1","title":"Added","text":"<ul> <li>Added falco rules to ignore redis operator related alerts.</li> </ul>"},{"location":"release-notes/#v0280","title":"v0.28.0","text":"<p>Released 2023-01-30</p>"},{"location":"release-notes/#changed_1","title":"Changed","text":"<ul> <li>Updated Rook alerts to <code>v1.10.5</code>.</li> <li>Nginx ingress controller service can now have multiple annotations instead of just one.</li> <li>Synced all grafana dashboards to use the default organization timezone.</li> <li>Several default resource requests and limits have changed for the included services.</li> </ul>"},{"location":"release-notes/#fixed","title":"Fixed","text":"<ul> <li>Use FQDN for services connecting from the workload cluster to the service cluster to prevent resolve timeouts.</li> <li>Fixed <code>KubeletDown</code> alert rule not alerting if a kubelet was missing.</li> <li>Added permissions to the <code>alerting_full_access</code> role in Opensearch to be able to view notification channels.</li> <li>Added <code>fluent-plugin-record-modifier</code> to the fluentd image to prevent mapping errors.</li> <li>Various fixes to network policies.</li> </ul>"},{"location":"release-notes/#added_2","title":"Added","text":"<ul> <li>Improved security posture by adding network policies for some of the networking and storage components.</li> <li>Added alert for less kubelets than nodes in the cluster.</li> <li>Added alert for object limits in buckets.</li> </ul>"},{"location":"release-notes/#v0270","title":"v0.27.0","text":"<p>Released 2022-11-17</p>"},{"location":"release-notes/#updated_1","title":"Updated","text":"<ul> <li>Updated Dex helm chart to <code>v0.12.0</code>, which also upgrades Dex to <code>v2.35.1</code>.</li> <li>Updated Falco helm chart to <code>2.2.0</code>, which also upgrades Falco to <code>0.33.0</code> and Falco Sidekick to <code>2.26.0</code>.</li> <li>Updated Falco Exporter helm chart to <code>0.9.0</code>, which also upgrades Falco Exporter to <code>0.8.0</code>.</li> <li>Updated Velero helm chart to <code>v2.31.8</code>, which also upgrades Velero to <code>v1.9.2</code>.</li> <li>Updated Grafana helm chart to <code>v6.43.4</code>, which also upgrades Grafana to <code>v9.2.4</code>.</li> </ul>"},{"location":"release-notes/#changed_2","title":"Changed","text":"<ul> <li>Improved Network security by adding Network policies to a lot of the included services.</li> <li>NetworkPolicies are now automatically propagated from a parent namespace to its subnamespaces in HNC.</li> <li>Several default resource requests and limits have changed for the included services.</li> <li>Lowered the default retention age for Kubernetes logs in the prod flavor down to 30 days.</li> <li>Made dex ID Token expiration time configurable.</li> <li>User alertmanager is now enabled by default.</li> </ul>"},{"location":"release-notes/#fixed_1","title":"Fixed","text":"<ul> <li>Fixed an issue with the \"Kubernetes cluster status\" Grafana dashboard not loading data for some panels</li> <li>Rclone can now be configured to run every x minutes/hours/days/week/month/year.</li> </ul>"},{"location":"release-notes/#added_3","title":"Added","text":"<ul> <li>Added RBAC for admin users to view Gatekeeper constraints.</li> <li>New section in the welcoming dashboards, displaying the most relevant features and changes for the user added in the last two releases.</li> <li>Added an option to configure alerts for growing indices in OpenSearch.</li> <li>The settings for this might need to be tweaked to better suit the environment.</li> <li>Added an alert for failed evicted pods (KubeFailedEvictedPods).</li> </ul>"},{"location":"release-notes/#v0260","title":"v0.26.0","text":"<p>Released 2022-09-19</p>"},{"location":"release-notes/#updated_2","title":"Updated","text":"<ul> <li>Harbor upgraded to <code>v2.6.0</code></li> <li>Upgraded Opensearch helm chart to <code>2.6.0</code>, this upgrades Opensearch to <code>2.3.0</code>. For more information about the upgrade, check out their 2.3 Launch Announcement.</li> </ul>"},{"location":"release-notes/#fixed_2","title":"Fixed","text":"<ul> <li>Fixed the welcome dashboard template for OpenSearch Dashboards</li> </ul>"},{"location":"release-notes/#added_4","title":"Added","text":"<ul> <li>Option to create custom solvers for letsencrypt issuers, including a simple way to add secrets</li> <li>Kube-bench runs on every node   Automated CIS tests are performed on each node using kube-bench   Added a CIS kube-bench Grafana dashboard</li> <li>Added option for kured to notify to slack when draning and rebooting nodes</li> <li>Allow users to proxy and port-forward to prometheus running in the workload cluster</li> </ul>"},{"location":"release-notes/#v0250","title":"v0.25.0","text":"<p>Released 2022-08-25</p>"},{"location":"release-notes/#added_5","title":"Added","text":"<ul> <li>Added Hierarchical Namespace Controller Allowing users to create and manage subnamespaces, namespaces within namespaces. You can read more about this in our FAQ.</li> <li>Added support for custom solvers in cluster issuers  Allowing DNS01 challenges for certificate requests.</li> <li>Added support for running Harbor in High Availability</li> </ul>"},{"location":"release-notes/#updated_3","title":"Updated","text":"<ul> <li> <p>Updated cert-manager from v1.6.1 to v1.8.2  API versions <code>v1alpha2</code>, <code>v1alpha3</code>, and <code>v1beta1</code> have been removed from the custom resource definitions (CRDs), certificate rotation policy will now be validated. See their changelog for more details.</p> </li> <li> <p>Updated OpenSearch with new usability improvements and features  Checkout their launch announcement.</p> </li> </ul>"},{"location":"release-notes/#changed_3","title":"Changed","text":"<ul> <li>New additions to the Kubernetes cluster status Grafana dashboard  It now shows information about resource requests and limits per node, and resource usage vs request per pod.</li> </ul>"},{"location":"release-notes/#v0241","title":"v0.24.1","text":"<p>Released 2022-08-01</p> <ul> <li>Required patch to be able to use release <code>v0.24.0</code></li> </ul>"},{"location":"release-notes/#fixed_3","title":"Fixed","text":"<ul> <li>Fixed a formatting issue with harbor s3 configuration.</li> </ul>"},{"location":"release-notes/#v0240","title":"v0.24.0","text":"<p>Released 2022-07-25</p>"},{"location":"release-notes/#updated_4","title":"Updated","text":"<ul> <li> <p>Upgraded Helm stack   Upgrades for Helm, Helmfile and Helm-secrets.</p> </li> <li> <p>Image upgrade to node-local-dns</p> </li> </ul>"},{"location":"release-notes/#changed_4","title":"Changed","text":"<ul> <li>Improved stability to automatic node reboots</li> </ul>"},{"location":"release-notes/#added_6","title":"Added","text":"<ul> <li>Further configurability to ingress-nginx</li> </ul>"},{"location":"release-notes/#v0230","title":"v0.23.0","text":"<p>Released 2022-07-06</p>"},{"location":"release-notes/#updated_5","title":"Updated","text":"<ul> <li>Updated the ingress controller <code>ingress-nginx</code> to image version v1.2.1</li> <li>You can find the changelog here.</li> </ul>"},{"location":"release-notes/#changed_5","title":"Changed","text":"<ul> <li>Added support for accessing Alertmanager via port-forward</li> </ul>"},{"location":"release-notes/#added_7","title":"Added","text":"<ul> <li>Backups can now be encrypted before they are replicated to an off-site S3 service.</li> <li>Improved metrics and alerting for OpenSearch.</li> </ul>"},{"location":"release-notes/#fixed_4","title":"Fixed","text":"<ul> <li>The deployment of Dex is now properly configured to be HA, ensuring that the Dex instances are placed on different Kubernetes worker nodes.</li> </ul>"},{"location":"release-notes/#v0220","title":"v0.22.0","text":"<p>Released 2022-06-01</p>"},{"location":"release-notes/#added_8","title":"Added","text":"<ul> <li> <p>Added support for Elastx and UpCloud!</p> </li> <li> <p>New 'Welcoming' dashboard in OpenSearch and Grafana.   Users can now access public docs and different urls to the services provided by Compliant Kubernetes.</p> </li> <li> <p>Improved availability of metrics and alerting.   Alertmanager now runs with two replicas by default, Prometheus can now be run in HA mode.</p> </li> <li> <p>Added Falco rules to reduce alerts for services in Compliant Kubernetes.   Falco now alerts less on operations that are expected out of these services.</p> </li> </ul>"},{"location":"release-notes/#fixed_5","title":"Fixed","text":"<ul> <li> <p>Fixed a bug where users couldn't silence alerts when portforwarding to alertmanager.</p> </li> <li> <p>Improved logging stack and fixed a number of issues to ensure reliability.</p> </li> </ul>"},{"location":"release-notes/#v0210","title":"v0.21.0","text":"<p>Released 2022-05-04</p>"},{"location":"release-notes/#changed_6","title":"Changed","text":"<ul> <li> <p>Users can now view ClusterIssuers.</p> </li> <li> <p>User admins can now add users to the ClusterRole user-view.   This is done by adding users to the ClusterRoleBinding <code>extra-user-view</code>.</p> </li> <li> <p>User can now get ClusterIssuers.</p> </li> <li> <p>Ensured all CISO dashboards are available to users.   All the grafana dashboards in our CISO docs are now available.</p> </li> <li> <p>Better stability for dex   Dex now runs with two replicas and has been updated.</p> </li> </ul>"},{"location":"release-notes/#updated_6","title":"Updated","text":"<ul> <li>Image upgrades to reduce number of vulnerabilities   Upgrades for fluentd, grafana, and harbor chartmuseum.</li> </ul>"},{"location":"release-notes/#v0200","title":"v0.20.0","text":"<p>Released 2022-03-21</p>"},{"location":"release-notes/#added_9","title":"Added","text":"<ul> <li> <p>Added kured - Kubernetes Reboot Daemon.   This enables automatic node reboots and security patching of the underlying base Operating System image, container runtime and Kubernetes cluster components.</p> </li> <li> <p>Added fluentd grafana dashboard and alerts.</p> </li> <li> <p>Added RBAC for admin users.   Admin users can now list pods cluster wide and run the kubectl top command.</p> </li> <li> <p>Added containerd support for fluentd.</p> </li> </ul>"},{"location":"release-notes/#changed_7","title":"Changed","text":"<ul> <li> <p>Added the new OPA policy.   To disallow the latest image tag.</p> </li> <li> <p>Persist Dex state in Kubernetes.   This ensure the JWT token received from an OpenID provider is valid even after security patching of Kubernetes cluster components.</p> </li> <li> <p>Add ingressClassName in ingresses where that configuration option is available.</p> </li> <li> <p>Thanos is now enabled by default.</p> </li> </ul>"},{"location":"release-notes/#updated_7","title":"Updated","text":"<ul> <li> <p>Upgraded nginx-ingress helm chart to v4.0.17   This upgrades nginx-ingress to v1.1.1. When upgrading an ingressClass object called nginx will be installed, this class has been set as the default class in Kubernetes. Ingress-nginx has been configured to still handle existing ingress objects that do not specify any ingressClassName.</p> </li> <li> <p>Upgraded starboard-operator helm chart to v0.9.1   This is upgrading starboard-operator to v0.14.1</p> </li> </ul>"},{"location":"release-notes/#removed","title":"Removed","text":"<ul> <li>Removed influxDB and dependent helm charts.</li> </ul>"},{"location":"release-notes/#v0191","title":"v0.19.1","text":"<p>Released 2022-03-01</p>"},{"location":"release-notes/#fixed_6","title":"Fixed","text":"<ul> <li>Fixed critical stability issue related to Prometheus rules being evaluated without metrics.</li> </ul>"},{"location":"release-notes/#v0190","title":"v0.19.0","text":"<p>Released 2022-02-01</p>"},{"location":"release-notes/#added_10","title":"Added","text":"<ul> <li> <p>Added Thanos as a new metrics backend.   Provides a much more efficient and reliable platform for long-term metrics, with the capabilities to keep metrics for much longer time periods than previously possible.   InfluxDB will still be supported in this release.</p> </li> <li> <p>Added a new feature to enable off-site replication of backups.   Synchronizes S3 buckets across regions or clouds to keep an off-site backup.</p> </li> <li> <p>Added a new feature to create and log into separate indices per namespace. Currently considered to be an alpha feature.</p> </li> </ul>"},{"location":"release-notes/#changed_8","title":"Changed","text":"<ul> <li> <p>Replacing Open Distro for Elasticsearch with OpenSearch.   In this release, since the Open Distro project has reached end of life, Elasticsearch is replaced with OpenSearch and Kibana with OpenSearch Dashboards.   OpenSearch is a fully open source fork of Elasticsearch with a compatible API and familiar User Experience. Note that recent versions of official Elasticsearch clients and tools will not work with OpenSearch as they employ a product check, compatible versions can be found here.</p> </li> <li> <p>Enforcing OPA policies by default.   Provides strict safeguards by default.</p> </li> <li> <p>Allowing viewers to inspect and temporarily edit panels in Grafana.   Gives more insight to the metrics and data shown.</p> </li> <li> <p>Setting Fluentd to log the reason why when it can't push logs to OpenSearch.</p> </li> </ul>"},{"location":"release-notes/#updated_8","title":"Updated","text":"<ul> <li>Large number of application and service updates, keeping up to date with new security fixes and changes.</li> </ul>"},{"location":"release-notes/#v0182","title":"v0.18.2","text":"<p>Released 2021-12-16.</p> <p>Changes:</p> <ul> <li>Updated Open Distro for Elasticsearch to 1.13.3 to mitigate CVE-2021-44228 &amp; CVE-2021-45046</li> </ul>"},{"location":"release-notes/#v0172","title":"v0.17.2","text":"<p>Released 2021-12-16.</p> <p>Changes:</p> <ul> <li>Updated Open Distro for Elasticsearch to 1.13.3 to mitigate CVE-2021-44228 &amp; CVE-2021-45046</li> </ul>"},{"location":"release-notes/#v0181","title":"v0.18.1","text":"<p>Released 2021-12-08.</p> <p>Changes:</p> <ul> <li>updated Grafana to 8.0.7 in order to fix CVE-2021-43798</li> </ul>"},{"location":"release-notes/#v0171","title":"v0.17.1","text":"<p>Released 2021-12-08.</p> <p>Changes:</p> <ul> <li>updated Grafana to 8.0.7 in order to fix CVE-2021-43798</li> </ul>"},{"location":"release-notes/#v0180","title":"v0.18.0","text":"<p>Released 2021-11-04.</p> <p>Changes:</p> <ul> <li>Ingress-nginx-controller has been updated from v0.28.0 to v0.49.3, bringing various updates.<ul> <li>Additionally, the configuration option <code>allow-snippet-annotations</code> has been set to <code>false</code> to mitigate known security issue CVE-2021-25742</li> </ul> </li> <li>Fixes, minor version upgrades, improvements to resource requests and limits for applications, improvements to stability.</li> </ul>"},{"location":"release-notes/#v0170","title":"v0.17.0","text":"<p>Released 2021-06-29.</p> <p>Changes:</p> <ul> <li>The dashboard tool Grafana has been updated to a new major version of 8.x.x. This introduces new features and fixes, as well as some possibly breaking changes. See their release notes for more information.</li> <li>The single-sign-on service Dex has been updated, bringing small changes and better consistency to the UI.</li> <li>Fixes, improvements to resource limits, resource usage, and stability.</li> </ul>"},{"location":"release-notes/#v0160","title":"v0.16.0","text":"<p>Released 2021-05-27.</p> <p>Changes:</p> <ul> <li>The default retention values have been changed and streamlined for <code>authlog*</code> and <code>other*</code>. The former will be kept for a longer period of time while the latter for shorter, both have reduced sized according to their actual usage.</li> <li>Updates, fixes, and features to improve the security of the platform.</li> </ul>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#gpu-support","title":"GPU support","text":"<p>Support for GPU nodes in Compliant Kubernetes workload clusters, subject to GPU availability on supported cloud providers.</p>"},{"location":"roadmap/#secrets-management-service","title":"Secrets management service","text":"<p>Convenient solution to manage secrets for Compliant Kubernetes application developers, such as SOPS or Sealed Secrets.</p>"},{"location":"roadmap/#saml-support","title":"SAML support","text":"<p>Support for SAML based Identity Providers (IDPs) as a complement to currently supported OpenID format.</p>"},{"location":"roadmap/#argocd-configation-that-adhere-to-security-safeguards","title":"ArgoCD configation that adhere to security safeguards","text":"<p>Locked down security profile for ArgoCD that adheres to Compliant Kubernetes security practices. By default, pull-based CD solutions such as ArgoCD (and Flux, for that matter) require too extensive security privileges.</p>"},{"location":"roadmap/#additional-dashboards","title":"Additional dashboards","text":"<p>User experience for Compliant Kubernetes operators, application developers, and CISOs will be continuously improved, including addition of single pane of glass dashboards that give overviews of all relevant services.</p>"},{"location":"roadmap/#non-goals","title":"Non-Goals","text":""},{"location":"roadmap/#opinionated-cicd","title":"Opinionated CI/CD","text":"<p>Compliant Kubernetes can be used with a wide range of CI/CD pipelines, including traditional push-style tools and pull-style solutions such as GitOps operators. Compliant Kubernetes will not dictate the use of one specific CI/CD technology.</p>"},{"location":"vocabulary/","title":"Vocabulary","text":"<p>This page introduces terminology used in the Compliant Kubernetes project. We assume that you are familiar with Kubernetes concepts.</p> <ul> <li>Administrator: A person or automation process (i.e., CI/CD pipeline) that creates, destoys, updates or otherwise maintains a Compliant Kubernetes installation.</li> <li>Control: \"Points\" in an organization that need a clear policy in order to comply with regulation.</li> <li>Regulation: Law or contractual requirements that an organization is required to follow to be allowed to operate.</li> <li>Operator: Software extension to Kubernetes (see Operator pattern). Rarely used to mean \"administrator\".</li> <li>Service cluster: Kubernetes cluster that hosts monitoring, logging and technical vulnerability management components. These components are separated from the workload cluster to give an extra layer of security, as is required by some regulations.</li> <li>Workload cluster: Kubernetes cluster hosting the application that exposes end-user -- front-office or back-office -- functionality.</li> <li>User: A person or an automation process (i.e., CI/CD pipeline) that interacts with Compliant Kubernetes for the purpose of running and monitoring an application hosted by Compliant Kubernetes.</li> </ul>"},{"location":"watch-demos/","title":"Watch Demos","text":"<p>Redirecting to Youtube Playlist ...</p>"},{"location":"adr/","title":"Architectural Decision Log","text":"","tags":["ISO 27001 A.14.1.1 Information Security Requirements Analysis & Specification","ISO 27001 A.14.2.4 Restrictions on Changes to Software Packages"]},{"location":"adr/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.14.1.1 \"Information security requirements analysis and specification\"</li> <li>A.14.2.4 \"Restrictions on Changes to Software Packages\"</li> </ul>","tags":["ISO 27001 A.14.1.1 Information Security Requirements Analysis & Specification","ISO 27001 A.14.2.4 Restrictions on Changes to Software Packages"]},{"location":"adr/#what-are-architectural-decisions","title":"What are architectural decisions?","text":"<p>Architectural decisions are high-level technical decisions that affect most stakeholders, in particular Compliant Kubernetes developers, administrators and users. A non-exhaustive list of architectural decisions is as follows:</p> <ul> <li>adding or removing tools;</li> <li>adding or removing components;</li> <li>changing what component talks to what other component;</li> <li>major (in the SemVer sense) component upgrades.</li> </ul> <p>Architectural decisions should be taken as directions to follow for future development and not issues to be fixed immediately.</p>","tags":["ISO 27001 A.14.1.1 Information Security Requirements Analysis & Specification","ISO 27001 A.14.2.4 Restrictions on Changes to Software Packages"]},{"location":"adr/#what-triggers-an-architectural-decision","title":"What triggers an architectural decision?","text":"<p>An architectural decision generally starts with one of the following:</p> <ul> <li>A new features was requested by product management.</li> <li>An improvement was requested by engineering management.</li> <li>A new risk was discovered, usually by the architect, but also by any stakeholder.</li> <li>A new technology was discovered, that may help with a new feature, an improvement or to mitigate a risk.</li> </ul>","tags":["ISO 27001 A.14.1.1 Information Security Requirements Analysis & Specification","ISO 27001 A.14.2.4 Restrictions on Changes to Software Packages"]},{"location":"adr/#how-are-architectural-decisions-captured","title":"How are architectural decisions captured?","text":"<p>Architectural decisions are captured via Architectural Decision Records or the tech radar. Both are stored in Git, hence a decision log is also captured as part of the Git commit messages.</p>","tags":["ISO 27001 A.14.1.1 Information Security Requirements Analysis & Specification","ISO 27001 A.14.2.4 Restrictions on Changes to Software Packages"]},{"location":"adr/#how-are-architectural-decisions-taken","title":"How are architectural decisions taken?","text":"<p>Architectural decisions need to mitigate the following information security risks:</p> <ul> <li>a component might not fulfill advertised expectations;</li> <li>a component might be abandoned;</li> <li>a component might change direction and deviate from expectations;</li> <li>a component might require a lot of (initial or ongoing) training;</li> <li>a component might not take security seriously;</li> <li>a component might change its license, prohibiting its reuse or making its use expensive.</li> </ul> <p>The Compliant Kubernetes architect is overall responsible for this risk.</p>","tags":["ISO 27001 A.14.1.1 Information Security Requirements Analysis & Specification","ISO 27001 A.14.2.4 Restrictions on Changes to Software Packages"]},{"location":"adr/#how-are-these-risks-mitigated","title":"How are these risks mitigated?","text":"<p>Before taking in any new component to Compliant Kubernetes, we investigate and evaluate them. We prefer components that are:</p> <ul> <li>community-driven open-source projects, to reduce the risk of a component becoming abandoned, changing its license or changing direction in the interest of a single entity; as far as possible, we choose CNCF projects (preferably graduated ones) or projects which are governed by at least 3 different entities;</li> <li>projects with a good security track record, to avoid unexpected security vulnerabilities or delays in fixing security vulnerabilities; as far as possible, we choose projects with a clear security disclosure process and a clear security announcement process;</li> <li>projects that are popular, both from a usage and contribution perspective; as far as possible, we choose projects featuring well-known users and many contributors;</li> <li>projects that rely on technologies that our team is already trained on, to reduce the risk of requiring a lot of (initial or ongoing) training; as far as possible, we choose projects that overlap with the projects already on our tech radar;</li> <li>projects that are simple to install and manage, to reduce required training and burden on administrators.</li> </ul> <p>Often, it is not possible to fulfill the above criteria. In that case, we take the following mitigations:</p> <ul> <li>Architectural Decision Records include recommendations on training to be taken by administrators.</li> <li>Closed-source or \"as-a-Service\" alternatives are used, if they are easy to replace thanks to broad API compatibility or standardization.</li> </ul> <p>These mitigations may be relaxed for components that are part of alpha or beta features, as these features -- and required components -- can be removed at our discretion.</p>","tags":["ISO 27001 A.14.1.1 Information Security Requirements Analysis & Specification","ISO 27001 A.14.2.4 Restrictions on Changes to Software Packages"]},{"location":"adr/#adrs","title":"ADRs","text":"<p>This log lists the architectural decisions for Compliant Kubernetes.</p> <ul> <li>ADR-0000 - Use Markdown Architectural Decision Records</li> <li>ADR-0001 - Use Rook for Storage Orchestrator</li> <li>ADR-0002 - Use Kubespray for Cluster Life-cycle</li> <li>ADR-0003 - [Superseded by ADR-0019] Push Metrics via InfluxDB</li> <li>ADR-0004 - Plan for Usage without Wrapper Scripts</li> <li>ADR-0005 - Use Individual SSH Keys</li> <li>ADR-0006 - Use Standard Kubeconfig Mechanisms</li> <li>ADR-0007 - Make Monitoring Forwarders Storage Independent</li> <li>ADR-0008 - Use HostNetwork or LoadBalancer for Ingress</li> <li>ADR-0009 - Use ClusterIssuers for LetsEncrypt</li> <li>ADR-0010 - Run managed services in workload cluster</li> <li>ADR-0011 - Let upstream projects handle CRDs</li> <li>ADR-0012 - [Superseded by ADR-0017] Do not persist Dex</li> <li>ADR-0013 - Configure Alerts in On-call Management Tool (e.g., Opsgenie)</li> <li>ADR-0014 - Use bats for testing bash wrappers</li> <li>ADR-0015 - We believe in community-driven open source</li> <li>ADR-0016 - gid=0 is okay, but not by default</li> <li>ADR-0017 - Persist Dex</li> <li>ADR-0018 - Use Probe to Measure Uptime of Internal Compliant Kubernetes Services</li> <li>ADR-0019 - Push Metrics via Thanos</li> <li>ADR-0020 - Filter by cluster label then data source</li> <li>ADR-0021 - Default to TLS for performance-insensitive additional services</li> <li>ADR-0022 - Use Dedicated Nodes for Additional Services</li> <li>ADR-0023 - Only allow Ingress Configuration Snippet Annotations after Proper Risk Acceptance</li> <li>ADR-0024 - Allow a Harbor robot account that can create other robot accounts with full privileges</li> <li>ADR-0025 - Use local-volume-provisioner for Managed Services that requires high-speed disks.</li> <li>ADR-0026 - Use <code>environment-name</code> as the default root of Hierarchical Namespace Controller (HNC)</li> <li>ADR-0027 - PostgreSQL - Enable external replication</li> <li>ADR-0028 - Harder pod eviction when nodes are going OOM</li> <li>ADR-0030 - Run ArgoCD on the Elastisys nodes</li> <li>ADR-0031 - Run csi-cinder-controllerplugin on the Elastisys nodes</li> <li>ADR-0032 - Boot disk size on nodes</li> <li>ADR-0033 - Run Cluster API controllers on service cluster</li> <li>ADR-0034 - How to run multiple AMS packages of the same type in the same ck8s environment</li> <li>ADR-0035 - Run Tekton on service cluster</li> <li>ADR-0036 - Run ingress-nginx as a daemonSet</li> </ul> <p>For new ADRs, please use template.md as basis. More information on MADR is available at https://adr.github.io/madr/. General information about architectural decision records is available at https://adr.github.io/.</p>","tags":["ISO 27001 A.14.1.1 Information Security Requirements Analysis & Specification","ISO 27001 A.14.2.4 Restrictions on Changes to Software Packages"]},{"location":"adr/#index-regeneration","title":"Index Regeneration","text":"<p>Pre-requisites:</p> <ul> <li>Install npm</li> <li>Install adr-log</li> <li>Install make</li> </ul> <p>Run <code>make -C docs/adr</code></p>","tags":["ISO 27001 A.14.1.1 Information Security Requirements Analysis & Specification","ISO 27001 A.14.2.4 Restrictions on Changes to Software Packages"]},{"location":"adr/0000-use-markdown-architectural-decision-records/","title":"Use Markdown Architectural Decision Records","text":""},{"location":"adr/0000-use-markdown-architectural-decision-records/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want to record architectural decisions made in this project. Which format and structure should these records follow?</p>"},{"location":"adr/0000-use-markdown-architectural-decision-records/#considered-options","title":"Considered Options","text":"<ul> <li>MADR 2.1.2 \u2013 The Markdown Architectural Decision Records</li> <li>Formless \u2013 No conventions for file format and structure</li> </ul>"},{"location":"adr/0000-use-markdown-architectural-decision-records/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"MADR 2.1.2\", because</p> <ul> <li>We need to start somewhere, and it's better to have some format than no format.</li> <li>MADR seems to be good enough for our current needs.</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/","title":"Use Rook for Storage Orchestrator","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cristian Klein, Lars Larsson, Pradyumna Kashyap, Daniel Harr, Viktor Forsberg, Fredrik Liv</li> <li>Date: 2020-11-16</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Compliant Kubernetes has the vision to reduce the compliance burden on multiple clouds (\"Multi-cloud. Open source. Compliant.\"). Many of the cloud providers we target do not have a storage provider or do not have a storage provider that integrates with Kubernetes. How should we support PersistentVolumeClaims in such cases?</p>"},{"location":"adr/0001-use-rook-storage-orchestrator/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes clusters.</li> <li>Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call administrators are not constantly woken up.</li> <li>Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.)</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#considered-options","title":"Considered Options","text":"<ul> <li>Rook</li> <li>GlusterFS</li> <li>Longhorn</li> <li>NFS Storage Provider</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Rook\", because it is CNCF graduated, hence it is most likely to drive development and adoption long-term. Prady tested it and showed it was easy to use. It supports Ceph as a backend, making it battle-tested. It has reasonable performance.</p>"},{"location":"adr/0001-use-rook-storage-orchestrator/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We no longer need to worry about cloud provider without native storage.</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We need to deprecate our NFS storage provider.</li> <li>Some manual steps are required to set up partitions for Rook. These will be automated when the burden justifies it.</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0001-use-rook-storage-orchestrator/#longhorn","title":"Longhorn","text":"<ul> <li>Good, because it is a CNCF project.</li> <li>Good, because it is well integrated with Kubernetes.</li> <li>Bad, because it is not the most mature CNCF project in the storage class.</li> <li>Bad, because it was not easy to set up.</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#glusterfs","title":"GlusterFS","text":"<ul> <li>Good, because it is battle-tested.</li> <li>Bad, because it is not as well integrated with Kubernetes as other projects.</li> <li>Bad, because it is not a CNCF project (driven by Red Hat).</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#nfs-storage-provider","title":"NFS Storage Provider","text":"<ul> <li>Good, because we used it before and we have experience.</li> <li>Bad, because it is a non-redundant, snowflake, brittle solution.</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/","title":"Use Kubespray for Cluster Life-cycle","text":"<ul> <li>Status: accepted</li> <li>Deciders: Lars, Johan, Cristian, Emil, Viktor, Geoff, Ewnetu, Fredrik (potentially others who attended the architecture meeting, but I can't remember)</li> <li>Date: 2020-11-17</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Compliant Kubernetes promises: \"Multi-cloud. Open source. Compliant\". So far, we delivered on our multi-cloud promise by using our in-house <code>ck8s-cluster</code> implementation. This strategy feels unsustainable for two reasons: First, we don't have the resources to catch up and keep up with open source projects in the cluster life-cycle space. Second, we don't want to differentiate on how to set up vanilla Kubernetes cluster, i.e., lower in the Kubernetes stack. Rather we want to differentiate on services on top of vanilla Kubernetes clusters.</p>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to differentiate on top of vanilla Kubernetes cluster.</li> <li>We want to be able to run Compliant Kubernetes on top of as many cloud providers as possible.</li> <li>We promise building on top of best-of-breeds open source projects.</li> <li>We want to reduce burden with developing and maintaining our in-house tooling for cluster life-cycle management.</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#considered-options","title":"Considered Options","text":"<ul> <li>Rancher</li> <li>kubeadm via in-house tools (ck8s-cluster)</li> <li>kubespray</li> <li>kops</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#decision-outcome","title":"Decision Outcome","text":"<p>We chose kubespray, because it is best aligned with our interests, both feature- and roadmap-wise. It has a large community and is expected to be well maintained in the future. It uses kubeadm for domain knowledge on how to set up Kubernetes clusters.</p>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We learn how to use a widely-used tool for cluster lifecycle management.</li> <li>We support many cloud providers.</li> <li>We can differentiate on top of vanilla Kubernetes.</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We need training on kubespray.</li> <li>We need to port our tooling and practices to kubespray.</li> <li>We need to port <code>compliantkubernetes-apps</code> to work on kubespray.</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#rancher","title":"Rancher","text":"<ul> <li>Good, because it provides cluster life-cycle management at scale.</li> <li>Bad, because it creates clusters in an opinionated way, which is insufficiently flexible for our needs.</li> <li>Bad, because it is not a community project, hence entails long-term licensing uncertainty.</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#kubeadm-via-in-house-tool-ck8s-cluster","title":"kubeadm via in-house tool (ck8s-cluster)","text":"<ul> <li>Good, because we know it and we built it.</li> <li>Good, because it works well for current use-cases.</li> <li>Bad, because it entails a lot of effort to develop and maintain.</li> <li>Bad, because it is lagging behind feature-wise with other cluster life-cycle solutions.</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#kops","title":"kops","text":"<ul> <li>Good, because it integrates well with the underlying cloud provider (e.g., AWS).</li> <li>Bad, because it supports fewer cloud providers than kubespray.</li> </ul> <p>NOTE: In the future, we might want to support <code>compliantkubernetes-apps</code> on top of both kops and kubespray, but this does not seem to bring value just now.</p>"},{"location":"adr/0003-push-metrics-via-influxdb/","title":"[Superseded by ADR-0019] Push Metrics via InfluxDB","text":"<ul> <li>Status: superseded by ADR-0019</li> <li>Deciders: Johan, Cristian, Viktor, Emil, Olle, Fredrik</li> <li>Date: 2020-11-19</li> </ul>"},{"location":"adr/0003-push-metrics-via-influxdb/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want to support workload multi-tenancy, i.e., one service cluster -- hosting the tamper-proof logging environment -- and multiple workload clusters. Currently, the service cluster exposes two end-points for workload clusters:</p> <ul> <li>Dex, for authentication;</li> <li>Elastisearch, for pushing logs (append-only).</li> </ul> <p>Currently, the service cluster pulls metrics from the workload cluster. This makes it difficult to have multiple workload clusters connected to the same service cluster.</p>"},{"location":"adr/0003-push-metrics-via-influxdb/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to support workload multi-tenancy.</li> <li>We want to untangle the life-cycle of the service cluster and workload cluster.</li> <li>The service cluster acts as a tamper-proof logging environment, hence it should be difficult to tamper with metrics from the workload cluster.</li> </ul>"},{"location":"adr/0003-push-metrics-via-influxdb/#considered-options","title":"Considered Options","text":"<ol> <li>Service cluster exposes InfluxDB; workload cluster pushes metrics into InfluxDB.</li> <li>Migrate from InfluxDB to Thanos</li> <li>Migrate from InfluxDB to Cortex</li> </ol>"},{"location":"adr/0003-push-metrics-via-influxdb/#decision-outcome","title":"Decision Outcome","text":"<p>We chose to push metrics from the workload cluster to the service cluster via InfluxDB, because it involves the least amount of effort and is sufficient for the current use-cases that we want to support. InfluxDB supports a writer role, which makes overwriting metrics difficult -- unfortunately, not impossible.</p>"},{"location":"adr/0003-push-metrics-via-influxdb/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>All of <code>*.$opsDomain</code> can point to the service cluster workers -- optionally fronted by a load-balancer -- which considerably simplifies setup.</li> <li>Multiple workload clusters can push metrics to the service cluster, which paves the path to workload multi-tenancy.</li> <li>The service cluster can be set up first, followed by one-or-more workload clusters.</li> <li>Workload clusters become more \"cattle\"-ish.</li> </ul>"},{"location":"adr/0003-push-metrics-via-influxdb/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Existing Compliant Kubernetes clusters will need some manual migration steps, in particular changing the <code>prometheus.$opsDomain</code> DNS entry.</li> <li>The service cluster exposes yet another endpoint, which should only be available to workload clusters and not the Internet. HTTP authentication (over HTTPS) feels sufficient for now, but we need a follow-up decision on how to add another layer of protection to these endpoints.</li> <li>The workload clusters will have to properly label their metrics.</li> <li>Although not easy, metrics can be overwritten from the workload cluster. We will improve on this when (a) demand for closing this risk increases, (b) we re-evaluate long-term metrics storage.</li> </ul>"},{"location":"adr/0003-push-metrics-via-influxdb/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":"<p>Both Thanos and Cortex seems worthy projects to replace InfluxDB. At the time of this writing, they were both having CNCF Incubating status. The two projects feature a healthy collaboration and are likely to merge in the future.</p> <p>However, right now, migrating away from InfluxDB feels like it adds more cost than benefits. We will reevaluate this decision when InfluxDB is no longer sufficient for our needs.</p>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/","title":"Plan for Usage without Wrapper Scripts","text":"<ul> <li>Status: accepted</li> <li>Deciders: Architecture Meeting</li> <li>Date: 2020-11-24</li> </ul>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We frequently write wrapper scripts. They bring the following value:</p> <ol> <li>They bind together several tools and make them work together as a whole, e.g., <code>sops</code> and <code>kubectl</code>.</li> <li>They encode domain knowledge and standard operating procedures, e.g., how to add a node, how a cluster should look like, where to find configuration files.</li> <li>They enforce best practices, e.g., encrypt secrets consumed or produced by tools.</li> </ol> <p>Unfortunately, wrapper scripts can also bring disadvantages:</p> <ol> <li>They make usages that are deviating from the \"good way\" difficult.</li> <li>They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.</li> <li>They add overhead when adding new features or supporting new use-cases.</li> <li>They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the Law of Leaky Abstractions.</li> </ol>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to make operations simple, predictable, resilient to human error and scalable.</li> <li>We want to have some predictability in how an environment is set up.</li> <li>We want to make Compliant Kubernetes flexible and agile.</li> </ul>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#considered-options","title":"Considered Options","text":"<ul> <li>On one extreme, we can enforce wrapper scripts as the only way forward. This would require significant investment, as these scripts would need to be very powerful and well documented.</li> <li>On the other extreme, we completely \"ban\" wrapper scripts.</li> </ul>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#decision-outcome","title":"Decision Outcome","text":"<p>We have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do brings the sought after value.</p> <p>This decision applies for new wrapper scripts. We will not rework old wrapper scripts.</p>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>The operations team can encode standard operating procedures and scale ways of working.</li> <li>Customer-facing developers can easily reuse artefacts for new use-cases, without significant development effort.</li> <li>Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.</li> </ul>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in a wrapper scripts and what not.</li> </ul>"},{"location":"adr/0005-use-individual-ssh-keys/","title":"Use Individual SSH Keys","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cristian, Fredrik, Olle, Johan</li> <li>Date: 2021-01-28</li> </ul> <p>Technical Story:</p> <ul> <li>Do not fiddle with the SSH key</li> <li>Create a process of how we should move to use personal SSH keys</li> </ul>"},{"location":"adr/0005-use-individual-ssh-keys/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Currently, we create per-cluster SSH key pairs, which are shared among administrators. This is problematic from an information security perspective for a few reasons:</p> <ol> <li>It reduces the auditability of various actions, e.g., who SSH-ed into the Kubernetes control plane Nodes.</li> <li>It makes credential management challenging, e.g., when onboarding/offboarding administrators.</li> <li>It makes credential rotation challenging, e.g., the new SSH key pair needs to be transmitted to all administrators.</li> <li>It encourages storing the SSH key pair without password protection.</li> <li>It makes it difficult to store SSH key pairs on an exfiltration-proof medium, such as a YubiKey.</li> <li>It violates the Principle of Least Astonishment.</li> </ol>"},{"location":"adr/0005-use-individual-ssh-keys/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We need to stick to information security best-practices.</li> </ul>"},{"location":"adr/0005-use-individual-ssh-keys/#considered-options","title":"Considered Options","text":"<ul> <li>Inject SSH keys via cloud-init.</li> <li>Manage SSH keys via an Ansible role.</li> </ul>"},{"location":"adr/0005-use-individual-ssh-keys/#decision-outcome","title":"Decision Outcome","text":"<p>We will manage SSH keys via an Ansible role, since it allows rotating/adding/deleting keys without rebooting nodes. Also, it caters to more environments, e.g., BYO-VMs and BYO-metal. The compliantkubernetes-kubespray project will make it easy to configure SSH keys.</p>"},{"location":"adr/0005-use-individual-ssh-keys/#bootstrapping","title":"Bootstrapping","text":"<p>The above decision raises a chicken-and-egg problem: Ansible needs SSH access to the nodes, but the SSH access is managed via Ansible. This issue is solved as follows.</p> <p>For cloud deployments, all Terraform providers support injecting at least one public SSH key via cloud-init:</p> <ul> <li>AWS</li> <li>Exoscale</li> <li>GCP</li> <li>OpenStack</li> </ul> <p>The administrator who creates the cluster bootstraps SSH access by providing their own public SSH key via cloud-init. Then, the Ansible role adds the public SSH keys of the other administrators.</p> <p>BYO-VM and BYO-metal deployments are handled similarly, except that the initial public SSH key is delivered by email/Slack to the VM/metal administrator.</p>"},{"location":"adr/0005-use-individual-ssh-keys/#recommendations-to-operators","title":"Recommendations to Operators","text":"<ul> <li> <p>Operators should devise procedures for onboarding and offboarding member of the on-call team, as well as rotating SSH keys.</p> </li> <li> <p>The public SSH keys of all on-call administrators could be stored in a repository in a single file with one key per line. The comment of the key should clearly identify the owner.</p> </li> <li> <p>Operator logs (be it stand-alone documents, git or GitOps-like repositories) should clearly list the SSH keys and identities of the administrators configured for each environment.</p> </li> </ul>"},{"location":"adr/0005-use-individual-ssh-keys/#links","title":"Links","text":"<ul> <li>ansible.posix.authorized_key Ansible Module</li> </ul>"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/","title":"Use Standard Kubeconfig Mechanisms","text":"<ul> <li>Status: accepted</li> <li>Deciders: Compliant Kubernetes Architecture Meeing</li> <li>Date: 2021-02-02</li> </ul>"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>To increase adoption of Compliant Kubernetes, we were asked to observe the Principle of Least Astonishment. Currently, Compliant Kubernetes's handing of kubeconfig is astonishing. Most tools in the ecosystem use the standard <code>KUBECONFIG</code> environment variable and kubecontext implemented in the client-go library. These tools leave it up to the user to set <code>KUBECONFIG</code> or use the default <code>~/.kube/config</code>. Similarly, there is a default kubecontext which can be overwritten via command-line. Tools that get cluster credentials generate a context related to the name of the cluster.</p> <p>Tools that behave as such include:</p> <ul> <li><code>gcloud container clusters get-credentials</code></li> <li><code>az aks get-credentials</code></li> <li><code>kops</code></li> <li><code>helmfile</code></li> <li><code>helm</code></li> <li><code>kubectl</code></li> <li><code>fluxctl</code></li> </ul>"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Compliant Kubernetes needs to observe the Principle of Least Astonishment.</li> <li>Compliant Kubernetes needs to be compatible with various \"underlying\" vanilla Kubernetes tools.</li> <li>Compliant Kubernetes needs to be usable with various tools \"on top\".</li> </ul>"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#considered-options","title":"Considered Options","text":"<ul> <li>Current solution, i.e., scripts wrapping kubeconfigs in sops which then execute \"fixed\" commands, like <code>helmfile</code>, <code>helm</code> and <code>kubectl</code>.</li> <li>\"Lighter\" scripts wrapping and unwrapping kubeconfig, allowing administrators to run <code>helmfile</code>, <code>helm</code> and <code>kubectl</code> as the administrator sees fit.</li> <li>Use standard kubeconfig mechanism.</li> </ul>"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#decision-outcome","title":"Decision Outcome","text":"<p>We chose using standard kubeconfig mechanism, because it improves integration both with tools \"below\" Compliant Kubernetas and \"on top\" of Compliant Kubernetes.</p> <p>Tools that produce Kubernetes contexts are expected to use an approach similar to <code>kubectl config set-cluster</code>, <code>set-credentials</code> and <code>set-context</code>. The name of the cluster, user and context should be derived from the name of the cluster.</p> <p>Tools that consume Kubernetes contexts are expected to use an approach similar to <code>kubectl</code>, <code>helm</code> or <code>helmfile</code> (see links below).</p>"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#links","title":"Links","text":"<ul> <li>Organizing Cluster Access Using kubeconfig Files</li> <li>kubectx / kubens</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/","title":"Make Monitoring Forwarders Storage Independent","text":"<ul> <li>Status: accepted</li> <li>Deciders: Axel, Cristian, Fredrik, Johan, Olle, Viktor</li> <li>Date: 2021-02-09</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>In the context of this ADR, forwarders refers to any components that are necessary to forward monitoring information -- specifically traces, metrics and logs -- to some monitoring database. As of February 2021, Compliant Kubernetes employs two projects as forwarders:</p> <ul> <li>Prometheus for metrics forwarding;</li> <li>fluentd for log forwarding.</li> </ul> <p>Similarly, two projects are employed as monitoring databases:</p> <ul> <li>InfluxDB for metrics;</li> <li>Elasticsearch for logs.</li> </ul> <p>Overall, the monitoring system needs to be one order of magnitude more resilient than the monitored system. Forwarders improve the resilience of the monitoring system by providing buffering: In case the database is under maintenance or down, the buffer of the forwarders will ensure that no monitoring information is lost. Hence, forwarders are subject to the following tensions:</p> <ul> <li>More buffering implies storage, which make the forwarders vulnerable to storage outages (e.g., disk full, CSI hiccups);</li> <li>Less buffering implies higher risk of losing monitoring information when the database is under maintenance or down.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want a robust monitoring system.</li> <li>We want to monitor the storage system.</li> <li>We want VM-template-based rendering of the workload cluster, which implies no cloud native storage integration.</li> <li>We want to make it easier to \"cleanup and start from a known good state\".</li> <li>We want to have self-healing and avoid manual actions after failure.</li> <li>We want to be able to find the root cause of an incident quickly.</li> <li>We want to run as many components non-root as possible and tightly integrate with securityContext.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#considered-options","title":"Considered Options","text":"<ul> <li>Use underlying storage provider for increased buffering resilience (current approach).</li> <li>Use Local Persistent Volumes.</li> <li>Use emptyDir volumes.</li> <li>Use hostPath volumes.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: emptyDir for Prometheus as forwarder, because it allows monitoring of the storage system in some cases (e.g. Rook) and can redeploy automatically after node failure. It also keeps the complexity down without much risk of data loss.</p> <p>Fluentd as forwarder is deployed via DaemonSet. Both, emptyDir and hostPath can be used.</p>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We can monitor the storage system.</li> <li>Failure of the storage system does not affect monitoring forwarder.</li> <li>Forwarder can be easier deployed \"fresh\".</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Buffered monitoring information is lost if node is lost.</li> <li>emptyDir can cause disk pressure. This can be handled by alerting on low disk space.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#use-underlying-storage-provider","title":"Use underlying storage provider","text":"<ul> <li>Good, because the forwarder can be restarted on any node.</li> <li>Good, because the buffer can be large.</li> <li>Good, because no buffered monitoring information is lost if a node goes down.</li> <li>Good, because buffered monitoring information is preserved if the forwarder is redeployed.</li> <li>Bad, because non-node-local storage is generally slower. Note, however, that at least SafeSpring and CityCloud use a central Ceph storage cluster for the VM's boot disk, which wipes out node-local's storage advantage.)</li> <li>Bad, because the forwarder will fail if storage provider goes down. This is especially problematic for Exoscale, bare-metal and BYO-VMs.</li> <li>Bad, because the forwarder cannot monitor the storage provider (circular dependency).</li> <li>Bad, because setting right ownership requires init containers or alpha features.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#use-local-persistent-volumes","title":"Use Local Persistent Volumes","text":"<ul> <li>Bad, because the forwarder cannot be restarted on any node without manual action: \"if a node becomes unhealthy, then the local volume becomes inaccessible by the pod. The pod using this volume is unable to run.\".</li> <li>Bad, because the amount of forwarding depends on the node's local disk size.</li> <li>Bad, because buffered monitoring information is lost if the forwarder's node goes down.</li> <li>Good, because buffered monitoring information is preserved if the forwarder is redeployed.</li> <li>Good, because node-local storage is generally faster.</li> <li>Good, because the forwarder will survive failure of storage provider.</li> <li>Good, because the forwarder can monitor the storage provider (no circular dependency).</li> <li>Bad, because local persistent storage requires an additional configuration step.</li> <li>Bad, because setting right ownership requires init containers or alpha features.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#use-emptydir","title":"Use emptyDir","text":"<ul> <li>Good, because the forwarder can be restarted on any node without manual action.</li> <li>Bad, because the amount of forwarding depends on the node's local disk size.</li> <li>Bad, because buffered monitoring information is lost if the forwarder's node goes down.</li> <li>Bad, because buffered monitoring information is lost if the forwarder is (not carefully enough) redeployed.</li> <li>Good, because node-local storage is generally faster.</li> <li>Good, because the forwarder will survive failure of storage provider.</li> <li>Good, because the forwarder can monitor the storage provider (no circular dependency).</li> <li>Good, because works out of the box.</li> <li>Good, because it integrates nicely with <code>securityContext</code>.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#use-hostpath","title":"Use hostPath","text":"<p>Similar to Local Persistent Volumes, but</p> <ul> <li>Worse, because if the forwarder is redeployed on a new node, buffering information may appear/disappear.</li> <li>Better, because it requires no extra storage provider configuration.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#links","title":"Links","text":"<ul> <li>Prometheus Operator Storage</li> </ul>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/","title":"Use HostNetwork or LoadBalancer for Ingress","text":"<ul> <li>Status: accepted</li> <li>Deciders: Axel, Cristian, Fredrik, Johan, Olle, Viktor</li> <li>Date: 2021-02-09</li> </ul> <p>Technical Story: Ingress configuration</p>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Many regulations require traffic to be encrypted over public Internet. Compliant Kubernetes solves this problem via an Ingress controller and cert-manager. As of February 2021, Compliant Kubernetes comes by default with nginx-ingress, but Ambassador is planned as an alternative. The question is, how does traffic arrive at the Ingress controller?</p>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to obey the Principle of Least Astonishment.</li> <li>We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for Kubernetes-controlled load balancer.</li> <li>Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.</li> <li>We want to keep things simple.</li> </ul>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#considered-options","title":"Considered Options","text":"<ul> <li>Via the host network, i.e., some workers expose the Ingress controller on their port 80 and 443.</li> <li>Over a NodePort service, i.e., <code>kube-proxy</code> exposes the Ingress controller on a port between 30000-32767 on each worker.</li> <li>As a Service Type LoadBalancer, i.e., above plus Kubernetes provisions a load balancer via Service controller.</li> </ul>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options:</p> <ol> <li> <p>Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker nodes with a manual or Terraform-controlled load-balancer. This includes:</p> <ul> <li>Where load-balancing does not add value, e.g., if a deployment is planned to have only a single-node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.</li> <li>Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.</li> <li>SafeSpring falls in this category, since it is missing load balancers.</li> <li>If the cloud provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.</li> </ul> </li> <li> <p>Use Service Type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.</p> </li> </ol> <p>Additional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until after we apply Compliant Kubernetes Apps. There is a risk for \"the Internet\" -- LetsEncrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:</p> <pre><code>*.$BASE_DOMAIN     60s A 203.0.113.123\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\n</code></pre> <p>203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by kops and should not feel astonishing.</p>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We make the best of each cloud provider.</li> <li>Obeys principle of least astonishment.</li> <li>We do not add a load balancer \"just because\".</li> </ul>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Complexity is a bit increased, however, this feels like essential complexity.</li> </ul>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#links","title":"Links","text":"<ul> <li>Cloud Controller Manager</li> <li>Ingress Nginx: Bare Metal Considerations</li> </ul>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/","title":"Use ClusterIssuers for LetsEncrypt","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cristian, Lennart</li> <li>Date: 2021-02-26</li> </ul> <p>Technical Story: Make apps less fragile</p>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Data protection regulations require encrypting network traffic over public networks, e.g., via HTTPS. This requires provisioning and rotating TLS certificates. To automate this task, we use the cert-manager, which automates provisioning and rotation of TLS certificates from Let's Encrypt.</p> <p>There are two ways to configure Let's Encrypt as an issuers for cert-manager: Issuer and ClusterIssuer. The former is namespaced, whereas the latter is cluster-wide. Should we use Issuer or ClusterIssuer?</p>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to make compliantkubernetes-apps less fragile, and LetsEncrypt ratelimiting is a cause of fragility.</li> <li>We want to make it easy for users to get started with Compliant Kubernetes in a \"secure by default\" manner.</li> <li>We want to have a clear separation between user and administrator resources, responsibilities and privileges.</li> <li>We want to keep the option open for \"light\" renderings, i.e., a single Kubernetes clusters that hosts both service cluster and workload cluster components.</li> </ul>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#considered-options","title":"Considered Options","text":"<ul> <li>Use one Issuer per namespace; users need to install their own Issuers in the workload clusters.</li> <li>Use ClusterIssuer in service cluster; let users install Issuers in the workload clusters as required.</li> <li>Use ClusterIssuer in both service cluster and workload cluster(s).</li> </ul>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Use ClusterIssuers in the service cluster; optionally enable ClusterIssuers in the workload cluster(s)\", because it reduces fragility, clarifies responsibilities, makes it easy to get started securely.</p> <p>Each cluster is configured with an optional ClusterIssuer called <code>letsencrypt-prod</code> for LetsEncrypt production and <code>letsencrypt-staging</code> for LetsEncrypt staging. The email address for the ClusterIssuers is configured by the administrator.</p>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#recommendations-to-operators","title":"Recommendations to Operators","text":""},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#direct-letsencrypt-emails-to-a-logging-mailbox","title":"Direct LetsEncrypt emails to a \"logging\" mailbox","text":"<p>Although LetsEncrypt does not require an email address, cert-managers seems to require all ClusterIssuers/Issuers to be configured with a syntactically valid email address. Said email address will receive notifications when certificates are close to expiry. Given that Compliant Kubernetes comes with Cryptography dashboards, these emails do not seem useful. Hence, ClusterIssuer emails should be directed to an address that has \"logging\" but not \"alerting\" status.</p>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#separate-registered-domains","title":"Separate registered domains","text":"<p>LetsEncrypt production has a rate limit of 50 certificates per week per registered domain. For example, if <code>awesome-website.workload-cluster.environment.elastisys.se</code> points to the workload cluster's Ingress controller, then an excessive creation and destruction of Ingress resources may trigger rate limiting for all of <code>elastisys.se</code>.</p> <p>It is therefore advisable to:</p> <ul> <li>Use separate registered domains for development and production environments.</li> <li>Use separate registered domains for workload cluster(s) and the service cluster, or restrict which Ingress resources can be created by the user.</li> </ul> <p>Note that, the rate limiting risk exists with both Issuers and ClusterIssuers and was not introduced by this ADR.</p>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/","title":"Run managed services in workload cluster","text":"<ul> <li>Status: proposed. This ADR did not reach consensus, with strong arguments on both sides. However, due to needing a decision in a timely manner, this ADR is actually followed. Therefore, this ADR serves both for visibility and to document a fait accompli.</li> <li>Deciders: Cristian</li> <li>Date: 2021-04-29</li> </ul>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>To truly offer our users an option to run containerized workloads in EU jurisdiction, they also need additional managed services, like databases, message queues, caches, etc.</p> <p>Where should these run?</p>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Some of these services are chatty and need low latency.</li> <li>Some of these services might assume trusted clients over a trusted network.</li> <li>We want to make it easy to run these services with regulatory compliance in mind, e.g., we should be able to reuse Compliant Kubernetes features around monitoring, logging, access control and network segregation.</li> <li>We want to make it difficult for Compliant Kubernetes users to negatively affect managed services.</li> <li>We want to keep support for multiple workload cluster, i.e., application multi-tenancy.</li> <li>Many cloud providers do not support Service Type LoadBalancer, which complicates exposing non-HTTP services outside a Kubernetes cluster.</li> <li>Service cluster might not exist in a future packaging of Compliant Kubernetes.</li> </ul>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#considered-options","title":"Considered Options","text":"<ul> <li>Run managed services in workload cluster</li> <li>Run managed services in service cluster</li> <li>Run managed services in yet another cluster</li> </ul>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"run managed services in workload cluster\".</p>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Latency is minimized: The application consuming the managed service is close to the managed service, without needing to go through intermediate software components, such as a TCP Ingress controller.</li> <li>NetworkPolicies can be reused for communication segregation.</li> <li>OpenID and RBAC in the workload cluster can be reused for user access control.</li> <li>Kubernetes audit log can be re-used for auditing user access managed services. Such access is required, e.g., for manual database migrations and \"rare\" operations like GDPR data correction requests.</li> <li>Ease of exposition: No need for Service Type LoadBalancer, which is not supported on all cloud providers.</li> </ul>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Blurs separation of responsibilities between user and administrator.</li> <li>The managed service is easier impacted by user misusage, e.g., bringing a Node into OOM.</li> <li>Workload cluster can no longer be deployed with \u201cfree for all\u201d security. Operators need to push and fight back against loose access control.</li> </ul>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#links","title":"Links","text":"<ul> <li>Service Type LoadBalancer</li> <li>Exposing TCP and UDP services with ingress-nginx</li> <li>Redis Security</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/","title":"Let upstream projects handle CRDs","text":"<ul> <li>Status: accepted</li> <li>Deciders: Compliant Kubernetes Arch Meeting</li> <li>Date: 2021-04-29</li> </ul> <p>Technical Story: #446 #369 #391 #402 #436.</p>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>CustomResourceDefinitions (CRDs) are tricky. They are essentially a mechanism to change the API of Kubernetes. Helm 2 had zero support for CRDs. Helm 3 has support for installing CRDs, but not upgrading them.</p> <p>How should we handle CRDs?</p>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>CRDs add complexity and need to be treated specially.</li> <li>Generally need to \u201ctrim fat\u201d and rely on upstream.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#considered-options","title":"Considered Options","text":"<ul> <li>Install and upgrade CRDs as part of the bootstrap step, which is a Helm 2 legacy.</li> <li>Rely on whatever mechanism is proposed by upstream Helm Charts.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Rely on upstream\", because it trims fat and reduces astonishment.</p> <p>At installation, rely on upstream's approach to install CRDs (see below). At upgrade, propagate upstream migration steps in CK8s migration steps in each release notes. An issue template was created to ensure we won't forget.</p> <p>Since we \"vendor in\" all Charts, CRDs can be discovered using:</p> <pre><code>grep -R 'kind: CustomResourceDefinition'\n</code></pre>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Less astonishing, compared to installing Chart \"by hand\".</li> <li>Less maintenance, i.e., there is only one source of truth for CRDs.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>None really.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#detailed-audit","title":"Detailed Audit","text":"<p>A detailed audit was performed of all CRDs in Compliant Kubernetes on 2021-04-27.</p> <p>As a summary, all projects encourage installing CRDs as part of standard <code>helm install</code>. Most projects encourage following manual migration steps to handle CRDs. Some projects handle CRD upgrades.</p> <p>A detailed analysis is listed below:</p>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#cert-manager","title":"cert-manager","text":"<ul> <li>Installation: The cert-manager Helm Chart includes the <code>installCRDs</code> value -- by default it is set to <code>false</code>. If set to <code>true</code>, then CRDs are automatically installed when installing cert-manager, albeit not using the CRDs mechanism provided by Helm.</li> <li>Upgrade: CRDs are supposed to be upgraded manually.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#dex","title":"dex","text":"<p>Dex can be configured without CRDs. ADR-0012 argues for that approach.</p>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#gatekeeper","title":"gatekeeper","text":"<ul> <li>Installation: Gatekeeper installs CRDs using the mechanism provided by Helm.</li> <li>Upgrade: Gatekeeper wants you to either uninstall-install or run a helm_migrate.sh.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#prometheus-kube-prometheus-stack","title":"Prometheus (kube-prometheus-stack)","text":"<ul> <li>Installation: kube-prometheus-stack installs CRDs using standard Helm mechanism.</li> <li>Upgrade: kube-prometheus-stack expects you to run manual upgrade steps.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#velero","title":"Velero","text":"<ul> <li>Installation: Velero install CRDs using standard Helm mechanism.</li> <li>Upgrade: Velero includes magic to upgrade CRDs.</li> </ul>"},{"location":"adr/0012-do-not-persist-dex/","title":"[Superseded by ADR-0017] Do not persist Dex","text":"<ul> <li>Status: superseded by ADR-0017</li> <li>Deciders: Compliant Kubernetes Architecture Meeting</li> <li>Date: 2021-04-29</li> </ul> <p>Technical Story: Reduce Helmfile concurrency for improved predictability</p>"},{"location":"adr/0012-do-not-persist-dex/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Dex requires persisting state to perform various tasks such as track refresh tokens, preventing replays, and rotating keys.</p> <p>What persistence option should we use?</p>"},{"location":"adr/0012-do-not-persist-dex/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>CRDs add complexity</li> <li>Storage adds complexity</li> </ul>"},{"location":"adr/0012-do-not-persist-dex/#considered-options","title":"Considered Options","text":"<ul> <li>Use \"memory\" storage</li> <li>Use CRD-based storage</li> </ul>"},{"location":"adr/0012-do-not-persist-dex/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"use memory\", because it simplified operations with little negative impact.</p>"},{"location":"adr/0012-do-not-persist-dex/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Dex brings no additional CRDs, which simplified upgrades.</li> <li>Dex brings no state, which simplified upgrades.</li> </ul>"},{"location":"adr/0012-do-not-persist-dex/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>The authentication flow is disrupted, if Dex is rebooted exactly during an authentication flow. There is no user impact if Dex is restarted after the JWT was issued. Cristian tested this with <code>kubectl</code> and Grafana. Since we will only reboot Dex during maintenance windows, this is unlikely to be an issue in the foreseeable future.</li> </ul>"},{"location":"adr/0012-do-not-persist-dex/#other-considerations","title":"Other Considerations","text":"<p>If Dex becomes a bottleneck and needs replication, or if we want to avoid disrupting authentication flows during operations on Dex, we will have to revisit this ADR.</p>"},{"location":"adr/0013-configure-alerts-in-omt/","title":"Configure Alerts in On-call Management Tool (e.g., Opsgenie)","text":"<ul> <li>Status: accepted</li> <li>Deciders: Compliant Kubernetes Architecture Meeting</li> <li>Date: 2021-06-03</li> </ul> <p>Technical Story: See \"Investigate how to systematically work with alerts\"</p>"},{"location":"adr/0013-configure-alerts-in-omt/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Alerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full. Terminology differs across tooling and organizations, but one generally cares about:</p> <ul> <li>P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;</li> <li>P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;</li> <li>P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.</li> </ul> <p>Other priorities (e.g., P4 and below) are generally used for informational purposes.</p> <p>Dealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when. \"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.</p> <p>Under-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\". Over-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored. Hence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.</p> <p>Where should alerting be configured, so as to quickly converge to the optimal alerting level?</p>"},{"location":"adr/0013-configure-alerts-in-omt/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Allow to quickly silence, un-silence and re-prioritize alerts.</li> <li>Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what cluster and namespaces should notification happen, etc.</li> <li>Leverage existing tools and processes.</li> </ul>"},{"location":"adr/0013-configure-alerts-in-omt/#considered-options","title":"Considered Options","text":"<ul> <li>Configure alerting in Compliant Kubernetes, specifically alertmanager.</li> <li>Configure alerting in an On-call Management Tool (OMT), e.g., Opsgenie, PagerDuty.</li> </ul>"},{"location":"adr/0013-configure-alerts-in-omt/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: Compliant Kubernetes \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie). Configuration of alerts happens in the OMT.</p>"},{"location":"adr/0013-configure-alerts-in-omt/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Clear separation of concerns.</li> <li>Alerting does not require per-customer configuration of Compliant Kubernetes.</li> <li>Leverages existing tools and processes.</li> <li>We do not need to implement complex alert filtering in Compliant Kubernetes, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.</li> </ul>"},{"location":"adr/0013-configure-alerts-in-omt/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Does not capture alerting know-how in Compliant Kubernetes.</li> <li>Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.</li> </ul>"},{"location":"adr/0013-configure-alerts-in-omt/#recommendations-to-operators","title":"Recommendations to Operators","text":"<ul> <li>Operators should familiarize themselves with the capabilities of OMT, e.g., OpsGenie. This should be first done using a web UI, since that improves discoverability of such capabilities.</li> <li>When alerting configuration becomes too complex and/or repetitive, administrators should employ a configuration management tools, such as Terraform, to configure the OMT.</li> </ul>"},{"location":"adr/0013-configure-alerts-in-omt/#links","title":"Links","text":"<ul> <li>Opsgenie documentation</li> <li>Alertmanager documentation</li> <li>Terraform Opsgenie provider</li> <li>Pulumni Opsgenie module</li> </ul>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/","title":"Use bats for testing bash wrappers","text":"<ul> <li>Status: accepted</li> <li>Deciders: Compliant Kubernetes Architecture Meeting</li> <li>Date: 2021-06-03</li> </ul>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We write wrapper scripts for simpler and consistent operations. How should we test these scripts?</p>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to use the best tools out there.</li> <li>We want to reduce tools sprawl, i.e., the collective cost (e.g., training) of adding a new tool should outweigh the collective benefit of the new tool.</li> <li>We want to make contributions inviting.</li> </ul>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#considered-options","title":"Considered Options","text":"<ul> <li>Do not test bash scripts. (We write perfect scripts 100% of the time, right? :smile:)</li> <li>Use <code>alias</code> for mocking, <code>diff</code> and <code>test</code> for assertions.</li> <li>Use bats</li> </ul>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"bats\", because the benefit of using a standard and rather light tool outweighs the cost of collective training on the new tool.</p>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We use a pretty standard tool for testing in the bash universe.</li> <li>We do not risk re-inventing the while by writing our own wrappers around <code>alias</code>, <code>diff</code> and <code>test</code>.</li> </ul>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We need to learn another tool, fortunately, it seems pretty light.</li> </ul>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#other-considerations","title":"Other Considerations","text":"<p>Be very mindful about not overusing bash. Generally bash should only be used for things that you would do in the terminal, but got tired of copy-pasting, like:</p> <ul> <li>Running commands</li> <li>Copying files</li> <li>Setting environment variables</li> <li>Minor path translations</li> </ul> <p>For more advanced functionality prefer upstreaming into Ansible roles/libraries, Helm Charts, upstream source code, etc.</p>"},{"location":"adr/0015-we-believe-in-community-driven-open-source/","title":"We believe in community-driven open source","text":"<ul> <li>Status: accepted</li> <li>Deciders: Rob, Johan, Cristian (a.k.a., Product Management working group)</li> <li>Date: 2021-08-17</li> </ul>","tags":["ISO 27001 A.17.1.1 Planning Information Security Continuity"]},{"location":"adr/0015-we-believe-in-community-driven-open-source/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We often get bombarded with questions like \"Why don't you use X?\" or \"Why don't you build on top of Y?\", sometimes preceded by \"product/project X already has feature Y\". Needless to say, this can cause a \"Simpsons Already Did It\" feeling.</p> <p>This ADR clarifies one of the core values of the Compliant Kubernetes project, namely our belief in community-driven open source. The ADR is useful to clarify both to internal and external stakeholders the choices we make.</p>","tags":["ISO 27001 A.17.1.1 Planning Information Security Continuity"]},{"location":"adr/0015-we-believe-in-community-driven-open-source/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We do not want to depend on the interests of any single company, be it small or large.</li> <li>Our customers need to have a business continuity plan, see ISO 27001, Annex A.17. Therefore, we want to make it easy to \"exit\" Compliant Kubernetes and take over platform management.</li> <li>We want to use the best tools out there.</li> </ul>","tags":["ISO 27001 A.17.1.1 Planning Information Security Continuity"]},{"location":"adr/0015-we-believe-in-community-driven-open-source/#considered-options","title":"Considered Options","text":"<ul> <li>Prefer closed source solutions.</li> <li>Prefer single-company open source solutions.</li> <li>Prefer community-drive open source solutions.</li> </ul>","tags":["ISO 27001 A.17.1.1 Planning Information Security Continuity"]},{"location":"adr/0015-we-believe-in-community-driven-open-source/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"prefer community-driven open source solutions\".</p>","tags":["ISO 27001 A.17.1.1 Planning Information Security Continuity"]},{"location":"adr/0015-we-believe-in-community-driven-open-source/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We do not depend on the interests of any single company.</li> <li>Our customers do not depend on the interests of any single company.</li> <li>Business continuity is significantly simplified for our customers.</li> <li>We have better chances at influencing projects in a direction that is useful to us and our customers. The smaller the project, the easier to influence.</li> </ul>","tags":["ISO 27001 A.17.1.1 Planning Information Security Continuity"]},{"location":"adr/0015-we-believe-in-community-driven-open-source/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Sometimes we might need to give up \"that cool new feature\" until the community-driven open source solution catches up with their closed source or single-company open source alternative. Alternatively, we might need to put extra time and effort to develop \"that cool new feature\" ourselves.</li> <li>As they are not bound by vendor liability -- e.g., end-of-life promises -- community-driven projects present a greater risk of being abandoned. The smaller the project, the higher the risk.</li> </ul>","tags":["ISO 27001 A.17.1.1 Planning Information Security Continuity"]},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/","title":"gid=0 is okay, but not by default","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cristian, Lars, Olle</li> <li>Date: 2021-08-23</li> </ul>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>OpenShift likes to shift (pun intended) the UID -- i.e., assign arbitrary UIDs -- to containers. They do this as an additional security feature, given that OpenShift is a multi-tentant Kubernetes solution. Each OpenShift project received a non-overlapping UID range. Hence, in case an attacker escapes a container, it will be more difficult to interfere with other processes.</p> <p>However, this shifting of UIDs introduces an additional complexity: What if a process wants to write to the filesystem? What uid, gid and permissions should the files and folders have? To solve this problem, the OpenShift documentation (see \"Support arbitrary user ids\") recommends setting gid=0 on those files and folders. Specifically, the Dockerfiles of the container images should contain:</p> <pre><code>RUN chgrp -R 0 /some/directory &amp;&amp; chmod -R g=u /some/directory\n</code></pre> <p>During execution, OpenShift assigns <code>gid=0</code> as a supplementary group to containers, so as to give them access to the required files.</p> <p>In contrast to OpenShift, Compliant Kubernetes is not a multi-tenant solution. Given previous vulnerabilities in Kubernetes that affected tenant isolation (e.g., CVE-2020-8554 ), we believe that non-trusting users should not share a workload cluster. Hence, we do not assign arbitrary UIDs to containers and do not need to assign <code>gid=0</code> as a supplementary group.</p> <p>The <code>gid=0</code> practice above seems to have made its way in quite a few Dockerfiles, however, it is far from being the default outside OpenShift.</p> <p>What should Compliant Kubernetes do with the <code>gid=0</code> practice?</p>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>For user expectations, we want to make it easy to start with Compliant Kubernetes.</li> <li>For better security and easier audits, we do not want to add unnecessary permissions.</li> <li>ID mapping in mounts has landed in Linux 5.12. Once this feature is used in container runtimes and Kubernetes, the <code>gid=0</code> problem will go away.</li> </ul>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#considered-options","title":"Considered Options","text":"<ul> <li>Allow <code>gid=0</code> by default.</li> <li>Disallow <code>gid=0</code> by default -- this is what Kubespray does.</li> <li>Never allow <code>gid=0</code>.</li> </ul>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"disallow <code>gid=0</code> by default\". Enabling it on a case-by-case basis is okay.</p>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We do not unnecessarily add a permission to containers.</li> </ul>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Some users will complain about their container images not starting, and we will need to add a less restricted PodSecurityPolicy in their cluster.</li> </ul>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#other-considerations","title":"Other Considerations","text":"<p>PodSecurityPolicies are deprecated in favor of PodSecurity Admission. This decision will have to be revisited once PodSecurity Admission is stable.</p> <p>In case we notice that the <code>gid=0</code> practice is gaining significant uptake, we will have to revisit this decision to allow <code>gid=0</code> by default.</p> <p>In case ID mapping is implemented in container runtimes and Kubernetes, this problem will likely go away. In that case, this decision might be revisited to never allow <code>gid=0</code>.</p>"},{"location":"adr/0017-persist-dex/","title":"Persist Dex","text":"<ul> <li>Status: accepted</li> <li>Deciders: Compliant Kubernetes Architecture Meeting</li> <li>Date: 2021-11-16</li> </ul> <p>Technical Story: Enable Dex persistence</p>"},{"location":"adr/0017-persist-dex/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Dex requires persisting state to perform various tasks such as track refresh tokens, preventing replays, and rotating keys.</p> <p>What persistence option should we use?</p>"},{"location":"adr/0017-persist-dex/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>CRDs add complexity</li> <li>Storage adds complexity</li> <li>We want to frequently reboot Nodes for security patching</li> <li>We want to deliver excellent user experience</li> </ul>"},{"location":"adr/0017-persist-dex/#considered-options","title":"Considered Options","text":"<ul> <li>Use \"memory\" storage</li> <li>Use CRD-based storage</li> </ul>"},{"location":"adr/0017-persist-dex/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"use CRD-based storage\", because it improves user experience when Nodes are rebooted.</p> <p>With \"memory\" storage, Dex loses the OpenID keys when restarted, which leads to the user being forced to eventually re-login. Worst off, this forced re-login happens unexpectedly from the user's perspective, when the Kubernetes apiserver chooses to refresh the OpenID keys.</p> <p>Here is the experiment to illustrate the issue:</p> <pre><code>$ curl https://dex.$DOMAIN/.well-known/openid-configuration &gt; before-openid-configuration.json\n$ curl https://dex.$DOMAIN/keys &gt; before-keys.json\n\n$ kubectl delete pods -n dex -l app.kubernetes.io/instance=dex\n\n$ curl https://dex.$DOMAIN/.well-known/openid-configuration &gt; after-openid-configuration.json\n$ curl https://dex.$DOMAIN/keys &gt; after-keys.json\n\n$ diff -y before-openid-configuration.json after-openid-configuration.json\n[empty output, no differences]\n\n$ diff -y before-keys.json after-keys.json\n[all keys are replaced]\n</code></pre>"},{"location":"adr/0017-persist-dex/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Nodes which host Dex can be rebooted for security patching</li> <li>User experience is optimized</li> </ul>"},{"location":"adr/0017-persist-dex/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Dex will have a more permissions in the Service Cluster (see <code>rbac.yaml</code>)</li> <li>We will need to closely monitor migration steps for Dex</li> </ul>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/","title":"Use Probe to Measure Uptime of Internal Compliant Kubernetes Services","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cristian, Lucian, Ravi</li> <li>Date: 2021-11-25</li> </ul>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We need to measure uptime for at least two reasons:</p> <ol> <li>To serve as feedback on what needs to be improved next.</li> <li>To demonstrate compliance with our SLAs.</li> </ol> <p>How exactly should we measure uptime?</p>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to reduce tools sprawl.</li> <li>We want to be mindful about capacity and infrastructure costs.</li> <li>We want to measure uptime as observed by a consumer -- i.e., application or user -- taking into account business continuity measures, such as redundancy, fail-over time, etc.</li> </ul>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#considered-options","title":"Considered Options","text":"<ul> <li>Blackbox exporter</li> <li>kubelet prober metrics</li> <li>Prometheus Operator Probe, which essentially wraps the Blackbox exporter in a <code>Probe</code> CustomResource.</li> </ul>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"use Probe for measuring uptime of internal Compliant Kubernetes services\", because it measures uptime as observed by a consumer. Although this requires a bit of extra capacity for running Blackbox, the costs are worth the benefits.</p> <p>Instead of configuring Blackbox directly, <code>Probe</code> is a cleaner abstraction provided by the Prometheus Operator.</p> <p>The following is an example for a Probe:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Probe\nmetadata:\nname: google-is-up\nlabels:\nprobe: google\nrelease: kube-prometheus-stack\nspec:\ninterval: 60s\nmodule: http_2xx\nprober:\nurl: blackbox-prometheus-blackbox-exporter.monitoring.svc.cluster.local:9115\ntargets:\nstaticConfig:\nstatic:\n- https://www.google.com\n</code></pre> <p>This will generate a metric as follows: <code>probe_success{cluster=\"ckdemo-wc\", instance=\"https://www.google.com\", job=\"probe/demo1/google-is-up\", namespace=\"demo1\"}</code>.</p>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We measure uptime as observed by a consumer.</li> <li>Increasing redundancy, reducing failure time, etc. will contribute positively to our uptime, as desired.</li> </ul>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We don't currently run Blackbox in the workload cluster, so we'll need a bit of extra capacity.</li> </ul>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#recommendations-to-operators","title":"Recommendations to Operators","text":"<p>Blackbox should only be used for measuring uptime of internal services, i.e., those that are only exposed within the Kubernetes cluster. Examples include additional services, such as PostgreSQL, Redis and RabbitMQ.</p> <p>For external endpoints -- specifically, Dex, Grafana, Kibana, Harbor and Ingress Controllers -- prefer using an external uptime service which integrates with an On-Call Management Tool, e.g., Uptime Cloud Monitor Integration for Opsgenie.</p> <p>External uptime measurement should achieve the similar effect as the commands below:</p> <pre><code>curl --head https://dex.$DOMAIN/healthz\ncurl --include https://harbor.$DOMAIN/api/v2.0/health\ncurl --head https://grafana.$DOMAIN/healthz\ncurl --head https://kibana.$DOMAIN/api/status\n\ncurl --head some-domain.$DOMAIN/healthz  # Pokes the WC Ingress Controller\n</code></pre>"},{"location":"adr/0019-push-metrics-via-thanos/","title":"Push Metrics via Thanos","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-01-20</li> </ul>"},{"location":"adr/0019-push-metrics-via-thanos/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Currently, the service cluster exposes several end-points for workload clusters:</p> <ul> <li>Dex, for authentication;</li> <li>OpenSearch, for pushing logs (append-only);</li> <li>InfluxDB, for pusing metrics;</li> <li>Harbor, for pulling container images.</li> </ul> <p>InfluxDB has served us really well over the years. However, as we enter a new era of growth, it no longer satisfies our needs. In particular:</p> <ul> <li>It is not community-driven (see ADR-0015 We believe in community-driven open source).</li> <li>The open-source version cannot be run replicated, hence it is a single point of failure.</li> <li>It is rather capacity hungry, eating as much as 2 CPUs and 15 Gi in a standard package environment.</li> <li>It is unsuitable for long-term metrics storage, which we need -- among others -- for proper capacity management.</li> </ul> <p>We decided to migrate from InfluxDB to Thanos, which can both push and pull metrics.</p> <p>Shall we push or pull metrics using Thanos?</p>"},{"location":"adr/0019-push-metrics-via-thanos/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to support multiple workload clusters.</li> <li>We want to untangle the life-cycle of the service cluster and workload cluster.</li> <li>The service cluster acts as a tamper-proof logging environment, hence it should be difficult to tamper with metrics from the workload cluster.</li> </ul>"},{"location":"adr/0019-push-metrics-via-thanos/#considered-options","title":"Considered Options","text":"<ol> <li>Push metrics from workload cluster to service cluster.</li> <li>Pull metrics from workload cluster to service cluster.</li> </ol>"},{"location":"adr/0019-push-metrics-via-thanos/#decision-outcome","title":"Decision Outcome","text":"<p>We chose to push metrics from the workload cluster to the service cluster via via Thanos Receive, because it keeps the \"direction\" of metrics flow. Hence, we keep support for multiple workload clusters without any changes.</p> <p>At the time of this writing, pulling metrics via Thanos sidecar seems to be the preferred way to deploy Thanos. We will monitor the ecosystem and our needs, and -- if needed -- move to pulling metrics.</p> <p>At any rate, even if we end up with Thanos Sidecar, migrating in two steps -- first from InfluxDB to Thanos Receive, then to Thanos Sidecar -- feels less risky.</p>"},{"location":"adr/0019-push-metrics-via-thanos/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>All of <code>*.$opsDomain</code> can point to the service cluster workers -- optionally fronted by a load-balancer -- which considerably simplifies setup.</li> <li>Multiple workload clusters can push metrics to the service cluster, which paves the path to workload multi-tenancy.</li> <li>The service cluster can be set up first, followed by one-or-more workload clusters.</li> <li>Workload clusters become more \"cattle\"-ish.</li> </ul>"},{"location":"adr/0019-push-metrics-via-thanos/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Metrics are less protected than in a pull architecture. E.g., compromising the workload cluster can easier be used to mount an attack against long-term metrics storage.</li> </ul>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/","title":"Filter by cluster label then data source","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2021-01-27</li> </ul> <p>Technical Story: https://github.com/elastisys/compliantkubernetes-apps/issues/742</p>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Compliant Kubernetes allows multiple workload clusters to be connected to a single service cluster. This allows the metrics of multiple workload clusters to be inspected via the same dashboards.</p> <p>How should we organise metrics to allow users and admins to select for which clusters they want to see metrics?</p>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to be able to see metrics for a single cluster, for multiple cluster, and even for all clusters.</li> <li>We want to be able to reuse upstream dashboards, and some are missing filters for the <code>cluster</code> variable.</li> <li>We want to stay flexible.</li> </ul>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/#considered-options","title":"Considered Options","text":"<ul> <li>Use only the <code>cluster</code> label and expose a single data source.</li> <li>Expose multiple data sources and ignore the <code>cluster</code> label.</li> <li>Filter primarily by <code>cluster</code> label, but allow filtering by data source.</li> <li>Filter primarily by data source, but allow filtering by <code>cluster</code> label.</li> </ul>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Filter primarily by <code>cluster</code> label, but allow filtering by data source\", because it fulfills the all decision drivers with little complexity.</p> <p>Prom-label-enforcer can be used to create multiple data sources from a single data store, discriminating by <code>cluster</code> label. To simplify Thanos configuration, we can also discriminate based on <code>tenant_id</code>, which will always contain the same value as <code>cluster</code>.</p> <p>In general, we will aim to fix dashboards missing the <code>cluster</code> variable upstream. However, by also providing filtering based on data source, we facilitate our users to reuse their dashboards, which might not be cluster-aware.</p>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We support both dashboards with <code>cluster</code> filter and without</li> <li>We can enforce metrics multi-tenancy, i.e., map Grafana users/orgs to datasources, to filter some metrics out.</li> </ul>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>[Minor] We need to configure data sources in <code>sc-config.yaml</code></li> <li>For example, if we forget to add the name of a workload cluster, the data source will be missing, but filtering based on <code>cluster</code> label is still possible.</li> <li>[Minor] Label enforcer uses a bit of resources.</li> <li>However, we already saved a lot by migrating from InfluxDB to Thanos, so we can afford go back a bit.</li> </ul>"},{"location":"adr/0021-tls-for-additional-services/","title":"Default to TLS for performance-insensitive additional services","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-02-16</li> </ul>"},{"location":"adr/0021-tls-for-additional-services/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We run additional services in the workload cluster, currently databases (PostgreSQL), in-memory caches (Redis) and message queues (RabbitMQ). Traditionally, when these services are provided as managed services, they are exposed via a TLS-encrypted endpoint. See examples for:</p> <ul> <li>Redis -- notice <code>rediss://</code>;</li> <li>RabbitMQ;</li> <li>PostgreSQL.</li> </ul> <p>In Compliant Kubernetes, the network is assumed trusted, either because we performed a provider audit or because we enabled Pod-to-Pod encryption via WireGuard. Hence, TLS does not improve data security.</p> <p>How should we expose additional services in Compliant Kubernetes? With or without TLS?</p>"},{"location":"adr/0021-tls-for-additional-services/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to stick to best practices and sane defaults.</li> <li>We want to make it easy to port applications to Compliant Kubernetes and its additional services.</li> <li>Some services are performance-sensitive: Redis suffers a significant performance drop with TLS.</li> <li>The Spotahome Redis Operator does not support TLS.</li> <li>Some services are performance-insensitive: PostgreSQL and RabbitMQ feature negligible performance impact with TLS.</li> </ul>"},{"location":"adr/0021-tls-for-additional-services/#considered-options","title":"Considered Options","text":"<ul> <li>Always disable TLS, since the network in Compliant Kubernetes is trusted.</li> <li>Always enable TLS.</li> <li>By default, enable TLS for performance-insensitive services and disable TLS for performance-sensitive services. Allow TLS to be disabled if the user requests it.</li> </ul>"},{"location":"adr/0021-tls-for-additional-services/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"By default, enable TLS for performance-insensitive services and disable TLS for performance-sensitive services. Allow TLS to be disabled if the user requests it.\"</p> <p>Specifically:</p> <ul> <li>Never enable TLS for Redis: Performance impact is huge and the network is already trusted. Furthermore, the Spotahome Redis Operator does not support TLS.</li> <li>Enable TLS by default for PostgreSQL and RabbitMQ: Performance impact is negligible and most application are already configured for it.</li> <li>Allow TLS to be disabled if requested for PostgreSQL and RabbitMQ.</li> </ul>"},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/","title":"Use Dedicated Nodes for Additional Services","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-03-03</li> <li>Updated: 2023-01-12</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A14"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We run additional services in the workload cluster, currently databases (PostgreSQL), in-memory caches (Redis), message queues (RabbitMQ) and distributed tracing (Jaeger).</p> <p>On what Nodes should they run?</p>","tags":["BSI IT-Grundschutz APP.4.4.A14"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to deliver a stable and secure platform.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A14"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#considered-options","title":"Considered Options","text":"<ul> <li>Spread additional services on application Nodes.</li> <li>Run additional services on dedicated Nodes.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A14"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"run additional service on dedicated Nodes\", because it improves the stability and security of the platform.</p> <p>Specifically, use the following Node labels</p> <pre><code>elastisys.io/node-type=postgresql\nelastisys.io/node-type=redis\nelastisys.io/node-type=rabbitmq\nelastisys.io/node-type=jaegertracing\n</code></pre> <p>and taints:</p> <pre><code>elastisys.io/node-type=postgresql:NoSchedule\nelastisys.io/node-type=redis:NoSchedule\nelastisys.io/node-type=rabbitmq:NoSchedule\nelastisys.io/node-type=jaegertracing:NoSchedule\n</code></pre> <p>Important</p> <p>Dedicated Nodes still contain some workload cluster components for logging, monitoring, intrusion detection, etc., so not all their capacity is available to the service.</p>","tags":["BSI IT-Grundschutz APP.4.4.A14"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Performance is more predictable.</li> <li>Responsibility is more clearly separated, i.e., application Nodes vs. additional services Nodes.</li> <li>Security and stability of additional services is somewhat improved, e.g., SystemOOM due to an application won't impact PostgreSQL.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A14"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Forces additional services to be sized based on available Node sizes. While some commonality exists, Node sizes are specific to each infrastructure provider.</li> <li>Latency is somewhat increased. This is an issue mostly for Redis, as other services are a bit more latency tolerant.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A14"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#recommendations-to-operators","title":"Recommendations to Operators","text":"<p>For better application performance and security, run system Deployments and StatefulSets -- such as Ingress Controllers, Prometheus, Velero, Gatekeeper and Starboard -- onto dedicated Nodes.</p> <p>Specifically, use the following Node label:</p> <pre><code>elastisys.io/node-type=elastisys\n</code></pre> <p>and taint:</p> <pre><code>elastisys.io/node-type=elastisys:NoSchedule\n</code></pre> <p>Remember to also add tolerations and Node affinity to all affected Pods.</p>","tags":["BSI IT-Grundschutz APP.4.4.A14"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#links","title":"Links","text":"<ul> <li>Taints and Tolerations</li> <li>Well-Known Labels, Annotations and Taints</li> <li>Kubespray <code>node_labels</code> and <code>node_taints</code></li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A14"]},{"location":"adr/0023-allow-snippets-annotations/","title":"Only allow Ingress Configuration Snippet Annotations after Proper Risk Acceptance","text":"<ul> <li>Status: accepted</li> <li>Deciders: architecture meeting</li> <li>Date: 2022-08-11</li> </ul>"},{"location":"adr/0023-allow-snippets-annotations/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Configuration snippet annotations are a powerful tool to allow injecting any kind of Nginx configuration into the Nginx Ingress Controller. For example, it allows things such as header renaming, custom authentication, etc.</p> <p>However, with great power comes great responsibility. Configuration snippet may break the Ingress Controller and cause downtime for all applications hosted in the workload cluster. Also, it opens up CVE-2021-25742, which means that application developers can exfiltrate all Secrets in the workload cluster.</p> <p>How shall we best serve application developers without compromising platform stability and security?</p>"},{"location":"adr/0023-allow-snippets-annotations/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to best serve application developers.</li> <li>We want to ensure platform stability and security.</li> </ul>"},{"location":"adr/0023-allow-snippets-annotations/#considered-options","title":"Considered Options","text":"<ul> <li>Allow <code>the use of \"config-snippets annotations\" with Ingress</code> by default.</li> <li>Disallow <code>the use of \"config-snippets annotations\" with Ingress</code> by default.</li> <li>Never allow <code>the use of \"config-snippets annotations\" with Ingress</code>.</li> <li>Allow <code>the use of \"config-snippets annotations\" with Ingress</code>, but only after application developer accepted the downtime and security risks.</li> </ul>"},{"location":"adr/0023-allow-snippets-annotations/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: Allow <code>the use of \"config-snippets annotations\" with Ingress</code>, but only after application developer accepted the downtime and security risks.</p>"},{"location":"adr/0023-allow-snippets-annotations/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Several use-cases commonly requested by application developers can be satisfied.</li> </ul>"},{"location":"adr/0023-allow-snippets-annotations/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Platform security is at a small risk if this feature is misused by application developers.</li> <li>Platform stability is at a small risk if this feature is misused by application developers.</li> </ul>"},{"location":"adr/0023-allow-snippets-annotations/#recommendation-to-operators","title":"Recommendation to Operators","text":"<p>If you enable this feature, then make sure application developers understand and accept the added stability and security risks. A message as follows could be used: <pre><code>Hello!\n\nAfter talking with the team, we have decided that it is okay to enable the `nginx.ingress.kubernetes.io/configuration-snippet` annotation provided that:\n\n(a) there is no other way for you to move forward;\n(b) you understand and are willing to accept the security risks;\n(c) you are okay to take responsibility for downtime caused by misconfiguration; and\n(d) you are okay to take responsibility for updating the annotation as required.\n\nFor (a), please confirm that you checked that you cannot use other annotations instead for your use cases. https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/\n\nFor (b), please confirm you are aware and understand the consequences of this CVE. https://github.com/kubernetes/ingress-nginx/issues/7837.\n\nFor (c), please confirm that you understand that this annotation is quite powerful, meaning that misconfiguration can lead to downtime for all your Ingress resources. Obviously, if Nginx goes down due to any custom configuration, then we cannot take responsibility for that.\n\nFor (d), please confirm that you are okay to take responsibility for making sure that the custom configuration is supported in newer versions of Nginx, as we sometimes upgrade Nginx. Both our release notes and calendar invites for the maintenance windows mention if we are upgrading Nginx.\n\nTo sum up, if you can confirm (a)-(d) then I can enable the `nginx.ingress.kubernetes.io/configuration-snippet` annotation.\n\nRegards,\n</code></pre></p>"},{"location":"adr/0024-allow-Harbor-robot-account/","title":"Allow a Harbor robot account that can create other robot accounts with full privileges","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-11-17</li> </ul>"},{"location":"adr/0024-allow-Harbor-robot-account/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We offer UI access to Harbor with admin privileges. Customer uses a Harbor operator that needs an admin robot account with privileges to create other robot accounts with full privileges. Should we allow creation of a Harbor robot account which can create other Harbor robot accounts?</p>"},{"location":"adr/0024-allow-Harbor-robot-account/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to deliver a platform that is easy to use and easy to automate.</li> <li>We want to ensure platform security and stability.</li> <li>We want to make it hard for customers to break the platform via trivial mistakes.</li> </ul>"},{"location":"adr/0024-allow-Harbor-robot-account/#considered-options","title":"Considered Options","text":"<ul> <li>Do not allow Harbor robot account which can create Harbor robot accounts.</li> <li>Allow Harbor robot accounts which can create Harbor robot accounts</li> </ul>"},{"location":"adr/0024-allow-Harbor-robot-account/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Allow Harbor robot accounts which can create Harbor robot accounts\", because it does not provide additional privileges, but instead offers a more self-service platform.</p>"},{"location":"adr/0024-allow-Harbor-robot-account/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Harbor is more flexible and easy to automate</li> </ul>"},{"location":"adr/0024-allow-Harbor-robot-account/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Increases the chances that the customer can cripple the Harbor service.</li> </ul>"},{"location":"adr/0024-allow-Harbor-robot-account/#recommendations-to-operators","title":"Recommendations to Operators","text":"<p>Make it clear in the ticket requesting this that the customer accepts the risk of \"shooting themselves in the foot\"</p>"},{"location":"adr/0025-local-storage/","title":"Use local-volume-provisioner for Managed Services that requires high-speed disks.","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-11-10</li> </ul>"},{"location":"adr/0025-local-storage/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>After performing several storage load testing and PostgreSQL load testing and benchmarking we have discovered that the local storage is significantly faster than network storage. We have one use case where the disk have proven to be too slow for a PostgreSQL database and the performance was not as expected. How should we expose local storage to Managed Services?</p>"},{"location":"adr/0025-local-storage/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to best serve our customer needs.</li> <li>We want to offer fast performant Managed Services.</li> <li>We want to find a solution which is scalable and minimizes administrator burden.</li> <li>We want to find a future-proof solution, which exposes local disks to any application.</li> </ul>"},{"location":"adr/0025-local-storage/#considered-options","title":"Considered Options","text":"<ul> <li>Use the fastest network storage with dedicated IOPS.</li> <li>Use local storage with local-volume-provisioner.</li> <li>Use local storage with local-path-provisioner</li> <li>Use hostPath</li> <li>Use local</li> </ul>"},{"location":"adr/0025-local-storage/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: Use local storage with local-volume-provisioner and move the code within the kubespray repo.</p>"},{"location":"adr/0025-local-storage/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Services using the local storage are performing better.</li> <li>We are able to provide a PostgreSQL service that meet the high performance requirements.</li> </ul>"},{"location":"adr/0025-local-storage/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Scaling the storage becomes harder as it will involve replacing the nodes.</li> <li>We are limited by the size of the volumes that are available within the cloud provider offering.</li> </ul>"},{"location":"adr/0025-local-storage/#recommendation-to-operators","title":"Recommendation to Operators","text":"<p>When using the local-volume-provisioner please create dedicated partitions and make sure to reserve enough space for the boot partition. Failing to do so can lead to entire disc to become full and the node will become unresponsive and crash.</p>"},{"location":"adr/0025-local-storage/#links","title":"Links","text":"<ul> <li>local-volume-provisioner</li> <li>local-path-provisioner</li> <li>hostPath</li> <li>local</li> </ul>"},{"location":"adr/0026-hnc/","title":"Use <code>environment-name</code> as the default root of Hierarchical Namespace Controller (HNC)","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-09-29</li> </ul>"},{"location":"adr/0026-hnc/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>As of apps v0.25 the HNC namespaces are available. Should we allow the root of the HNC to be configurable? What default value should it have?</p>"},{"location":"adr/0026-hnc/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to reduce the number of tickets related to namespace creation.</li> <li>We want to make the operator life easier.</li> <li>HNC recommends against using default, as the user could edit the kubernetes svc in the default namespace.</li> </ul>"},{"location":"adr/0026-hnc/#considered-options","title":"Considered Options","text":"<ul> <li>Hard-code the root of HNC to <code>default</code></li> <li>Hard-code the root of HNC to a value other than <code>default</code></li> <li>Allow the root of HNC to be configured, but provide a default value</li> </ul>"},{"location":"adr/0026-hnc/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options: Allow the root of Hierarchical Namespace to be configured, but provide a default value. The default value is the <code>environment-name</code>. If a different value was previously used, then no migration is needed.</p>"},{"location":"adr/0026-hnc/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We make the operator life easier by not having to handle namespace requests tickets</li> <li>Operator does not have to do the migration</li> <li>We offer flexibility to our customers in choosing if they want to keep existing non-HNC namespaces and from further on use the new HNC namespace to create new namespaces on their own.</li> <li>Increase customer autonomy</li> </ul>"},{"location":"adr/0026-hnc/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Some snowflakiness will exist by keeping both the non-HNC and HNC namespaces</li> <li>Some customers might get confused at the beginning by not knowing which namespaces are HNC and which are non-HNC.</li> </ul>"},{"location":"adr/0026-hnc/#recommendation-to-operators","title":"Recommendation to Operators","text":"<p>Do not use the \"default\" namespace as the name of the default root HNC namespace because the user could edit the kubernetes svc in the default namespace. Exclude the core and the AMS namespaces from HNC as we do not want those to be managed by HNC. Do not migrate customer to the new default root Hierarchical Namespace, but do inform the customers that they have the choice to migrate to it, if desired.</p>"},{"location":"adr/0026-hnc/#links","title":"Links","text":"<ul> <li>HNC repo</li> <li>HNC</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/","title":"PostgreSQL - Enable external replication","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-10-27</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We have received a few requests from customers to enable external replication on PostgreSQL so that they are able to create copies of the PostgreSQL cluster themselves to use for testing, development, tuning and disaster recovery purposes. Should we allow and enable external replication for PostgreSQL, or should we offer an alternative that can mimic that?</p>"},{"location":"adr/0027-postgresql-external-replication/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to best serve our customer needs.</li> <li>We want to make the operator life easier.</li> <li>We want to ensure platform security and stability.</li> <li>We want to make it hard for customers to break the platform via trivial mistakes.</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#considered-options","title":"Considered Options","text":"<ul> <li>Allow external replication on PostgreSQL.</li> <li>Do not allow external replication on PostgreSQL.</li> <li>Allow read access to the S3 bucket containing the files that can mimic the replication.</li> <li>Clone the S3 bucket containing the files using rclone to another bucket in a new project preferably owned by the customer and from there the customer is free to use it.</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options: Clone the S3 bucket containing the files using rclone to another bucket in a new project preferably owned by the customer and from there the customer is free to use it. We rclone the backups (once per night) in this new S3 bucket. We provide no SLA on the rclone job.</p> <p>The option <code>Allow external replication on PostgreSQL</code> is putting the platform stability and integrity at risk, because external replication is done via the replication slots and if the destination of the replication is unreachable or stopped, then the WAL files are kept on disc until the files are sent to destination and confirmed to be received. This means that the WAL files will pile up until the cluster runs out of space and crashes. This also leads to data loss and data corruption.</p> <p>On cloud providers that do not have native S3 support with ACL capabilities we need endpoint/credentials from the user to a S3 bucket of their choosing that we can rclone to. We rclone the backups (once per night) in this new S3 bucket and provide credentials to the customer. We provide no SLA on the rclone job. The diagram of the solution looks like this: </p>"},{"location":"adr/0027-postgresql-external-replication/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We make the operator life easier by offering them a possibility to clone/replicate their PostgreSQL cluster from the S3 bucket containing the basebackup and WAL files.</li> <li>Operator can now use 3rd party tools that can pull the basebackup and WAL files and clone the PostgreSQL cluster in another location.</li> <li>Increase customer autonomy</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>For providers that do not offer S3 with ACL, we need to create and maintain an rclone job that copies the files from the initial S3 backup bucket to a new S3 bucket (preferably owned by the customer)</li> <li>When rclone is involved, the time to recover will be up to 24 hours old as we will rclone once per day.</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#recommendation-to-operators","title":"Recommendation to Operators","text":"<p>On providers that offer S3 with ACL, this can be done with small effort. On providers that do not offer S3 with ACL we need to clone the bucket containing the file to another S3 bucket that is preferably owned by the customer. This is done with no SLA on it. Offer this only on request.</p>"},{"location":"adr/0027-postgresql-external-replication/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0027-postgresql-external-replication/#option-1-allow-external-replication-on-postgresql","title":"[option 1] - Allow external replication on PostgreSQL.","text":"<ul> <li>Good, because we satisfy the customer need</li> <li>Bad, because it comes with a big risk of breaking the PostgreSQL cluster and cause downtime.</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#option-2-do-not-allow-external-replication-on-postgresql","title":"[option 2] - Do not allow external replication on PostgreSQL","text":"<ul> <li>Good, because we do not have to do any changes/work.</li> <li>Good, because the integrity and stability of the platform is kept intact.</li> <li>Bad, because we are not flexible and do not try to satisfy the customer need.</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#option-3-allow-read-access-to-the-s3-bucket-containing-the-files-that-can-mimic-the-replication","title":"[option 3] - Allow read access to the S3 bucket containing the files that can mimic the replication.","text":"<ul> <li>Good, we can satisfy the custmer need and demonstrate we are flexible and customer oriented.</li> <li>Good, because we keep the platform stability and integrity intact.</li> <li>Bad, because it involves extra work and some snowflakiness.</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#links","title":"Links","text":"<ul> <li>PostgreSQL replication</li> <li>Streaming ReplicationHNC</li> </ul>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/","title":"Harder pod eviction when nodes are going OOM","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-12-08</li> </ul>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We had several incidents where nodes are going OOM and become unresponsive when too many pods are being scheduled on them. This also leads to pods getting evicted, and this applies to our pods that are responsible for the platform security and compliance. Should we enable kubelet hard eviction so as to behold the vital components?</p>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to ensure platform security and stability.</li> <li>We want to make it hard for customers to break the platform via trivial mistakes.</li> <li>We want to make sure that the critical components are not evicted when nodes are overcommitted.</li> </ul>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#considered-options","title":"Considered Options","text":"<ul> <li>Enable kubelet hard eviction.</li> <li>Adjust OOM score so that kernel does not OOM critical Pods</li> <li>Setup priority class for all our apps</li> <li>Do not make any changes and reinforce the responsibility to the custmer for not overloading the nodes.</li> </ul>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options: <code>Enable kubelet hard eviction</code> &amp; <code>Setup priority class for all our apps</code> .</p> <p>The option <code>Adjust OOM score so that kernel does not OOM critical Pods</code> is  not viable as it can be used only if we set requests=limits on all our pods (see here and here), and this will not allow us to benefit from the burstable capabilities of kubernetes and have a static allocation of resources which locks the resources on the nodes even if they are not used most of the time. This reduces the available resources for the customer on the nodes.</p>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Security and stability of the platform is somewhat improved.</li> </ul>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Customer pods will be stuck in pending until resources are available and this might make the customer feel that their pods are less important.</li> <li>kubectl may not observe pressure right away</li> </ul>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#recommendation-to-operators","title":"Recommendation to Operators","text":"<p>Test multiple comfigurations using kubelet hard eviction, priotiry classes and other option to obtain the desired behaviour where the nodes do not become unresponsive and our components are not getting evicted when the node is overcommitted by the customer.</p>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/","title":"Run ArgoCD on the Elastisys nodes","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2023-01-26</li> </ul>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We run additional services in the workload cluster on dedicated nodes, currently databases (PostgreSQL), in-memory caches (Redis), message queues (RabbitMQ) and distributed tracing (Jaeger). Where should we run ArgoCD?</p>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to deliver a stable and secure platform.</li> <li>We want to deliver an affordable adittional managed service and avoid resource waste.</li> <li>ArgoCD has a small footprint, e.g., 200m CPU and 0.5GB RAM for one of our largest deployments.</li> <li>Want want to keep application Nodes \"light\", so the application team knows what capacity is available for their application.</li> </ul>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#considered-options","title":"Considered Options","text":"<ul> <li>Spread ArgoCD services on application Nodes.</li> <li>Run ArgoCD services on dedicated Nodes.</li> <li>Run ArgoCD services on Elastisys Nodes, and scale up the nodes.</li> </ul>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Run ArgoCD services on Elastisys Nodes, and scale up the nodes.\",  because it improves the stability and security of the platform, avoids resource waste and makes the ArgoCD service more affordable to our customers.</p> <p>Scale up the Elastisys Nodes to 4C8GB before installing managed ArgoCD.</p>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>The ArgoCD service infrastructure footprint is lower than when using dedicated nodes, due to less per-Node overhead (fluentd, Falco)..</li> <li>Security and stability of additional services is somewhat improved, e.g., SystemOOM due to an application won't impact ArgoCD</li> </ul>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We need to scale up the Elastisys nodes to 4C8GB</li> <li>We are sharing the Elastisys nodes resources with the other Elastisys platform components, e.g., Ingress Controller.</li> </ul>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#recommendations-to-operators","title":"Recommendations to Operators","text":"<p>Specifically, use the following Node labels</p> <pre><code>elastisys.io/node-type=elastisys\n</code></pre> <p>and taint:</p> <pre><code>elastisys.io/node-type=elastisys:NoSchedule\n</code></pre> <p>Remember to also add tolerations and Node affinity to all affected Pods.</p>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#links","title":"Links","text":"<ul> <li>Taints and Tolerations</li> <li>Well-Known Labels, Annotations and Taints</li> <li>Kubespray <code>node_labels</code> and <code>node_taints</code></li> </ul>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/","title":"Run csi-cinder-controllerplugin on the Elastisys nodes","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2023-01-26</li> </ul>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We use the Cinder CSI Driver to manage the lifecycle of OpenStack Cinder Volumes. Currently the csi-cinder-controllerplugin is running on arbitrary nodes. Where should we run the csi-cinder-controllerplugin?</p>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to deliver a stable and secure platform.</li> <li>Want want to keep application Nodes \"light\", so the application team knows what capacity is available for their application.</li> </ul>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#considered-options","title":"Considered Options","text":"<ul> <li>Run the csi-cinder-controllerplugin on arbitrary Nodes.</li> <li>Run the csi-cinder-controllerplugin on Elastisys Nodes.</li> </ul>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Run the csi-cinder-controllerplugin on Elastisys Nodes\",  because it improves the stability and security of the platform and makes the application nodes \"light\"</p>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>The application Nodes infrastructure footprint is lower.</li> <li>Security and stability of additional services is somewhat improved, e.g., SystemOOM due to an application won't impact the csi-cinder-controllerplugin.</li> </ul>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We need to change the code to be able to make csi-cinder-controllerplugin run on the Elastisys Nodes.</li> </ul>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#recommendations-to-operators","title":"Recommendations to Operators","text":"<p>Specifically, use the following Node labels</p> <pre><code>elastisys.io/node-type=elastisys\n</code></pre> <p>and taint:</p> <pre><code>elastisys.io/node-type=elastisys:NoSchedule\n</code></pre> <p>Remember to also add tolerations and Node affinity to all affected Pods.</p>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#links","title":"Links","text":"<ul> <li>Taints and Tolerations</li> <li>Well-Known Labels, Annotations and Taints</li> <li>Kubespray <code>node_labels</code> and <code>node_taints</code></li> </ul>"},{"location":"adr/0032-boot-disk-size/","title":"Boot disk size on nodes","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cristian Klein, Olle Larsson, Lucian Vlad, Fredrik Liv, Robin Wallace, Pavan Gunda</li> <li>Date: 2023-02-14</li> </ul>"},{"location":"adr/0032-boot-disk-size/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We have often defaulted to using boot disks of 50GB where possible but as of late we have noticed that for some environments this is not sufficient. We also noticed that the available space is commonly filled up by customer container images. We would like to have same boot disk sizes for all our nodes on all the cloud providers if possible. Should we increase the boot disk size to a bigger size?</p>"},{"location":"adr/0032-boot-disk-size/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to best serve our customer needs.</li> <li>We want to find a solution which is scalable and minimizes administrator burden.</li> <li>We don't want to waste infrastructure.</li> </ul>"},{"location":"adr/0032-boot-disk-size/#considered-options","title":"Considered Options","text":"<ul> <li>Keep boot disk size to 50Gb but increase the alert threshold and do regular cleanup</li> <li>Keep boot disk size to 50Gb and increase boot disk size only where is needed</li> <li>Increase the boot disk size to 100GB for all nodes irrespective of node size</li> <li>Increase the boot disk size to 100GB only for nodes that are bigger than 4C8GB</li> <li>Use local disk of 100GB size for controlplane nodes</li> </ul>"},{"location":"adr/0032-boot-disk-size/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: Increase the boot disk size to 100GB for all nodes irrespective of node size &amp; Use local disk of 100GB size for controlplane nodes</p>"},{"location":"adr/0032-boot-disk-size/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>There will be more available space for pulling images</li> <li>Reduce the number of alerts and ops time</li> <li>Improve platform stability and scalability</li> </ul>"},{"location":"adr/0032-boot-disk-size/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We need to replace all existing nodes for our existing environments</li> <li>Cost will increase depending on number of nodes and price per GB of storage</li> </ul>"},{"location":"adr/0032-boot-disk-size/#recommendation-to-operators","title":"Recommendation to Operators","text":"<ul> <li>Try to use same VM flavors on all environments</li> <li>Use VM flavor with local disk of 100GB or whichever is closest to this size depending on cloud provider for controlplane nodes.</li> <li>For worker nodes use 100GB boot disk size on cloud providers that allow it and on the ones that we can't use the VM flavor with the closest disk size.</li> </ul>"},{"location":"adr/0032-boot-disk-size/#links","title":"Links","text":"<ul> <li>Some recomandations</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/","title":"Run Cluster API controllers on service cluster","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cluster API management and Tekton meeting</li> <li>Date: 2023-03-15</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>With Cluster API there multiple different ways to organise the management hierarchy that have different impacts on the environment in regard to cost, availability, security and ease of deployment and maintainability.</p> <p>Where should we run the Cluster API controller?</p>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to minimise the impact it has on resources</li> <li>We want to ease the deployment and maintenance process</li> <li>We want to be able to tolerate faults in management and workload clusters</li> <li>We want to maintain good security posture</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#considered-options","title":"Considered Options","text":"<ul> <li>Independent clusters</li> <li>All clusters run the Cluster API controllers and all clusters manage themselves independently.</li> <li>Management cluster</li> <li>A new separate management cluster runs the Cluster API controllers and manages all clusters in the environment.</li> <li>Service cluster</li> <li>The service cluster runs the Cluster API controllers and manages all clusters in the environment.</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Service cluster\", because it strikes the balance between security, as it has relatively good availablilty properties and no Kubernetes admin credentails have to be stored in the workload cluster, and maintainability, as clusters can be managed centralised.</p>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Requires no additional resources</li> <li>Requires single instance of Cluster API controllers per environment</li> <li>Requires less deployment and maintenance efforts to manage</li> <li>Service cluster already provides services to allow backups, monitoring, and logging</li> <li>Workload cluster will not have Kubernetes admin credentials</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Cluster API controllers availability relies on service cluster</li> <li>Main consideration is control plane, since service cluster has sufficient nodes for the controller to reschedule</li> <li>Workload cluster will still function but it will lose auto heal and auto scaling functions</li> <li>Service cluster will have Kubernetes admin credentials for the workload cluster</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#independent-cluster","title":"Independent cluster","text":"<ul> <li>Good, no additional resource requirements</li> <li>Good, all clusters have required supporting services</li> <li>Good, all clusters are independently impacted by failures</li> <li>Good, all clusters are independently impacted by configuration mistakes, although...</li> <li>Bad, it is easier to do configuration mistakes</li> <li>Bad, it requires more effort to manage for each cluster</li> <li>Bad, all clusters have to be bootstrapped</li> <li>Bad, will contain Kubernetes admin credentials in workload cluster</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#management-cluster","title":"Management cluster","text":"<ul> <li>Bad, requires additional resources</li> <li>Bad, requires additional supporting services</li> <li>Bad, service and workload cluster lose management (auto healing and auto scaling) on management cluster failure, although...</li> <li>Good, service and workload cluster state can be backed up and restored</li> <li>Good, environment can be managed as a group or individually if needed, although...</li> <li>Bad, all cluster can be impacted by configuration mistakes, although...</li> <li>Good, it is harder for configuration mistakes</li> <li>Good, it requires less effort to manage each cluster</li> <li>Good, single cluster has to be bootstrapped</li> <li>Good, will not contain Kubernetes admin credentials in workload cluster</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#service-cluster","title":"Service cluster","text":"<ul> <li>Good, no additional resource requirements</li> <li>Good, service cluster has required supporting services</li> <li>Bad, workload cluster lose management (auto healing and auto scaling) on service cluster failure, although...</li> <li>Good, workload cluster state can be backed up and restored</li> <li>Good, environment can be managed as a group or individually if needed, although...</li> <li>Bad, all cluster can be impacted by configuration mistakes, although...</li> <li>Good, it is harder for configuration mistakes</li> <li>Good, it requires less effort to manage each cluster</li> <li>Good, single cluster has to be bootstrapped</li> <li>Good, will not contain Kubernetes admin credentials in workload cluster</li> </ul>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/","title":"How to run multiple AMS packages of the same type in the same ck8s environment","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2023-03-23</li> </ul>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Some of our customers request multiple AMS packages of the same size or different sizes, using 1 package per request or multiple packages in the same request (batch-order). How should we add the second or n-th package? Should we scale the nodes vertically or horizontally? If we scale horizontally, then in some cases we need to add AMS packages of different sizes and this brings up the problem where we need to stick the package to a specific set of nodes otherwise for example, after a maintenance that reboots the nodes, a postgres-4 package might be scheduled on a node that was intended for a postgres-8 package and the postgres-8 pod will not be able to be scheduled anymore because it does not fit on the postgres-4 node. How can we overcome this issue? Should we add a <code>elastisys.io/ams-cluster-name</code> label/taint for the AMS with dedicated nodes?</p>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to deliver a stable and secure platform.</li> <li>We want to best serve our customer needs.</li> <li>We want to find a solution which is scalable and minimizes administrator burden.</li> <li>We don't want to waste infrastructure.</li> </ul>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#considered-options","title":"Considered Options","text":"<ol> <li>Always scale the nodes vertically and fit multiple postgres packages on the same pair of nodes.<ul> <li>but how many packages should we place on the same pair of nodes? max 3 packages? max 5 packages?</li> <li>up to what node sizes?</li> <li>what about resource waste? we will not find VM flavors that match exactly our sum of packages. On the other hand we reduce the resources allocated to our services(each node is eating ~ 1 CPU and 2GB ram, by placing 3 packages on the same node we reduce by 2CPU and 4GB RAM the resource waste)</li> </ul> </li> <li> <p>Always scale horizontally and place each package on its own set of dedicated nodes.</p> </li> <li> <p>Scale the AMS dedicated nodes vertically so that it fits all packages on the same set of nodes.</p> <ul> <li>but how many packages should we place on the same pair of nodes? max 3 packages? max 5 packages?</li> <li>up to what node sizes?</li> <li>what about resource waste? we will not find VM flavors that match exactly our sum of packages.</li> </ul> </li> </ol>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options: 2</p> <ol> <li>Always scale horizontally and place each package on its own set of dedicated nodes.<ul> <li>\"Add additional label <code>elastisys.io/ams-cluster-name</code> to a set of nodes dedicated to a specific package\"</li> <li>\"Do not taint the nodes.\" Respect ADR-0022 and add the <code>elastisys.io/node-type</code> taint and label.</li> </ul> </li> </ol>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>The platform stability and scalability is improved.</li> <li>We provide extra isolation of the AMS.</li> <li>Being able to choose from options of scaling the nodes both vertically and horizontally shows that we are flexible and we can satisfy more of the customer needs.</li> <li>We can add more labels that will better describe and schedulle our AMS services, like <code>local-disk</code> and others.</li> <li>With options 1 and 3 more resources are available to the customer.</li> </ul>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>For option 2 we need to add the new label to the AMS repo and update documentation.</li> <li>The infrastructure footprint is increased for option 2.</li> <li>With options 1 and 3 the stability of the AMS package is reduced, because if 1 node is unresponsive then it will affect not 1 AMS package, but multiple AMS packages.</li> </ul>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#recommendations-to-operators","title":"Recommendations to Operators","text":"<ul> <li>Use label like: <code>elastisys.io/ams-cluster-name</code></li> <li>Update the AMS repo and documentation with this and set it by default to automatically picked up from kubectl</li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/","title":"Run Tekton on service cluster","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cluster API management and Tekton meeting</li> <li>Date: 2023-03-15</li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>With Tekton we need to decide where to run it, as it will need considerable permissions in the target cluster to be able to manage it.</p> <p>So, where should we run Tekton?</p>"},{"location":"adr/0035-run-tekton-on-service-cluster/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to ease the maintenance and management process</li> <li>We want to maintain good security posture</li> <li>We want to be able to tolerate faults</li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/#considered-options","title":"Considered Options","text":"<ul> <li>Tekton on service cluster</li> <li>Tekton on each cluster</li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option \"Tekton on service cluster\", because it will follow nicely together with the decision for Cluster API controller and management hierarchy. No credentials have to be stored in the workload cluster, and management can be done centralised.</p>"},{"location":"adr/0035-run-tekton-on-service-cluster/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Requires single instance of Tekton per environment</li> <li>Requires less maintenance and management efforts</li> <li>Tekton should be unaffected if the workload cluster is subjected to faults or a bad change and should be able to keep managing the workload cluster and perform rollback as needed.</li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Possibility that Tekton itself goes into a bad state by applying a bad change</li> <li>This should however only impact the service cluster</li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0035-run-tekton-on-service-cluster/#tekton-on-service-cluster","title":"Tekton on service cluster","text":"<ul> <li>Good, single instance to setup and manage</li> <li>Good, centralised management of all clusters</li> <li>Good, environment can be managed as a group or individually if needed</li> <li>Good, Tekton itself should be unaffected if it applies a bad change to the workload cluster and will be available to perform rollback</li> <li>Bad, Tekton itself can potentially go into a bad state if it applies a bad change to the service cluster</li> <li>Good, no need for high privilege credentials in workload cluster</li> <li>Bad, aggregating high privilege credentials in service cluster</li> <li>Good, limited access to service cluster</li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/#tekton-on-each-cluster","title":"Tekton on each cluster","text":"<ul> <li>Bad, multiple instances to setup and manage</li> <li>Bad, individual management of each cluster</li> <li>Good, each cluster individually impacted by failures</li> <li>Bad, Tekton itself can potentially go into a bad state if it applies a bad change to the cluster</li> <li>Bad, need for high privilege credentials in workload cluster</li> <li>Good, no aggregation of high privilege credentials in service cluster</li> <li>Bad, wider access to workload cluster</li> </ul>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/","title":"Run ingress-nginx as a daemonSet","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2023-03-16</li> </ul>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Currently we run ingress-nginx as a daemonSet. This can potentially feel like a waste of resources in large environments. Running the ingress controller as a deployment with at least two replicas is a possibility.</p> <p>Should we run ingress-nginx as a deployment or as a daemonSet? What do we do for Infra Providers that do not have service type loadbalancer?</p>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to deliver a stable and secure platform.</li> <li>Want want to keep application Nodes \"light\", so the application team knows what capacity is available for their application.</li> <li>We want to find a solution which is scalable and minimizes administrator burden.</li> <li>We don't want to waste infrastructure.</li> <li>We want to keep things simple.</li> </ul>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#considered-options","title":"Considered Options","text":"<ol> <li>Keep running the ingress-nginx as a daemonSet.</li> <li>Run ingress-nginx as a deployment with 2 or more replicas depending on the environment size and requirements.</li> <li>Do not run ingress-nginx on the AMS nodes.</li> <li>For Infra Providers without service type loadbalancer continue using host network as decided in adr0008</li> <li>For Infra Providers without service type loadbalancer start using service type NodePort for nginx and also use the external load balancer to route traffic from ports 80/443 to node ports 30080/30443.</li> </ol>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options: 1 &amp; 3 &amp; 5 * \"Keep running ingress-nginx as a daemonSet.\" * \"Do not run ingress-nginx on the AMS nodes.\" * \"For Infra Providers without service type loadbalancer start using service type NodePort for nginx and also use the external load balancer to route traffic from ports 80/443 to node ports 30080/30443\" -&gt; This superseeds adr0008.</p>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We keep things simple and have the same solution on all Infrastructure Providers.</li> <li>We keep the platform stable and secure and not risk when we replace nodes or nodes become unavailable.</li> <li>No changes are needed.</li> <li>More resources are available on the AMS nodes.</li> <li>Reduce complexity.</li> </ul>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Feels like some resources are wasted on very large environments with many nodes.</li> </ul>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#recommendation-to-operators","title":"Recommendation to Operators","text":"<ul> <li>Do not run the ingress-nginx on the AMS nodes.</li> <li>For Infra Providers without service type loadbalancer start using service type NodePort for nginx and also use the external load balancer to route traffic from ports 80/443 to node ports 30080/30443</li> </ul>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#links","title":"Links","text":"<ul> <li>Issue using host network</li> </ul>"},{"location":"adr/template/","title":"[short title of solved problem and solution]","text":"<ul> <li>Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 </li> <li>Deciders: [list everyone involved in the decision] </li> <li>Date: [YYYY-MM-DD when the decision was last updated] </li> </ul> <p>Technical Story: [description | ticket/issue URL] </p>"},{"location":"adr/template/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]</p>"},{"location":"adr/template/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>[driver 1, e.g., a force, facing concern, \u2026]</li> <li>[driver 2, e.g., a force, facing concern, \u2026]</li> <li>\u2026 </li> </ul>"},{"location":"adr/template/#considered-options","title":"Considered Options","text":"<ul> <li>[option 1]</li> <li>[option 2]</li> <li>[option 3]</li> <li>\u2026 </li> </ul>"},{"location":"adr/template/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].</p>"},{"location":"adr/template/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"adr/template/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>[e.g., compromising quality attribute, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"adr/template/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/template/#option-1","title":"[option 1]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"adr/template/#option-2","title":"[option 2]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"adr/template/#option-3","title":"[option 3]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"adr/template/#links","title":"Links","text":"<ul> <li>[Link type] [Link to ADR] </li> <li>\u2026 </li> </ul>"},{"location":"ciso-guide/","title":"CISO Guide Overview","text":"<p>This guide is for the Chief Information Security Officer (CISO) who needs to prove to an internal or external auditor that the application runs on top of a compliant platform.</p> <p>The CISO can be described via the following user stories:</p> <ul> <li>As an information security officer, I want to audit the Compliant Kubernetes cluster, so as to comply with continuous compliance policies.</li> <li>As an information security officer, I want to quickly identify compliance violation and convert them into actionable tasks for developers.</li> </ul> <p></p> <p>The CISO only needs:</p> <ul> <li>a modern browser (recent versions of Chrome, Firefox or Edge will do);</li> <li>the URL to the Compliant Kubernetes dashboard (usually https://grafana.example.com);</li> <li>credentials for the Compliant Kubernetes cluster.</li> </ul> <p>If in doubt, contact the Compliant Kubernetes administrator.</p>"},{"location":"ciso-guide/audit-logs/","title":"Audit Logs","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.12.4.3 Administrator &amp; Operator Logs</li> </ul> <p>Compliant Kubernetes comes built in with audit logs, which can be accessed via OpenSearch Dashboard. The audit logs are stored in the <code>kubeaudit*</code> index pattern. The audit logs cover calls to the Kubernetes API, specifically who did what and when on which Kubernetes cluster.</p> <p>Thanks to integration with your Identity Provider (IdP), if who is a person, their email address will be shown. If who is a system -- e.g., a CI/CD pipeline -- the name of the ServiceAccount is recorded.</p> <p>Your change management or incident management process should ensure that you also cover why.</p> <p>Both users (application developers) and administrators will show in the audit log. The former will change resources related to their application, whereas the latter will change Compliant Kubernetes system components.</p> <p></p> <p>Note</p> <p>It might be tempting to enable audit logging for \"everything\", e.g., service cluster Kubernetes API, Harbor, Grafana, Kibana, etc. Compliant Kubernetes takes a risk-reward approach and captures audit logs for the events that pose the highest risk to personal data. Don't forget that, at the end of the day, logs are only as useful as someone looks at them.</p>","tags":["ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","HIPAA S48 - Audit Controls - \u00a7 164.312(b)","MSBFS 2020:7 4 kap. 16 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/audit-logs/#ssh-access-logs","title":"SSH Access Logs","text":"<p>Compliant Kubernetes also captures highly privileged SSH access to the worker Nodes in the <code>authlog*</code> index pattern. Only administrators should have such access.</p> <p></p> <p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.9.2.1 User Registration and Deregistration</li> </ul> <p>Many data protection regulation will require you to individually identify administrators, hence individual SSH keys. This allows you to individually identify administrators in the SSH access log.</p>","tags":["ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","HIPAA S48 - Audit Controls - \u00a7 164.312(b)","MSBFS 2020:7 4 kap. 16 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/audit-logs/#audit-logs-for-additional-services","title":"Audit Logs for Additional Services","text":"<p>The Kubernetes Audit Logs capture user access to additional services, i.e., <code>kubectl exec</code> or <code>kubectl port-forward</code> commands. Additional services usually do not have audit logging enabled, since that generates a lot of log entries. Too often the extra bandwidth, storage capacity, performance loss comes with little benefit to data security.</p> <p>Prefer audit logs in your application to capture audit-worthy events, such as login, logout, patient record access, patient record change, etc. Resist the temptation to enable audit logging too \"low\" in the stack. Messages like \"Redis client connected\" are plenty and add little value to your data protection posture.</p> <p>Out of all additional services, audit logging for the database makes the most sense. It can be enabled via pgaudit. Make sure you discuss your auditing requirements with the service-specific administrator, to ensure you find the best risk-reduction-to-implementation-cost trade-off. Typically, you want to discuss:</p> <ul> <li>which databases and tables are audited: e.g., audit <code>app.users</code>, but not <code>app.emailsSent</code>;</li> <li>what operations are audited: e.g., audit <code>INSERT/UPDATE/DELETE</code>, but not <code>SELECT</code>;</li> <li>by which users: e.g., audit person access, but not application access.</li> </ul>","tags":["ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","HIPAA S48 - Audit Controls - \u00a7 164.312(b)","MSBFS 2020:7 4 kap. 16 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/audit-logs/#further-reading","title":"Further Reading","text":"<ul> <li>Kubernetes Auditing</li> <li>pgaudit</li> </ul>","tags":["ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","HIPAA S48 - Audit Controls - \u00a7 164.312(b)","MSBFS 2020:7 4 kap. 16 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/backup/","title":"Backup Dashboard","text":"","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/backup/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/backup/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added]</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/backup/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40:</p> <p>S\u00e4kerhetskopiering</p> <p>12 \u00a7 V\u00e5rdgivaren ska s\u00e4kerst\u00e4lla att personuppgifter som behandlas i informationssystem s\u00e4kerhetskopieras med en fastst\u00e4lld periodicitet. S\u00e4kerhetskopiorna ska f\u00f6rvaras p\u00e5 ett s\u00e4kert s\u00e4tt, v\u00e4l \u00e5tskilda fr\u00e5n originaluppgifterna.</p> <p>13 \u00a7 V\u00e5rdgivaren ska besluta om hur l\u00e4nge s\u00e4kerhetskopiorna ska sparas och hur ofta \u00e5terl\u00e4sningstester av kopiorna ska g\u00f6ras.</p> <p>Allm\u00e4nna r\u00e5d: Hur ofta \u00e5terl\u00e4sningstester ska g\u00f6ras b\u00f6r styras av resultaten av \u00e5terkommande riskanalyser.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/backup/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.12.3.1 Information Backup</li> <li>A.17.1.1 Planning Information Security Continuity</li> </ul>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/backup/#compliant-kubernetes-backup-dashboard","title":"Compliant Kubernetes Backup Dashboard","text":"<p>The Compliant Kubernetes Backup Dashboard allows to quickly audit the status of backups and ensure the Recovery Point Objective are met.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/backup/#handling-non-compliance","title":"Handling Non-Compliance","text":"<p>In case there is a violation of backup policies:</p> <ul> <li>Ask the administrator to check the status of the backup jobs.</li> <li>Ask the developers to check if they correctly marked Kubernetes resources with the necessary backup annotations.</li> </ul>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/capacity-management/","title":"Capacity Management (Kubernetes Status) Dashboard","text":"","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"ciso-guide/capacity-management/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"ciso-guide/capacity-management/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>The ability to ensure the ongoing confidentiality, integrity, availability and resilience of processing systems and services; [highlights added]</p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"ciso-guide/capacity-management/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40:</p> <p>10 \u00a7 V\u00e5rdgivaren ska vid utveckling, idrifttagande och \u00e4ndring av informationssystem som anv\u00e4nds f\u00f6r behandling av personuppgifter s\u00e4kerst\u00e4lla att personuppgifternas tillg\u00e4nglighet, riktighet, konfidentialitet och sp\u00e5rbarhet inte riskeras.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"ciso-guide/capacity-management/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li> <p>A.12.1.3 Capacity Management</p> <p>The use of resources must be monitored, tuned and projections made of future capacity requirements to ensure the required system performance to meet the business objectives.</p> </li> </ul>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"ciso-guide/capacity-management/#compliant-kubernetes-status-dashboard","title":"Compliant Kubernetes Status Dashboard","text":"<p>The Compliant Kubernetes Status Dashboard shows a quick overview of the status of your kubernetes cluster. This includes:</p> <ul> <li>Unhealthy pods</li> <li>Unhealthy nodes</li> <li>Resource requested of the total resources in the cluster</li> <li>Pods with missing resource requests</li> </ul> <p>This makes it easy to identify when your cluster is not working correctly and helps you identify configuration that isn't following best practise.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"ciso-guide/cryptography/","title":"Cryptography Dashboard","text":"","tags":["ISO 27001 A.10.1.2 Key Management","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"ciso-guide/cryptography/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["ISO 27001 A.10.1.2 Key Management","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"ciso-guide/cryptography/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data;</p> <p>In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added]</p>","tags":["ISO 27001 A.10.1.2 Key Management","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"ciso-guide/cryptography/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40:</p> <p>Behandling av personuppgifter i \u00f6ppna n\u00e4t</p> <p>15 \u00a7 Om v\u00e5rdgivaren anv\u00e4nder \u00f6ppna n\u00e4t vid behandling av personuppgifter, ska denne ansvara f\u00f6r att</p> <ol> <li> <p>\u00f6verf\u00f6ring av uppgifterna g\u00f6rs p\u00e5 ett s\u00e5dant s\u00e4tt att inte obeh\u00f6riga kan ta del av dem, och</p> </li> <li> <p>elektronisk \u00e5tkomst eller direkt\u00e5tkomst till uppgifterna f\u00f6reg\u00e5s av stark autentisering.</p> </li> </ol>","tags":["ISO 27001 A.10.1.2 Key Management","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"ciso-guide/cryptography/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.10.1.2 Key Management</li> </ul>","tags":["ISO 27001 A.10.1.2 Key Management","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"ciso-guide/cryptography/#compliant-kubernetes-cryptography-dashboard","title":"Compliant Kubernetes Cryptography Dashboard","text":"<p>The Compliant Kubernetes Cryptography Dashboard allows to quickly audit the status of cryptography. It shows, amongst others, the public Internet endpoints (Ingresses) that are encrypted and the expiry time. Default Compliant Kubernetes configurations automatically renew certificates before expiry.</p>","tags":["ISO 27001 A.10.1.2 Key Management","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"ciso-guide/cryptography/#handling-non-compliance","title":"Handling Non-Compliance","text":"<p>In case there is a violation of cryptography policies:</p> <ul> <li>If a certificate is expired and was not renewed, ask the administrator to check the status of <code>cert-manager</code> and <code>ingress-controller</code> component.</li> <li>If an endpoint is not encrypted, ask the developers to set the necessary Ingress annotations.</li> </ul>","tags":["ISO 27001 A.10.1.2 Key Management","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"ciso-guide/faq/","title":"CISO FAQ","text":"","tags":["ISO 27001 A.18.1.2 Intellectual Property Rights"]},{"location":"ciso-guide/faq/#do-we-need-to-make-our-application-source-code-public-when-using-compliant-kubernetes","title":"Do we need to make our application source code public when using Compliant Kubernetes?","text":"<p>TL;DR</p> <p>Definitely NOT, you own your application source code and you decide what to do with it.</p> <p>Elastisys hereby confirms that Compliant Kubernetes and its Additional Managed Services (AMS) are NOT putting application developers (users) in a situation which obliges them to make their software or source code running on Compliant Kubernetes available to the public.</p> <p>Should we at Elastisys become aware of such an issue existing, we will immediately rectify the situation by replacing problematic components. You can read more about how we take architectural decisions here.</p> <p>As evidence, that the architectural decision process works \u2013 in particular when it comes to licensing issues \u2013 here are some decisions we took:</p> <ul> <li>We decided only to offer TimescaleDB Apache 2 Edition (licensed under Apache 2.0) and NOT TimescaleDB \u201cCommunity Edition\u201d (licensed under the Timescale license). The Timescale license contains some problematic clauses and is, to our knowledge, not tested in court. You can read more about the subtle differences between the TimescaleDB versions by opening this link.</li> <li>We replaced Elasticsearch with OpenSearch (licensed under Apache 2.0), when Elasticsearch changed to the Elastic license. You can read more about the context by opening this link.</li> <li>We replaced InfluxDB with Thanos. This was due to the fact that the open-source version was too limiting. You can read more about this decision by opening this link.</li> <li>We made a risk assessment regarding Grafana, and determined that its AGPL license does not pose a problem. You can read more about our assessment below.</li> </ul> <p>You can read more about our commitment to community-driven open-source by opening this link.</p>","tags":["ISO 27001 A.18.1.2 Intellectual Property Rights"]},{"location":"ciso-guide/faq/#will-grafanalabs-change-to-agpl-licenses-affect-compliant-kubernetes","title":"Will GrafanaLabs change to AGPL licenses affect Compliant Kubernetes?","text":"<p>TL;DR</p> <p>Users and administrators of Compliant Kubernetes are unaffected.</p> <p>Part of Compliant Kubernetes -- specifically the CISO dashboards -- are built on top of Grafana, which recently changed its license to AGPLv3. In brief, if Grafana is exposed via a network connection -- as is the case with Compliant Kubernetes -- then AGPLv3 requires all source code including modifications to be made available.</p> <p>The exact difference between \"aggregate\" and \"modified version\" is somewhat unclear. Compliant Kubernetes only configures Grafana and does not change its source code. Hence, we determined that Compliant Kubernetes is an \"aggregate\" work and is unaffected by the \"viral\" clauses of AGPLv3.</p> <p>As a result, Compliant Kubernetes continues to be distributed under Apache 2.0 as before.</p>","tags":["ISO 27001 A.18.1.2 Intellectual Property Rights"]},{"location":"ciso-guide/faq/#will-minio-change-to-agpl-licenses-affect-compliant-kubernetes","title":"Will Min.io change to AGPL licenses affect Compliant Kubernetes?","text":"<p>TL;DR</p> <p>Users and administrators of Compliant Kubernetes are unaffected.</p> <p>Min.io recently changed its license to AGPLv3.</p> <p>Certain installations of Compliant Kubernetes may use Min.io for accessing object storage on Azure or GCP. However, Compliant Kubernetes does not currently include Min.io. In brief, if Min.io is exposed via a network connection, then AGPLv3 requires all source code including modifications to be made available.</p> <p>The exact difference between \"aggregate\" and \"modified version\" is somewhat unclear. When using Min.io with Compliant Kubernetes, we only use Min.io via its S3-compatible API. Hence, we determined that Compliant Kubernetes is an \"aggregate\" work and is unaffected by the \"viral\" clauses of AGPLv3.</p> <p>As a result, Compliant Kubernetes continues to be distributed under Apache 2.0 as before.</p>","tags":["ISO 27001 A.18.1.2 Intellectual Property Rights"]},{"location":"ciso-guide/intrusion-detection/","title":"Intrusion Detection Dashboard","text":"","tags":["ISO 27001 A.12.2.1 Controls Against Malware","ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","ISO 27001 A.16 Information Security Incident Management","HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7"]},{"location":"ciso-guide/intrusion-detection/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["ISO 27001 A.12.2.1 Controls Against Malware","ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","ISO 27001 A.16 Information Security Incident Management","HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7"]},{"location":"ciso-guide/intrusion-detection/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data;</p> <p>In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added]</p>","tags":["ISO 27001 A.12.2.1 Controls Against Malware","ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","ISO 27001 A.16 Information Security Incident Management","HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7"]},{"location":"ciso-guide/intrusion-detection/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40:</p> <p>Utv\u00e4rdering av skyddet mot olovlig \u00e5tkomst</p> <p>18 \u00a7 V\u00e5rdgivaren ska \u00e5rligen utv\u00e4rdera skyddet mot s\u00e5v\u00e4l intern som extern olovlig \u00e5tkomst till datorn\u00e4tverk och informationssystem som anv\u00e4nds f\u00f6r behandling av personuppgifter.</p>","tags":["ISO 27001 A.12.2.1 Controls Against Malware","ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","ISO 27001 A.16 Information Security Incident Management","HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7"]},{"location":"ciso-guide/intrusion-detection/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.12.2.1 Controls Against Malware</li> <li>A.12.6.1 Management of Technical Vulnerabilities</li> <li>A.16.1.7 Collection of Evidence</li> </ul>","tags":["ISO 27001 A.12.2.1 Controls Against Malware","ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","ISO 27001 A.16 Information Security Incident Management","HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7"]},{"location":"ciso-guide/intrusion-detection/#compliant-kubernetes-intrusion-detection-dashboard","title":"Compliant Kubernetes Intrusion Detection Dashboard","text":"<p>The Compliant Kubernetes Intrusion Detection Dashboard allows to quickly audit any suspicious activity performed by code inside the cluster, such as writing to suspicious files (e.g., in <code>/etc</code>) or attempting suspicious external network connections (e.g., SSH to a command-and-control server). Such activities may indicate anything from a misconfiguration issue to an ongoing attack. Therefore, this dashboard should be regularly reviewed, perhaps even daily.</p>","tags":["ISO 27001 A.12.2.1 Controls Against Malware","ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","ISO 27001 A.16 Information Security Incident Management","HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7"]},{"location":"ciso-guide/intrusion-detection/#handling-non-compliance","title":"Handling Non-Compliance","text":"<p>Make sure you have a proper incident management policy in place. If an attack is ongoing, it might be better to take the system offline to protect data from getting in the wrong hands. Operators need to be trained on what events justify such an extreme action, otherwise, escalating the issue along the reporting chain may add delays that favor the attacker.</p> <p>In less severe cases, simply contact the developers to investigate their code and fix any potential misconfiguration.</p>","tags":["ISO 27001 A.12.2.1 Controls Against Malware","ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","ISO 27001 A.16 Information Security Incident Management","HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7"]},{"location":"ciso-guide/log-review/","title":"Log Review","text":"<p>This document highlights the risks that can be mitigated by regularly reviewing logs and makes concrete recommendations on how to do log review.</p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/log-review/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/log-review/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>Taking into account the state of the art, the costs of implementation and the nature, scope, context and purposes of processing as well as the risk of varying likelihood and severity for the rights and freedoms of natural persons, the controller and the processor shall implement appropriate technical and organisational measures to ensure a level of security appropriate to the risk, including inter alia as appropriate: [...] a process for regularly testing, assessing and evaluating the effectiveness of technical and organisational measures for ensuring the security of the processing.</p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/log-review/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40:</p> <p>2 \u00a7 V\u00e5rdgivaren ska genom ledningssystemet s\u00e4kerst\u00e4lla att [...] 4. \u00e5tg\u00e4rder kan h\u00e4rledas till en anv\u00e4ndare (sp\u00e5rbarhet) i informationssystem som \u00e4r helt eller delvis automatiserade.</p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/log-review/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.12.4.1 \"Event Logging\"</li> <li>A.12.4.3 \"Administrator and Operator Logs\"</li> </ul>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/log-review/#purpose","title":"Purpose","text":"<p>Compliant Kubernetes captures application logs and audit logs in a tamper-proof logging environment, which we call the service cluster. By \"tamper-proof\", we mean that even a complete compromise of production infrastructure does not allow an attacker to erase or change existing log entries, as would be required to hide their activity and avoid suspecion.</p> <p>Note</p> <p>Attackers can, however, inject new \"weird\" logs entries. However, that wouldn't remove their tracks and would only trigger more suspecion.</p> <p>However, said logs only help with information security if they are regularly reviewed for suspicious activity. Prefer to use logs for catching \"unknown unknowns\". For known bad failures -- e.g., a fluentd Pod restarting -- prefer alerts.</p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/log-review/#risks","title":"Risks","text":"<p>Periodically reviewing logs can mitigate the following information security risks:</p> <ul> <li>Information disclosure: Regularly reviewing logs can reveal an attack attempt or an ongoing attack.</li> <li>Downtime: Regularly reviewing logs can reveal misbehaving components (e.g., Pod restarts, various errors) and inform fixes before it leads to downtime.</li> <li>Silent corruption: Regularly reviewing logs can reveal data corruption.</li> </ul>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/log-review/#how-to-do-log-review","title":"How to do log review","text":"<p>By review period, we mean the time elapsed since the last review of the logs, e.g., 30 days.</p> <p>Aim for a review which is both wide and deep. By wide we mean that you should vary the time interval, time point, filters, etc., when reviewing log entries. By deep we mean that you should actually read and try to understand a sample of logs.</p> <ol> <li>Open up a browser and open the Compliant Kubernetes logs of the cluster you are reviewing. This functionality is currently offered by OpenSearch or Kibana and Elasticsearch depending on version, but the procedure is the same.</li> <li>Search for the following keywords on all indices -- i.e., search over each index pattern -- over the last review period: <code>error</code>, <code>failed</code>, <code>failure</code>, <code>deny</code>, <code>denied</code>, <code>blocked</code>, <code>invalid</code>, <code>expired</code>, <code>unable</code>, <code>unauthorized</code>, <code>bad</code>, <code>401</code>, <code>403</code>, <code>500</code>, <code>unknown</code>. Sample a few keywords you recently encountered during your work, e.g., <code>already installed</code> or <code>not found</code>; be creative and unpredictable.</li> <li>Vary the time point, the time interval, filters, etc.</li> <li>Go wide: For each query (index pattern, keyword, timepoint, time interval and filter combination), look at the timeline and see if there is an unexpected increase or decrease in the count of log lines. If you find any, focus your attention on those.</li> <li>Go deep: For each query, sample at least 10 log entries, read them and make sure you understand what they mean. Think about the following:<ul> <li>What are potential causes?</li> <li>What are potential implications?</li> <li>Time: Do the entries appear periodically or randomly?</li> <li>Space: Does a specific component trigger them? Is the entry generated by the platform or the application?</li> </ul> </li> <li>If anything catches your attention vary the time point, time interval and various filters to understand if the log entry is a risk indicator or not. Look for unknown unknowns. Any failures, especially authentication failures, which feature a significant increase are risk indicators.</li> <li>Contact the person owning the component, e.g., the application developer or Compliant Kubernetes architect, to better understand if the entry is suspicious or not. Perhaps it is due to a recent change -- as indicated by an operator log -- and indicates no risk.</li> </ol>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/log-review/#log-review-dashboard","title":"Log review dashboard","text":"<p>Their is a dashboard made to get a overview of the log landscape.</p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/log-review/#possible-resolutions","title":"Possible resolutions","text":"<ol> <li>If you found a suspecious activity, escalate.</li> <li>If the log entry is due to a bug in Compliant Kubernetes, file an issue.</li> </ol>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/network-security/","title":"Network Security Dashboard","text":"","tags":["ISO 27001 A.13 Network Security"]},{"location":"ciso-guide/network-security/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["ISO 27001 A.13 Network Security"]},{"location":"ciso-guide/network-security/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data;</p> <p>In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added]</p>","tags":["ISO 27001 A.13 Network Security"]},{"location":"ciso-guide/network-security/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40:</p> <p>Utv\u00e4rdering av skyddet mot olovlig \u00e5tkomst</p> <p>18 \u00a7 V\u00e5rdgivaren ska \u00e5rligen utv\u00e4rdera skyddet mot s\u00e5v\u00e4l intern som extern olovlig \u00e5tkomst till datorn\u00e4tverk och informationssystem som anv\u00e4nds f\u00f6r behandling av personuppgifter.</p>","tags":["ISO 27001 A.13 Network Security"]},{"location":"ciso-guide/network-security/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.13 Communications Security</li> </ul>","tags":["ISO 27001 A.13 Network Security"]},{"location":"ciso-guide/network-security/#compliant-kubernetes-network-security-dashboard","title":"Compliant Kubernetes Network Security Dashboard","text":"<p>The Compliant Kubernetes Network Security Dashboard allows to audit violations of NetworkPolicies (i.e., \"firewall rules\"). In the best case, denied traffic indicates a misconfiguration. In worst case, denied traffic indicates an ongoing security attack.</p> <p>Significant or unexpected increases of allowed traffic should also be closely monitored. In best case, these may indicate inefficient application code which may cause capacity issues later. In worst case, these may indicate an attempt to exfiltrate large amounts of data or to use the cluster as a reflector for an amplification attack.</p> <p>Therefore, this dashboard should be regularly reviewed, perhaps even daily.</p>","tags":["ISO 27001 A.13 Network Security"]},{"location":"ciso-guide/network-security/#handling-non-compliance","title":"Handling Non-Compliance","text":"<p>Make sure you have a proper incident management policy in place. If an attack is ongoing, it might be better to take the system offline to protect data from getting in the wrong hands. Operators need to be trained on what events justify such an extreme action, otherwise, escalating the issue along the reporting chain may add delays that favor the attacker.</p> <p>In less severe cases, simply contact the developers to investigate their code, fix needless communication attempts or update their NetworkPolicies accordingly to fix any potential misconfiguration.</p>","tags":["ISO 27001 A.13 Network Security"]},{"location":"ciso-guide/network-security/#further-reading","title":"Further Reading","text":"<ul> <li>Network Policies</li> </ul>","tags":["ISO 27001 A.13 Network Security"]},{"location":"ciso-guide/policy-as-code/","title":"Policy-as-Code Dashboard","text":"","tags":["ISO 27001 A.18.2.2 Compliance with Security Policies & Standards","ISO 27001 A.18.2.3 Technical Compliance Review"]},{"location":"ciso-guide/policy-as-code/#relevant-regulations","title":"Relevant Regulations","text":"<p>Although \"policy-as-code\" is not explicit in any regulation, enforcing policies in a consistent technical manner (\"policy-as-code\") is seen as an important strategy to reduce compliance violations, as well as reduce the overhead of complying.</p>","tags":["ISO 27001 A.18.2.2 Compliance with Security Policies & Standards","ISO 27001 A.18.2.3 Technical Compliance Review"]},{"location":"ciso-guide/policy-as-code/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.18.2.2 Compliance with Security Policies &amp; Standards</li> <li>A.18.2.3 Technical Compliance Review</li> </ul>","tags":["ISO 27001 A.18.2.2 Compliance with Security Policies & Standards","ISO 27001 A.18.2.3 Technical Compliance Review"]},{"location":"ciso-guide/policy-as-code/#compliant-kubernetes-policy-as-code-dashboard","title":"Compliant Kubernetes Policy-as-Code Dashboard","text":"<p>Some of your policies are best enforced in code, e.g., Ingress resources do not have encryption set up or PersistentVolumeClaims do not have the necessary backup annotations. Setting up such policies as code is highly dependent on your organization, your risk appetite and your operations. Policies that make sense enforcing by code may be required in some organizations, whereas others might see it as unnecessary and prefer simply treat codified policies as aspirational.</p> <p>Whatever your situation, the Compliant Kubernetes Policy-as-Code Dashboard allows to quickly audit what Kubernetes resources are set up in a non-compliant way or how many policy violations were avoided by Compliant Kubernetes.</p>","tags":["ISO 27001 A.18.2.2 Compliance with Security Policies & Standards","ISO 27001 A.18.2.3 Technical Compliance Review"]},{"location":"ciso-guide/policy-as-code/#handling-non-compliance","title":"Handling Non-Compliance","text":"<p>If an application or user keeps violating a policy, start by reviewing the policy. If the policy seems well codified, contact the developer or the application owner to determine why policy violations occurs or need to be prevented by Compliant Kubernetes.</p> <p>If a policy is missing or too strict, contact the Compliant Kubernetes administrators.</p>","tags":["ISO 27001 A.18.2.2 Compliance with Security Policies & Standards","ISO 27001 A.18.2.3 Technical Compliance Review"]},{"location":"ciso-guide/vulnerability/","title":"Vulnerability Dashboard","text":"","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"ciso-guide/vulnerability/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"ciso-guide/vulnerability/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] a process for regularly testing, assessing and evaluating the effectiveness of technical and organisational measures for ensuring the security of the processing.</p> <p>In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added]</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"ciso-guide/vulnerability/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40:</p> <p>10 \u00a7 V\u00e5rdgivaren ska vid utveckling, idrifttagande och \u00e4ndring av informationssystem som anv\u00e4nds f\u00f6r behandling av personuppgifter s\u00e4kerst\u00e4lla att personuppgifternas tillg\u00e4nglighet, riktighet, konfidentialitet och sp\u00e5rbarhet inte riskeras.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"ciso-guide/vulnerability/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.12.6.1 Management of Technical Vulnerabilities</li> </ul>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"ciso-guide/vulnerability/#compliant-kubernetes-vulnerability-dashboard","title":"Compliant Kubernetes Vulnerability Dashboard","text":"<p>The Compliant Kubernetes Vulnerability Dashboard allows to audit what vulnerable container images are running in production. The dashboard allows to asses increase or decrease of exposure over time. It also allows to prioritize vulnerabilities based on CVE score (CVSS).</p> <p>Therefore, this dashboard should be regularly reviewed, perhaps even daily. A vulnerability management process should be in place to decide how to systematically handle vulnerabilities.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"ciso-guide/vulnerability/#handling-non-compliance","title":"Handling Non-Compliance","text":"<p>Containers should preferably be redeployed with an image that received the necessary security fixes. In case the security fix cannot be deployed in a timely manner -- e.g., due to a slow fix from the vendor -- then the affected containers should be terminated. In all cases, isolating a container using NetworkPolicies, non-root user accounts, no service account token, etc. can make a vulnerability more difficult to exploit.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"ciso-guide/vulnerability/#further-reading","title":"Further Reading","text":"<ul> <li>Vulnerability management</li> <li>CVE</li> <li>CVSS</li> <li>Starboard</li> </ul>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"contributor-guide/","title":"Contributor guide","text":"","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"contributor-guide/#definition-of-done","title":"Definition of Done","text":"<p>When working in regulated industries, it is really important to have the bar high for when something can be called \"done\". In Compliant Kubernetes, we use the following definition of done:</p> <ol> <li>Code and documentation is merged on the main branch of upstream projects. This may cause time delays which are outside your control. However, if we cannot convince upstream projects to take our contributions, then we better know about this as soon as possible. A Compliant Kubernetes relying on an abandoned upstream branch is unsustainable.</li> <li>Code is merged in the Compliant Kubernetes project.</li> <li>Documentation is up-to-date. IT systems used in regulated industries need to have documentation. (See ISO 27001 A.12.1.1 \"Documented Operating Procedures\"). You may either point to upstream documentation -- if Compliant Kubernetes does not add any specifics -- or write a dedicated section/page. Prefer to refer to upstream documentation -- potentially updating that one -- instead of duplicating it in Compliant Kubernetes.</li> <li>You provide evidence for completion. This can be terminal output, screenshot or -- even better, but more time consuming -- a screencast with voice-over explanations. Ideally, these should be attached in the PR to convince the reviewer that the code and documentation are as intended.</li> </ol>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"contributor-guide/#submitting-prs","title":"Submitting PRs","text":"<p>To make the review process as smooth as possible for everyone we have some steps that we'd like you to follow</p> <ul> <li> <p>Look through our DEVELOPMENT.md</p> </li> <li> <p>The pre-commit hook will run on all PRs to <code>main</code>, so either make sure to have it installed by running:</p> <pre><code>pre-commit install\n</code></pre> <p>Or manually run it before committing</p> <pre><code>pre-commit run\n</code></pre> </li> <li> <p>Make sure to follow the PR template, see this for more details.   Alternatively start a PR and you'll see it there.</p> </li> </ul>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"contributor-guide/#setting-up-your-environment","title":"Setting up your environment","text":"<p>To install all required tools, please follow the instructions here.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"contributor-guide/#tips-and-tricks","title":"Tips and tricks","text":"<p>To make your life easier we suggest to use language server for the language that you're editing.</p> <p>E.g.</p> <ul> <li>terraform: terraform-ls</li> <li>yaml: yaml-language-server</li> </ul> <p>To catch pre-commit errors early, direct in your editor, it's also suggested to install plugins for these tools.</p> <ul> <li>markdownlint</li> <li>shellcheck</li> </ul> <p>When developing and you only working on a single application it will be faster to only deploy that application instead of applying all charts. This can be done by figuring out the app label for the application in question by running:</p> <pre><code>bin/ck8s ops helmfile {wc|sc} list\n</code></pre> <p>When you figured out the app label (lets say it's <code>dex</code> in this case) you can check the diff of your work by running:</p> <pre><code>bin/ck8s ops helmfile {wc|sc} -l app=dex diff\n</code></pre> <p>Instead of running <code>helmfile apply</code>, it might be useful to run <code>helmfile sync</code>. This will do a 3-way upgrade and make sure that the helm state matches the objects actually running in kubernetes. This will make sure that you haven't manually edited something for debugging and forgot about it.</p> <pre><code>bin/ck8s ops helmfile {wc|sc} -l app=dex sync\n</code></pre>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"contributor-guide/#object-storage","title":"Object storage","text":"<p>To make creating and deletion of buckets easy, we've a script to help you with that, see here (the quickstart has instructions on how to use it).</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"contributor-guide/#dns","title":"DNS","text":"<p>These following snippets can be used to setup/remove all DNS records required for ck8s using exoscales cli.</p> <p>Start by setting up some variables:</p> <pre><code>DOMAIN=\"example.com\"\nIP=\"203.0.113.123\" # IP to LB/ingress endpoint for the service cluster\nCK8S_ENVIRONMENT_NAME=\"my-cluster-name\"\n\nSUBDOMAINS=(  \"*.ops.${CK8S_ENVIRONMENT_NAME}\"\n              \"grafana.${CK8S_ENVIRONMENT_NAME}\"\n              \"harbor.${CK8S_ENVIRONMENT_NAME}\"\n              \"kibana.${CK8S_ENVIRONMENT_NAME}\"\n              \"dex.${CK8S_ENVIRONMENT_NAME}\"\n              \"notary.harbor.${CK8S_ENVIRONMENT_NAME}\" )\n</code></pre> <pre><code># Adding the A records\nfor SUBDOMAIN in \"${SUBDOMAINS[@]}\"; do\n  exo dns add A \"${DOMAIN}\" -a \"${IP}\" -n \"${SUBDOMAIN}\"\ndone\n</code></pre> <pre><code># Removing the records\nfor SUBDOMAIN in \"${SUBDOMAINS[@]}\"; do\n  exo dns remove \"${DOMAIN}\" \"${SUBDOMAIN}\"\ndone\n</code></pre>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"contributor-guide/#reusing-clusters","title":"Reusing clusters","text":"<p>If you for some reason need to reinstall Compliant Kubernetes from scratch, we have some scripts that removes all objects created by this repo. The scripts can be found here (clean-sc.sh and clean-wc.sh).</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"operator-manual/","title":"Operator Manual Overview","text":"<p>This manual is for Compliant Kubernetes administrators.</p> <p>Operators can be described via the following user stories:</p> <ul> <li>As an administrator I want to create/destroy/upgrade a Compliant Kubernetes cluster.</li> <li>As an administrator I want to re-configure a Compliant Kubernetes cluster.</li> <li>As an on-call administrator I want to be alerted when abnormal activity is detected, suggesting a pending intrusion.</li> <li>As an on-call administrator I want to be alerted when the Compliant Kubernetes cluster is unhealthy.</li> <li>As an on-call administrator I want \"break glass\" to investigate and recover an unhealthy Compliant Kubernetes cluster.</li> </ul>"},{"location":"operator-manual/access-control/","title":"Access control","text":"<p>This guide describes how to set up and make use of group claims for applications.</p> <p>Note</p> <p>This guide assumes your group claim name is <code>groups</code></p>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/access-control/#kubernetes","title":"Kubernetes","text":"<p>To set up kubelogin to fetch and use groups make sure that your kubeconfig looks something like this.</p> <pre><code>users:\n- name: user@my-cluster\nuser:\nexec:\napiVersion: client.authentication.k8s.io/v1beta1\nargs:\n- oidc-login\n- get-token\n- --oidc-issuer-url=https://dex.my-cluster-domain.com\n- --oidc-client-id=my-client-id\n- --oidc-client-secret=my-client-secret\n- --oidc-extra-scope=email,groups # Make sure groups are here\ncommand: kubectl\n</code></pre> <p>Tips</p> <p>Your token can be found in <code>~/.kube/cache/oidc-login/</code>. This is useful if you're trying to debug your claims since you can just paste the token to jwt.io and check it.</p> <p>Example:</p> <pre><code>$ ls ~/.kube/cache/oidc-login/\n\n$ kubectl get pod\n&lt;log in&gt;\n\n$ ls ~/.kube/cache/oidc-login/\n13b165965d8e80749ce3b8d442da3e4e9f5ff5e38900ef104eee99fde85a39d4\n\n$ cat ~/.kube/cache/oidc-login/13b165965d8e80749ce3b8d442da3e4e9f5ff5e38900ef104eee99fde85a39d4 | jq -r .id_token\neyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2RleC5teS1jbHVzdGVyLWRvbWFpbi5jb20iLCJpYXQiOjE2MjE1MTUxNzcsImV4cCI6MTY1MzEzNzU3NywiYXVkIjoibXktY2xpZW50LWlkIiwic3ViIjoiSGlVUE92S1BKMmVwWUkwR1R1U0JYWGRxYTJTV2ZxRnc1ZjBXNVBQeThTWSIsIm5vdW5jZSI6IkNoVXhNRFk0TVRZNE1qRXpORFUzTURVM01ERXlNREFTQm1kdmIyZHNaUSIsImF0X2hhc2giOiI1aUZjbF9Sc1JvblhHekZaMU0xQ2JnIiwiZW1haWwiOiJ1c2VyQG15LWRvbWFpbi5jb20iLCJlbWFpbF92ZXJpZmllZCI6InRydWUiLCJncm91cHMiOlsibXktZ3JvdXAtb25lIiwibXktZ3JvdXAtdHdvIl19.s65Aowfn6B1PiyQvRGPRu9KgX7G39nkLtx6yCAEElao\n</code></pre> <p>Copy the token to jwt.io and ensure that the payload includes the expected groups claim.</p>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/access-control/#opensearch","title":"OpenSearch","text":"<p>To enable OpenSearch to use the groups for OpenSearch Dashboards access. <pre><code>opensearch:\nsso:\nscope: \"... groups\" # Add groups to existing\nextraRoleMappings:\n- mapping_name: kibana_user\ndefinition:\nbackend_roles:\n- my-group-name\n- mapping_name: kubernetes_log_reader\ndefinition:\nbackend_roles:\n- my-group-name\n</code></pre></p> <p>Note</p> <p>For Open Distro for Elasticsearch and Kibana used in v0.18 and earlier, the same configuration applies under the root key <code>elasticsearch</code> instead of <code>opensearch</code>.</p>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/access-control/#harbor","title":"Harbor","text":"<p>Set correct group claim name since the default scopes includes groups already. This groups can be assigned to projects or as admin group.</p> <pre><code>harbor:\noidc:\ngroupClaimName: groups\n</code></pre> <p>Note</p> <p>When OIDC (e.g. DeX) is enabled we cannot create static users using the Harbor web interface. But when anyone logs in via DeX they automatically get a user and we can promote that user to admin. Once there is one admin, they can set specific permissions for other users (there should be at least a few users promoted to admins).</p>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/access-control/#grafana","title":"Grafana","text":"<p>Note</p> <p>This section assumes that elastisys/compliantkubernetes-apps/pull/450 is merged</p>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/access-control/#ops-grafana","title":"OPS Grafana","text":"<pre><code>prometheus:\ngrafana:\noidc:\nenabled: true\nuserGroups:\ngrafanaAdmin: my-admin-group\ngrafanaEditor: my-editor-group\ngrafanaViewer: my-viewer-group\nscopes: \".... groups\" # Add groups to existing\nallowedDomains:\n- my-domain.com\n</code></pre>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/access-control/#user-grafana","title":"User Grafana","text":"<pre><code>user:\ngrafana:\noidc:\nscopes: \"... groups\" # Add groups to existing\nallowedDomains:\n- my-domain.com\nuserGroups:\ngrafanaAdmin: my-admin-group\ngrafanaEditor: my-editor-group\ngrafanaViewer: my-viewer-group\n</code></pre>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/aws/","title":"Aws","text":"<p>This page is out of date</p> <p>We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date.</p> <p>Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly.</p>"},{"location":"operator-manual/aws/#compliant-kubernetes-deployment-on-aws","title":"Compliant Kubernetes Deployment on AWS","text":"<p>This document describes how to set up Compliant Kubernetes on AWS. The setup has two major parts:</p> <ol> <li>Deploying at least two vanilla Kubernetes clusters</li> <li>Deploying Compliant Kubernetes apps</li> </ol> <p>Before starting, make sure you have all necessary tools.</p> <p>Note</p> <p>This guide is written for compliantkubernetes-apps v0.17.0</p>"},{"location":"operator-manual/aws/#setup","title":"Setup","text":"<p>Choose names for your service cluster and workload clusters, as well as the DNS domain to expose the services inside the service cluster:</p> <pre><code>SERVICE_CLUSTER=\"testsc\"\nWORKLOAD_CLUSTERS=( \"testwc0\" )\nBASE_DOMAIN=\"example.com\"\n</code></pre> <p>Note</p> <p>If you want to set up multiple workload clusters you can add more names. E.g. <code>WORKLOAD_CLUSTERS=( \"testwc0\" \"testwc1\" \"testwc2\" )</code></p> <p><code>SERVICE_CLUSTER</code> and each entry in <code>WORKLOAD_CLUSTERS</code> must be maximum 17 characters long.</p>"},{"location":"operator-manual/aws/#deploying-vanilla-kubernetes-clusters","title":"Deploying vanilla Kubernetes clusters","text":"<p>We suggest to set up Kubernetes clusters using kubespray. If you haven't done so already, clone the Elastisys Compliant Kubernetes Kubespray repo as follows:</p> <pre><code>git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray\ncd compliantkubernetes-kubespray\n</code></pre>"},{"location":"operator-manual/aws/#infrastructure-setup-using-terraform","title":"Infrastructure Setup using Terraform","text":"<p>Note</p> <p>This step will also create the necessary IAM Roles for control plane Nodes to make integration with the cloud provider work. This will ensure that both Service Type LoadBalancer and PersistentVolumes (backed by AWS EBS volumes) will work. The necessary credentials are pulled automatically by control plane Nodes via AWS EC2 instance metadata and require no other configuration.</p>"},{"location":"operator-manual/aws/#expose-aws-credentials-to-terraform","title":"Expose AWS credentials to Terraform","text":"<p>We suggest exposing AWS credentials to Terraform via environment variables, so they are not accidentally left on the file-system:</p> <pre><code>export TF_VAR_AWS_ACCESS_KEY_ID=\"xyz\" # Access key for AWS\nexport TF_VAR_AWS_SECRET_ACCESS_KEY=\"zyx\" # Secret key for AWS\nexport TF_VAR_AWS_SSH_KEY_NAME=\"foo\" # Name of the AWS key pair to use for the EC2 instances\nexport TF_VAR_AWS_DEFAULT_REGION=\"bar\" # Region to use for all AWS resources\n</code></pre> <p>Tip</p> <p>We suggest generating the SSH key locally, then importing it to AWS.</p>"},{"location":"operator-manual/aws/#customize-your-infrastructure","title":"Customize your infrastructure","text":"<p>Create a configuration for the service cluster and the workload cluster:</p> <pre><code>pushd kubespray\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\ncat contrib/terraform/aws/terraform.tfvars \\\n| sed \\\n-e \"s@^aws_cluster_name =.*@aws_cluster_name = \\\"$CLUSTER\\\"@\" \\\n-e \"s@^inventory_file =.*@inventory_file = \\\"../../../inventory/hosts-$CLUSTER\\\"@\" \\\n-e \"s@^aws_kube_worker_size =.*@aws_kube_worker_size = \\\"t3.large\\\"@\" \\\n&gt; inventory/terraform-$CLUSTER.tfvars\ndone\npopd\n</code></pre> <p>Review and, if needed, adjust the files in <code>kubespray/inventory/</code>.</p>"},{"location":"operator-manual/aws/#initialize-and-apply-terraform","title":"Initialize and Apply Terraform","text":"<pre><code>pushd kubespray/contrib/terraform/aws\nterraform init\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nterraform apply \\\n-var-file=../../../inventory/terraform-$CLUSTER.tfvars \\\n-auto-approve \\\n-state=../../../inventory/tfstate-$CLUSTER.tfstate\ndone\npopd\n</code></pre> <p>Important</p> <p>The Terraform state is stored in <code>kubespray/inventory/tfstate-*</code>. It is precious. Consider backing it up or using Terraform Cloud.</p>"},{"location":"operator-manual/aws/#check-that-the-ansible-inventory-was-properly-generated","title":"Check that the Ansible inventory was properly generated","text":"<pre><code>ls -l kubespray/inventory/hosts-*\n</code></pre> <p>You may also want to check the AWS Console if the infrastructure was created correctly:</p> <p></p>"},{"location":"operator-manual/aws/#deploying-vanilla-kubernetes-clusters-using-kubespray","title":"Deploying vanilla Kubernetes clusters using Kubespray","text":"<p>With the infrastructure provisioned, we can now deploy both the sc and wc Kubernetes clusters using kubespray. Before trying any of the steps, make sure you are in the repo's root folder.</p>"},{"location":"operator-manual/aws/#init-the-kubespray-config-in-your-config-path","title":"Init the Kubespray config in your config path","text":"<pre><code>export CK8S_CONFIG_PATH=~/.ck8s/aws\nexport CK8S_PGP_FP=&lt;your GPG key fingerprint&gt;  # retrieve with gpg --list-secret-keys\n</code></pre> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\n./bin/ck8s-kubespray init $CLUSTER aws $CK8S_PGP_FP\ndone\n</code></pre>"},{"location":"operator-manual/aws/#copy-the-inventories-generated-by-terraform-above-in-the-right-place","title":"Copy the inventories generated by Terraform above in the right place","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\ncp kubespray/inventory/hosts-$CLUSTER $CK8S_CONFIG_PATH/$CLUSTER-config/inventory.ini\ndone\n</code></pre>"},{"location":"operator-manual/aws/#run-kubespray-to-deploy-the-kubernetes-clusters","title":"Run kubespray to deploy the Kubernetes clusters","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\n./bin/ck8s-kubespray apply $CLUSTER --flush-cache -e ansible_user=ubuntu\ndone\n</code></pre> <p>This may take up to 20 minutes per cluster.</p>"},{"location":"operator-manual/aws/#correct-the-kubernetes-api-ip-addresses","title":"Correct the Kubernetes API IP addresses","text":"<p>Find the DNS names of the load balancers fronting the API servers:</p> <pre><code>grep apiserver_loadbalancer $CK8S_CONFIG_PATH/*-config/inventory.ini\n</code></pre> <p>Locate the encrypted kubeconfigs <code>kube_config_*.yaml</code> and edit them using sops. Copy the URL of the load balancer from inventory files shown above into <code>kube_config_*.yaml</code>. Do not overwrite the port.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops $CK8S_CONFIG_PATH/.state/kube_config_$CLUSTER.yaml\ndone\n</code></pre>"},{"location":"operator-manual/aws/#test-access-to-the-clusters-as-follows","title":"Test access to the clusters as follows","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file $CK8S_CONFIG_PATH/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get nodes'\ndone\n</code></pre>"},{"location":"operator-manual/aws/#deploying-compliant-kubernetes-apps","title":"Deploying Compliant Kubernetes Apps","text":"<p>Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps.</p>"},{"location":"operator-manual/aws/#clone-compliantkubernetes-apps-and-install-pre-requisites","title":"Clone <code>compliantkubernetes-apps</code> and Install Pre-requisites","text":"<p>If you haven't done so already, clone the <code>compliantkubernetes-apps</code> repo and install pre-requisites.</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes-apps.git\ncd compliantkubernetes-apps\nansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127.0.0.1, get-requirements.yaml\n</code></pre>"},{"location":"operator-manual/aws/#initialize-the-apps-configuration","title":"Initialize the apps configuration","text":"<pre><code>export CK8S_ENVIRONMENT_NAME=my-environment-name\n#export CK8S_FLAVOR=[dev|prod] # defaults to dev\nexport CK8S_CONFIG_PATH=~/.ck8s/my-cluster-path\nexport CK8S_CLOUD_PROVIDER=# [exoscale|safespring|citycloud|aws|baremetal]\nexport CK8S_PGP_FP=&lt;your GPG key fingerprint&gt;  # retrieve with gpg --list-secret-keys\n\n./bin/ck8s init\n</code></pre> <p>This will initialise the configuration in the <code>${CK8S_CONFIG_PATH}</code> directory. Generating configuration files <code>sc-config.yaml</code> and <code>wc-config.yaml</code>, as well as secrets with randomly generated passwords in <code>secrets.yaml</code>. This will also generate read-only default configuration under the directory <code>defaults/</code> which can be used as a guide for available and suggested options.</p> <pre><code>ls -l $CK8S_CONFIG_PATH\n</code></pre>"},{"location":"operator-manual/aws/#configure-the-apps","title":"Configure the apps","text":"<p>Edit the configuration files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code>, <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/secrets.yaml</code> and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted.</p> <pre><code>vim ${CK8S_CONFIG_PATH}/sc-config.yaml\nvim ${CK8S_CONFIG_PATH}/wc-config.yaml\nsops ${CK8S_CONFIG_PATH}/secrets.yaml\n</code></pre> <p>Tip</p> <p>The default configuration for the service cluster and workload cluster are available in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code> and can be used as a reference for available options.</p> <p>Warning</p> <p>Do not modify the read-only default configurations files found in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code>. Instead configure the cluster by modifying the regular files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> as they will override the default options.</p> <p>The following are the minimum change you should perform:</p> <pre><code># sc-config.yaml and wc-config.yaml\nglobal:\nbaseDomain: \"set-me\"  # set to $BASE_DOMAIN\nopsDomain: \"set-me\"  # set to ops.$BASE_DOMAIN\nissuer: letsencrypt-prod\n\nobjectStorage:\ntype: \"s3\"\ns3:\nregion: \"set-me\"  # Region for S3 buckets, e.g, eu-central-1\nregionEndpoint: \"set-me\"  # e.g., https://s3.us-west-1.amazonaws.com\n</code></pre> <pre><code># sc-config.yaml\nharbor:\noidc:\ngroupClaimName: \"set-me\" # set to group claim name used by OIDC provider\nadminGroupName: \"set-me\" # name of the group that automatically will get admin\n\nissuers:\nletsencrypt:\nprod:\nemail: \"set-me\"  # set this to an email to receive LetsEncrypt notifications\nstaging:\nemail: \"set-me\"  # set this to an email to receive LetsEncrypt notifications\n</code></pre> <pre><code># secrets.yaml\nobjectStorage:\ns3:\naccessKey: \"set-me\" #put your s3 accesskey\nsecretKey: \"set-me\" #put your s3 secretKey\n</code></pre>"},{"location":"operator-manual/aws/#create-placeholder-dns-entries","title":"Create placeholder DNS entries","text":"<p>To avoid negative caching and other surprises. Create two placeholders as follows (feel free to use the \"Import zone\" feature of AWS Route53):</p> <pre><code>echo \"\"\"\n*.$BASE_DOMAIN     60s A 203.0.113.123\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\n\"\"\"\n</code></pre> <p>NOTE: 203.0.113.123 is in TEST-NET-3 and okey to use as placeholder.</p>"},{"location":"operator-manual/aws/#install-compliant-kubernetes-apps","title":"Install Compliant Kubernetes apps","text":"<p>Start with the service cluster:</p> <pre><code>ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml\n./bin/ck8s apply sc  # Respond \"n\" if you get a WARN\n</code></pre> <p>Then the workload clusters:</p> <pre><code>for CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n    ./bin/ck8s apply wc  # Respond \"n\" if you get a WARN\ndone\n</code></pre>"},{"location":"operator-manual/aws/#settling","title":"Settling","text":"<p>Important</p> <p>Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes.</p> <p>You can check if the system settled as follows:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces pods'\ndone\n</code></pre> <p>Check the output of the command above. All Pods needs to be Running or Completed.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates'\ndone\n</code></pre> <p>Check the output of the command above. All resources need to have the Ready column True.</p>"},{"location":"operator-manual/aws/#setup-required-dns-entries","title":"Setup required DNS entries","text":"<p>You will need to set up the following DNS entries. First, determine the public IP of the load-balancer fronting the Ingress controller of the service cluster:</p> <pre><code>SC_INGRESS_LB_HOSTNAME=$(sops exec-file $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml 'kubectl --kubeconfig {} get -n ingress-nginx svc ingress-nginx-controller -o jsonpath={.status.loadBalancer.ingress[0].hostname}')\nSC_INGRESS_LB_IP=$(dig +short $SC_INGRESS_LB_HOSTNAME | head -1)\necho $SC_INGRESS_LB_IP\n</code></pre> <p>Then, import the following zone in AWS Route53:</p> <pre><code>echo \"\"\"\n*.ops.$BASE_DOMAIN    60s A $SC_INGRESS_LB_IP\ndex.$BASE_DOMAIN      60s A $SC_INGRESS_LB_IP\ngrafana.$BASE_DOMAIN  60s A $SC_INGRESS_LB_IP\nharbor.$BASE_DOMAIN   60s A $SC_INGRESS_LB_IP\nkibana.$BASE_DOMAIN   60s A $SC_INGRESS_LB_IP\n\"\"\"\n</code></pre>"},{"location":"operator-manual/aws/#testing","title":"Testing","text":"<p>After completing the installation step you can test if the apps are properly installed and ready using the commands below.</p> <p>Start with the service cluster:</p> <pre><code>ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml\n./bin/ck8s test sc  # Respond \"n\" if you get a WARN\n</code></pre> <p>Then the workload clusters:</p> <pre><code>for CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n    ./bin/ck8s test wc  # Respond \"n\" if you get a WARN\ndone\n</code></pre> <p>Done. Navigate to the endpoints, for example <code>grafana.$BASE_DOMAIN</code>, <code>kibana.$BASE_DOMAIN</code>, <code>harbor.$BASE_DOMAIN</code>, etc. to discover Compliant Kubernetes's features.</p>"},{"location":"operator-manual/aws/#teardown","title":"Teardown","text":""},{"location":"operator-manual/aws/#removing-compliant-kubernetes-apps-from-your-cluster","title":"Removing Compliant Kubernetes Apps from your cluster","text":"<p>To remove the applications added by compliant kubernetes you can use the two scripts <code>clean-sc.sh</code> and <code>clean-wc.sh</code>, they are located here in the scripts folder.</p> <p>They perform the following actions:</p> <ol> <li>Delete the added helm charts</li> <li>Delete the added namespaces</li> <li>Delete any remaining PersistentVolumes</li> <li>Delete the added CustomResourceDefinitions</li> </ol> <p>Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.</p>"},{"location":"operator-manual/aws/#remove-infrastructure","title":"Remove infrastructure","text":"<p>Note</p> <p>Even if you want to completely destroy the cluster with all its infrastructure, it is recommended to first execute the <code>clean</code> scripts described above, otherwise resources created by the cloud controller (e.g. volumes and loadbalancers) are not removed and <code>terraform destroy</code> might fail.</p> <pre><code>pushd kubespray/contrib/terraform/aws\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nterraform destroy \\\n-auto-approve \\\n-state=../../../inventory/tfstate-$CLUSTER.tfstate\ndone\npopd\n</code></pre>"},{"location":"operator-manual/aws/#further-reading","title":"Further Reading","text":"<ul> <li>Compliant Kubernetes apps repo</li> <li>Configurations option</li> </ul>"},{"location":"operator-manual/azure/","title":"Azure","text":"<p>This page is out of date</p> <p>We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date.</p> <p>Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly.</p>"},{"location":"operator-manual/azure/#compliant-kubernetes-deployment-on-azure","title":"Compliant Kubernetes Deployment on Azure","text":"<p>This document contains instructions on how to setup a service cluster and a workload cluster in Azure. The following are the main tasks addressed in this document:</p> <ol> <li>Infrastructure setup for two clusters: one service and one workload cluster</li> <li>Deploying Compliant Kubernetes on top of the two clusters.</li> <li>Creating DNS Records</li> <li>Deploying Rook Storage Orchestration Service</li> <li>Deploying Compliant Kubernetes apps</li> </ol> <p>Before starting, make sure you have all necessary tools.</p> <p>Note</p> <p>This guide is written for compliantkubernetes-apps v0.13.0</p>"},{"location":"operator-manual/azure/#setup","title":"Setup","text":"<p>Choose names for your service cluster and workload clusters, as well as the DNS domain to expose the services inside the service cluster:</p> <pre><code>SERVICE_CLUSTER=\"sc-test\"\nWORKLOAD_CLUSTERS=( \"wc-test0\" )\nBASE_DOMAIN=\"example.com\"\n</code></pre>"},{"location":"operator-manual/azure/#infrastructure-setup-using-azurerm","title":"Infrastructure Setup using AzureRM","text":"<p>We suggest to set up Kubernetes clusters using kubespray. If you haven't done so already, clone the Elastisys Compliant Kubernetes Kubespray repo as follows:</p> <pre><code>git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray\ncd compliantkubernetes-kubespray\n</code></pre>"},{"location":"operator-manual/azure/#install-azure-cli","title":"Install azure-cli","text":"<p>If you haven't done so already, please install and configure azure-cli.</p>"},{"location":"operator-manual/azure/#login-with-azure-cli","title":"Login with azure-cli","text":"<pre><code>az login\n</code></pre>"},{"location":"operator-manual/azure/#customize-your-infrastructure","title":"Customize your infrastructure","text":"<p>Create a configuration for the service and the workload clusters:</p> <pre><code>pushd kubespray/contrib/azurerm/\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\naz group create -g $CLUSTER -l northeurope\n    mkdir -p $CLUSTER/inventory\ndone\npopd\n</code></pre> <p>Note</p> <p>Please specify the value for the <code>ssh_public_keys</code> variable in <code>kubespray/contrib/azurerm/group_vars/all</code>. It must be your SSH public key to access your Azure virtual machines. Besides, the value for the <code>cluster_name</code> variable must be globally unique due to some restrictions in Azure. Make sure that <code>$SERVICE_CLUSTER</code> and <code>$WORKLOAD_CLUSTERS</code> are unique.</p> <p>Review and, if needed, adjust the files in <code>kubespray/contrib/azurerm/group_vars/all</code> accordingly.</p>"},{"location":"operator-manual/azure/#generate-and-apply-the-templates","title":"Generate and apply the templates","text":"<pre><code>pushd kubespray/contrib/azurerm/\ntmp=\"\"\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\ncat group_vars/all \\\n| sed \\\n-e \"s@^cluster_name:.*@cluster_name: \\\"$CLUSTER\\\"@\" \\\n&gt; group_vars/all1\n   cat group_vars/all1 &gt; group_vars/all\n   rm group_vars/all1\n   if [ -z $tmp ]\nthen\nsed -i \"s/{{ playbook_dir }}/{{ playbook_dir }}\\/$CLUSTER/g\"  roles/generate-templates/tasks/main.yml\n\n           ansible-playbook generate-templates.yml\n\n           az deployment group create --template-file ./$CLUSTER/.generated/network.json -g $CLUSTER\naz deployment group create --template-file ./$CLUSTER/.generated/storage.json -g $CLUSTER\naz deployment group create --template-file ./$CLUSTER/.generated/availability-sets.json -g $CLUSTER\naz deployment group create --template-file ./$CLUSTER/.generated/bastion.json -g $CLUSTER\naz deployment group create --template-file ./$CLUSTER/.generated/masters.json -g $CLUSTER\naz deployment group create --template-file ./$CLUSTER/.generated/minions.json -g $CLUSTER\nelse\nsed -i \"s/{{ playbook_dir }}\\/$tmp/{{ playbook_dir }}\\/$CLUSTER/g\"  roles/generate-templates/tasks/main.yml\n\n           ansible-playbook generate-templates.yml\n\n           az deployment group create --template-file ./$CLUSTER/.generated/network.json -g $CLUSTER\naz deployment group create --template-file ./$CLUSTER/.generated/storage.json -g $CLUSTER\naz deployment group create --template-file ./$CLUSTER/.generated/availability-sets.json -g $CLUSTER\naz deployment group create --template-file ./$CLUSTER/.generated/bastion.json -g $CLUSTER\naz deployment group create --template-file ./$CLUSTER/.generated/masters.json -g $CLUSTER\naz deployment group create --template-file ./$CLUSTER/.generated/minions.json -g $CLUSTER\nfi\ntmp=$CLUSTER\ndone\nsed -i \"s/{{ playbook_dir }}\\/$tmp/{{ playbook_dir }}/g\"  roles/generate-templates/tasks/main.yml\n sed -i \"s/{{ playbook_dir }}\\/$tmp/{{ playbook_dir }}/g\"  roles/generate-inventory_2/tasks/main.yml\n sed -i \"s/{{ playbook_dir }}\\/$tmp/{{ playbook_dir }}/g\"  roles/generate-inventory/tasks/main.yml\npopd\n</code></pre>"},{"location":"operator-manual/azure/#generating-an-inventory-for-kubespray","title":"Generating an inventory for kubespray","text":"<pre><code>pushd kubespray/contrib/azurerm/\ntmp=\"\"\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nif [ -z $tmp ]\nthen\nsed -i \"s/{{ playbook_dir }}/{{ playbook_dir }}\\/$CLUSTER/g\"  roles/generate-inventory_2/tasks/main.yml\n         sed -i \"s/{{ playbook_dir }}/{{ playbook_dir }}\\/$CLUSTER/g\"  roles/generate-inventory/tasks/main.yml\n         ./generate-inventory.sh $CLUSTER\n\nelse\nsed -i \"s/{{ playbook_dir }}\\/$tmp/{{ playbook_dir }}\\/$CLUSTER/g\"  roles/generate-inventory_2/tasks/main.yml\n           sed -i \"s/{{ playbook_dir }}\\/$tmp/{{ playbook_dir }}\\/$CLUSTER/g\"  roles/generate-inventory/tasks/main.yml\n           ./generate-inventory.sh $CLUSTER\nfi\ntmp=$CLUSTER\ndone\nsed -i \"s/{{ playbook_dir }}\\/$tmp/{{ playbook_dir }}/g\"  roles/generate-inventory_2/tasks/main.yml\nsed -i \"s/{{ playbook_dir }}\\/$tmp/{{ playbook_dir }}/g\"  roles/generate-inventory/tasks/main.yml\npopd\n</code></pre> <p>The inventory files for for cluster will be created under <code>*/inventory/</code>. Besides, two <code>loadBalancer_vars.yaml</code> files will be created, one for each cluster.</p> <p>You may also want to check the Azure portal if the infrastructure was created correctly. The figure below shows for <code>wc-test0</code>.</p> <p></p>"},{"location":"operator-manual/azure/#deploying-vanilla-kubernetes-clusters-using-kubespray","title":"Deploying vanilla Kubernetes clusters using Kubespray","text":"<p>With the infrastructure provisioned, we can now deploy Kubernetes using kubespray. First, change to the <code>compliantkubernetes-kubespray</code> root directory.</p> <pre><code>cd ..\n</code></pre>"},{"location":"operator-manual/azure/#init-the-kubespray-config-in-your-config-path","title":"Init the Kubespray config in your config path","text":"<pre><code>export CK8S_CONFIG_PATH=~/.ck8s/azure\nexport CK8S_PGP_FP=&lt;your GPG key fingerprint&gt;  # retrieve with gpg --list-secret-keys\n\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\n./bin/ck8s-kubespray init $CLUSTER default $CK8S_PGP_FP\ndone\n</code></pre>"},{"location":"operator-manual/azure/#copy-the-generated-inventory-files-in-the-right-location","title":"Copy the generated inventory files in the right location","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTERS[@]}; do\n#add calico to the inventory file\ncat kubespray/contrib/azurerm/$CLUSTER/inventory/inventory.j2 \\\n| sed  '/\\[k8s_cluster:children\\]/i \\[calico-rr\\]' \\\n&gt; $CK8S_CONFIG_PATH/$CLUSTER-config/inventory.ini\necho \"calico-rr\" &gt;&gt; $CK8S_CONFIG_PATH/$CLUSTER-config/inventory.ini $CK8S_CONFIG_PATH/$CLUSTER-config/inventory.ini\n    # Add ansible_user ubuntu (note that this assumes you have set admin_username in azurerm/group_vars/all to ubuntu)\necho -e 'ansible_user: ubuntu' &gt;&gt; $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml\n\n    # Get the IP address of the loadbalancer (to be added in kubadmin certSANs list which will be used for kubectl)\nip=$(grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' kubespray/contrib/azurerm/$CLUSTER/loadbalancer_vars.yml)\necho 'supplementary_addresses_in_ssl_keys: [\"'$ip'\"]' &gt;&gt; $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml\n    echo -e 'nameservers:\\n  - 1.1.1.1' &gt;&gt; $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml\n    echo 'resolvconf_mode: host_resolvconf' &gt;&gt; $CK8S_CONFIG_PATH/$CLUSTER-config/group_vars/k8s_cluster/ck8s-k8s_cluster.yaml\n\ndone\n</code></pre>"},{"location":"operator-manual/azure/#run-kubespray-to-deploy-the-kubernetes-clusters","title":"Run kubespray to deploy the Kubernetes clusters","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\n./bin/ck8s-kubespray apply $CLUSTER --flush-cache\ndone\n</code></pre> <p>This may take up to 30 minutes per cluster.</p> <p>Please increase the value for timeout, e.g <code>timeout=30</code>, in <code>kubespray/ansible.cfg</code> if you face the following issue while running step-3.</p> <pre><code>TASK [bootstrap-os : Fetch /etc/os-release] ****************************************************\nfatal: [minion-0]: FAILED! =&gt; {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"}\nfatal: [minion-1]: FAILED! =&gt; {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"}\nfatal: [minion-2]: FAILED! =&gt; {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"}\nfatal: [master-0]: FAILED! =&gt; {\"msg\": \"Timeout (12s) waiting for privilege escalation prompt: \"}\n</code></pre>"},{"location":"operator-manual/azure/#correct-the-kubernetes-api-ip-addresses","title":"Correct the Kubernetes API IP addresses","text":"<p>Get the public IP address of the loadbalancer:</p> <pre><code>grep -o '[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}' kubespray/contrib/azurerm/$CLUSTER/loadbalancer_vars.yml\n</code></pre> <p>Locate the encrypted kubeconfigs <code>kube_config_*.yaml</code> and edit them using sops. Copy the IP shown above into <code>kube_config_*.yaml</code>. Do not overwrite the port.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml\ndone\n</code></pre>"},{"location":"operator-manual/azure/#test-access-to-the-clusters-as-follows","title":"Test access to the clusters as follows","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get nodes'\ndone\n</code></pre>"},{"location":"operator-manual/azure/#deploy-rook","title":"Deploy Rook","text":"<p>To deploy Rook, please go to the <code>compliantkubernetes-kubespray</code> repo root directory and run the following.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops --decrypt ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml &gt; $CLUSTER.yaml\n    export KUBECONFIG=$CLUSTER.yaml\n    ./rook/deploy-rook.sh\n    shred -zu $CLUSTER.yaml\ndone\n</code></pre> <p>Please restart the operator pod, <code>rook-ceph-operator*</code>, if some pods stalls in initialization state as shown below:</p> <pre><code>rook-ceph     rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg    0/1     Init:0/2   0          24m\nrook-ceph     rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh   0/1     Init:0/2   0          36m\nrook-ceph     rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55   0/1     Init:0/2   0          14m\n</code></pre> <p>Important</p> <p>Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances.</p>"},{"location":"operator-manual/azure/#test-rook","title":"Test Rook","text":"<p>To test Rook, proceed as follows:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml 'kubectl --kubeconfig {} apply -f https://raw.githubusercontent.com/rook/rook/release-1.5/cluster/examples/kubernetes/ceph/csi/rbd/pvc.yaml';\ndone\n\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml 'kubectl --kubeconfig {} get pvc';\ndone\n</code></pre> <p>You should see PVCs in Bound state. If you want to clean the previously created PVCs:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml 'kubectl --kubeconfig {} delete pvc rbd-pvc';\ndone\n</code></pre>"},{"location":"operator-manual/azure/#deploying-compliant-kubernetes-apps","title":"Deploying Compliant Kubernetes Apps","text":"<p>Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps.</p>"},{"location":"operator-manual/azure/#clone-compliantkubernetes-apps-and-install-pre-requisites","title":"Clone <code>compliantkubernetes-apps</code> and Install Pre-requisites","text":"<p>If you haven't done so already, clone the <code>compliantkubernetes-apps</code> repo and install pre-requisites.</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes-apps.git\ncd compliantkubernetes-apps\nansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127.0.0.1, get-requirements.yaml\n</code></pre>"},{"location":"operator-manual/azure/#initialize-the-apps-configuration","title":"Initialize the apps configuration","text":"<pre><code>export CK8S_ENVIRONMENT_NAME=my-environment-name\n#export CK8S_FLAVOR=[dev|prod] # defaults to dev\nexport CK8S_CONFIG_PATH=~/.ck8s/my-cluster-path\nexport CK8S_CLOUD_PROVIDER=# [exoscale|safespring|citycloud|aws|baremetal]\nexport CK8S_PGP_FP=&lt;your GPG key fingerprint&gt;  # retrieve with gpg --list-secret-keys\n\n./bin/ck8s init\n</code></pre> <p>This will initialise the configuration in the <code>${CK8S_CONFIG_PATH}</code> directory. Generating configuration files <code>sc-config.yaml</code> and <code>wc-config.yaml</code>, as well as secrets with randomly generated passwords in <code>secrets.yaml</code>. This will also generate read-only default configuration under the directory <code>defaults/</code> which can be used as a guide for available and suggested options.</p> <pre><code>ls -l $CK8S_CONFIG_PATH\n</code></pre>"},{"location":"operator-manual/azure/#configure-the-apps","title":"Configure the apps","text":"<p>Edit the configuration files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code>, <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/secrets.yaml</code> and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted.</p> <pre><code>vim ${CK8S_CONFIG_PATH}/sc-config.yaml\nvim ${CK8S_CONFIG_PATH}/wc-config.yaml\nsops ${CK8S_CONFIG_PATH}/secrets.yaml\n</code></pre> <p>Tip</p> <p>The default configuration for the service cluster and workload cluster are available in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code> and can be used as a reference for available options.</p> <p>Warning</p> <p>Do not modify the read-only default configurations files found in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code>. Instead configure the cluster by modifying the regular files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> as they will override the default options.</p> <p>The following are the minimum change you should perform:</p> <pre><code># ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml\nglobal:\nbaseDomain: \"set-me\"  # set to &lt;enovironment_name&gt;.$DOMAIN\nopsDomain: \"set-me\"  # set to ops.&lt;environment_name&gt;.$DOMAIN\nissuer: letsencrypt-prod\n\nobjectStorage:\ntype: \"s3\"\ns3:\nregion: \"set-me\"  # Region for S3 buckets, e.g, west-1\nregionEndpoint: \"set-me\"  # e.g., https://s3.us-west-1.amazonaws.com\n\nstorageClasses:\ndefault:  rook-ceph-block\nnfs:\nenabled: false\ncinder:\nenabled: false\nlocal:\nenabled: false\nebs:\nenabled: false\n</code></pre> <pre><code># ${CK8S_CONFIG_PATH}/sc-config.yaml\ningressNginx:\ncontroller:\nservice:\ntype: \"this-is-not-used\"\nannotations: \"this-is-not-used\"\n\nharbor:\noidc:\ngroupClaimName: \"set-me\" # set to group claim name used by OIDC provider\n\nissuers:\nletsencrypt:\nprod:\nemail: \"set-me\"  # set this to an email to receive LetsEncrypt notifications\nstaging:\nemail: \"set-me\"  # set this to an email to receive LetsEncrypt notifications\n</code></pre> <pre><code># ${CK8S_CONFIG_PATH}/secrets.yaml\nobjectStorage:\ns3:\naccessKey: \"set-me\" #set to your s3 accesskey\nsecretKey: \"set-me\" #set to your s3 secretKey\n</code></pre>"},{"location":"operator-manual/azure/#install-compliant-kubernetes-apps","title":"Install Compliant Kubernetes apps","text":"<p>Start with the service cluster:</p> <pre><code>ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml\n./bin/ck8s apply sc  # Respond \"n\" if you get a WARN\n</code></pre> <p>Then the workload clusters:</p> <pre><code>for CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n    ./bin/ck8s apply wc  # Respond \"n\" if you get a WARN\ndone\n</code></pre>"},{"location":"operator-manual/azure/#settling","title":"Settling","text":"<p>Important</p> <p>Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes.</p> <p>You can check if the system settled as follows:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces pods'\ndone\n</code></pre> <p>Check the output of the command above. All Pods needs to be Running or Completed.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates'\ndone\n</code></pre> <p>Check the output of the command above. All resources need to have the Ready column True.</p>"},{"location":"operator-manual/azure/#testing","title":"Testing","text":"<p>After completing the installation step you can test if the apps are properly installed and ready using the commands below.</p> <p>Start with the service cluster:</p> <pre><code>ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml\n./bin/ck8s test sc  # Respond \"n\" if you get a WARN\n</code></pre> <p>Then the workload clusters:</p> <pre><code>for CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n    ./bin/ck8s test wc  # Respond \"n\" if you get a WARN\ndone\n</code></pre> <p>Done. Navigate to the endpoints, for example <code>grafana.$BASE_DOMAIN</code>, <code>kibana.$BASE_DOMAIN</code>, <code>harbor.$BASE_DOMAIN</code>, etc. to discover Compliant Kubernetes's features.</p>"},{"location":"operator-manual/azure/#teardown","title":"Teardown","text":""},{"location":"operator-manual/azure/#removing-compliant-kubernetes-apps-from-your-cluster","title":"Removing Compliant Kubernetes Apps from your cluster","text":"<p>To remove the applications added by compliant kubernetes you can use the two scripts <code>clean-sc.sh</code> and <code>clean-wc.sh</code>, they are located here in the scripts folder.</p> <p>They perform the following actions:</p> <ol> <li>Delete the added helm charts</li> <li>Delete the added namespaces</li> <li>Delete any remaining PersistentVolumes</li> <li>Delete the added CustomResourceDefinitions</li> </ol> <p>Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.</p>"},{"location":"operator-manual/azure/#remove-infrastructure","title":"Remove infrastructure","text":"<p>To teardown the cluster, please go to the <code>compliantkubernetes-kubespray</code> repo root directory and run the following.</p> <pre><code>pushd kubespray/contrib/azurerm\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nansible-playbook generate-templates.yml\n    az group deployment create -g \"$CLUSTER\" --template-file ./$CLUSTER/.generated/clear-rg.json --mode Complete\ndone\npopd\n</code></pre>"},{"location":"operator-manual/azure/#further-reading","title":"Further Reading","text":"<ul> <li>Elastisys Compliant Kubernetes Kubespray</li> <li>Compliant Kubernetes apps repo</li> <li>Configurations option</li> </ul>"},{"location":"operator-manual/break-glass/","title":"Break-glass","text":"<p>In this section we  describe a workaround when access  to the environment is broken for  the kubernetes administrators/operators and/or users.</p>","tags":["HIPAA S45 - Access Control - Emergency Access Procedure - \u00a7 164.312(a)(2)(ii)"]},{"location":"operator-manual/break-glass/#kubernetes-administrator-access","title":"Kubernetes Administrator Access","text":"<p>When Dex or the OpenID provider is malfunctioning, the Kubernetes administrator might be unable to access the cluster. The following steps will give you temporary access sufficient for troubleshooting and recovery:</p> <ol> <li> <p><code>SSH</code> to one of the control-plane nodes.</p> </li> <li> <p>Use <code>/etc/kubernetes/admin.conf</code> and run <code>kubectl</code> commands to check the problem</p> <pre><code>export KUBECONFIG=/etc/kubernetes/admin.conf\n#run kubctl command\nsudo kubectl get po -A\n</code></pre> </li> </ol> <p>NOTE: This is a temporary solution and access should be disabled once the issue with dex is resolved.</p> <p>If dex is broken, you can manually create a <code>kubeconfig</code> file for a user. While there are different ways to create <code>kubeconfig</code> files, we will will use the X.509 client certificates with OpenSSL. Follow the steps below to create a user <code>kubeconfig</code> file.</p> <ol> <li>Create a private key:     <pre><code>openssl genrsa -out user1.key 2048\n</code></pre></li> <li>Create a certificate signing request (CSR). <code>CN</code> is the username and <code>O</code> the group.     <pre><code>openssl req -new -key user1.key \\\n-out user1.csr \\\n-subj \"/CN=user1/O=companyname\"\n</code></pre></li> <li>Get the Base64 encoding for the generated CSR file.     <pre><code>cat user1.csr | base64 | tr -d '\\n'\n</code></pre></li> <li>Create a Certificate Signing Request with Kubernetes     <pre><code>cat &lt;&lt;EOF | kubectl  apply -f -\napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n    name: user1\nspec:\n    groups:\n    - system:authenticated\n    request: # put here the  Base64 encoded text for the CRS that you get in step 3\n    signerName: kubernetes.io/kube-apiserver-client\n    usages:\n    - client auth\nEOF\n</code></pre></li> <li>Approve the CSR     <pre><code>kubectl certificate approve user1\n</code></pre></li> <li> <p>Get the certificate.     Retrieve the certificate from the CSR:     <pre><code>kubectl get csr/user1 -o yaml\n</code></pre>     The certificate value is in Base64-encoded format under <code>status.certificate</code>. Put the content under <code>client-certificate-data:</code>.  And also get the base64 encoded content for the private key and put it under <code>client-key-data:</code>. To get the base64 encoded content <code>cat user1.key | base64 | tr -d '\\n'</code>.</p> <p>The kubeconfig file for <code>user1</code> user looks like:</p> <pre><code>```yaml\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: &lt;CA&gt;\n    server: https://control-node-ip:6443 # ip address of one of the control nodes\n\nname: &lt;cluster-name&gt;\ncontexts:\n- context:\n    cluster: &lt;cluster-name&gt;\n    user: user1 # &lt;USER&gt;\nname: &lt;USER&gt;@&lt;CLUSTER-NAME&gt;\nkind: Config\nusers:\n- name: user1\nuser:\n    client-certificate-data: &lt;CLIENT-CRT-DATA&gt;\n    client-key-data: &lt;CLIENT-KEY-DATA&gt;\n```\n</code></pre> <ol> <li>Add the user and  namespaces that s/he has access to in wc-config.yaml file.</li> </ol> <pre><code>user:\n# This only controls if the namespaces should be created, user RBAC is always created.\ncreateNamespaces: true\nnamespaces:\n- namespace1 # namespaces that the user is allowed to access\nadminUsers:\n- user1 # the user\n</code></pre> </li> </ol>","tags":["HIPAA S45 - Access Control - Emergency Access Procedure - \u00a7 164.312(a)(2)(ii)"]},{"location":"operator-manual/break-glass/#kubernetes-user-access","title":"Kubernetes User Access","text":"","tags":["HIPAA S45 - Access Control - Emergency Access Procedure - \u00a7 164.312(a)(2)(ii)"]},{"location":"operator-manual/capacity-management/","title":"Capacity Management","text":"<p>Our users trust us -- the Compliant Kubernetes administrators -- to keep applications up and secure. Keeping the application up means that there is sufficient capacity in the environment, both for headroom -- in case the application suddenly gets popular -- and resilience to Node or Zone failure. Keeping the application secure means having sufficient capacity in the environment to allow rolling Node restarts -- as required for keeping the base OS up-to-date and secure -- without causing downtime.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"operator-manual/capacity-management/#types-of-failure","title":"Types of Failure","text":"<p>Compliant Kubernetes environments are set up to withstand either:</p> <ul> <li>a single Node failure (Node resilient); or</li> <li>a single Zone failure (Zone resilient).</li> </ul> <p>Zone resilient environments are set up over three Zones, two active and one arbiter. The arbiter only runs some control-plane components (e.g., Kubernetes Data Plane, Ceph Mon), whereas the active Zones run both control-plane and data-plane components (e.g., Ceph OSD, Kubernetes data plane Nodes).</p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"operator-manual/capacity-management/#upscaling","title":"Upscaling","text":"","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"operator-manual/capacity-management/#when","title":"When?","text":"<p>Compliant Kubernetes triggers a P2 alert when any capacity dimension is predicted to exceed 66% (for Node resilient) or 50% (for Zone resilient) within 3 days.</p> <p>That was very information dense, so let's break it down.</p> <ul> <li>Why a P2 alert? P2 alerts are events that need to be dealt with within a business day. Capacity can be easily predicted and added in advance. Excess capacity is cheaper than frustrated administrators. There is no need to disturb anyone's sleep.</li> <li>What capacity dimensions?<ul> <li>CPU:<ul> <li>sum of (Kubernetes) CPU requests to CPU allocatable;</li> <li>sum of CPU used to CPU allocatable;</li> <li>load average: since this is not a percentage, scale up when above 3;</li> </ul> </li> <li>Memory:<ul> <li>sum of (Kubernetes) memory request to memory allocatable;</li> <li>sum of memory non-available to memory total;</li> </ul> </li> <li>Storage:<ul> <li>host disk used to size;</li> <li>PersistentVolumeClaim used to size;</li> <li>Rook/Ceph OSD used to size;</li> </ul> </li> </ul> </li> <li>Why 66% or 50%? Most Node-resilient Kubernetes clusters will feature 3 Nodes (see discussion below about too many Nodes). Hence, 1 extra Node means 66% capacity. Zone-resilient environments need 50% extra capacity, so that each active Zone can take over the load of the other active Zone.</li> <li>Why within 3 days? This should ensure sufficient time to act on the capacity shortage, without ruining anyone's weekend.</li> </ul> <p>Note</p> <p>Compliant Kubernetes can be configured to require resource requests for all Pods.</p> <p>Important</p> <p>Nodes dedicated for data services, such as PostgreSQL, are excluded from Kubernetes requests to allocatable calculation.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"operator-manual/capacity-management/#how","title":"How?","text":"<p>Add a new Node of the same type as the other Nodes in the cluster.</p> <p>If the cluster has 6 Nodes, consider consolidating to 3 Nodes of twice-the-size -- in number of CPUs or memory or both -- if the infrastructure cost is reduced. Before doing this, get in touch with application developers to ensure they don't have Kubernetes scheduling constraints that would cause issues on the consolidated environment.</p> <p>If you are about to double the number of Nodes, get in touch with application developers to ensure their application is not misbehaving, before upscaling.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"operator-manual/capacity-management/#optimization","title":"Optimization","text":"<p>If the cluster has at least 5 Nodes, consider reducing the watermark to 80% to reduce extra capacity.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"operator-manual/capacity-management/#downscaling","title":"Downscaling","text":"<p>We hope that the applications we host will only grow in popularity and that downscaling is never needed. Nevertheless, application developer trust us to keep infrastructure costs down, if their application hasn't gone viral -- yet!</p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"operator-manual/capacity-management/#when_1","title":"When?","text":"<p>The capacity of the environment should be regularly reviewed, for example, after a maintenance window.</p> <p>Important</p> <p>Downscaling may put application uptime at risk. Therefore, be conservative when downscaling.</p> <p>Before downscaling you should:</p> <ul> <li>Evaluate the capacity trends in last 3 to 6 months and take decision based on that. Notice that capacity usage may be smaller during weekends, at the beginning or end of the month, during vacation periods, etc.</li> <li>Ask the user if the reduction in capacity usage is a real trend, and not just sporadic due to quiet periods or vacation periods. E.g., an EdTech app won't be used as intensively during school holidays.</li> <li>Ask the user if they foresee any increase in capacity due to new app releases or new apps additions or something that will require more resources.</li> </ul>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"operator-manual/capacity-management/#how_1","title":"How?","text":"<p>If any capacity dimension -- as defined above -- was below 33% for at least 3 days, then remove one Node at a time, until capacity is above 33%. Make sure to drain and cordon the Node before decommissioning it.</p> <p>If you are about to go below 3 Nodes, consider replacing the Nodes with 6 Nodes of half-the-size before downscaling. Before doing this, get in touch with application developers to ensure they don't have Kubernetes scheduling constraints that would  cause issues on the consolidated environment.</p> <p>If you are about to half the number of Nodes, get in touch with application developers to ensure their application is not misbehaving, before downscaling.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"operator-manual/capacity-management/#optimization_1","title":"Optimization","text":"<p>Removing capacity is more dangerous than having extra capacity, when it comes to application uptime. Furthermore, we need to avoid oscillations: Removing capacity to only add it back a few days later is no fun for the administrator. Therefore, downscaling should only be performed periodically, whereas upscaling should be performed as soon as predictions show it is needed.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"operator-manual/clean-up/","title":"Removing Compliant Kubernetes Apps from your cluster","text":"<p>To remove the applications added by Compliant Kubernetes you can use the two scripts <code>clean-sc.sh</code> and <code>clean-wc.sh</code>, they are located here in the scripts folder.</p> <p>They perform the following actions:</p> <ol> <li>Delete the added helm charts</li> <li>Delete the added namespaces</li> <li>Delete any remaining PersistentVolumes</li> <li>Delete the added CustomResourceDefinitions</li> </ol> <p>Note</p> <p>If user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.</p>"},{"location":"operator-manual/clock-synchronization/","title":"Clock Synchronization","text":"<p>Important</p> <p>Compliant Kubernetes is about to add clock synchronization with <code>ntp.se</code> by default. This will be done according to best practices. To check progress, see this issue.</p> <p>Clock synchronization is important for the following reasons:</p> <ul> <li>Several Kubernetes components, in particular etcd, Rook/Ceph, do not work correctly if Nodes' clock drifts by more than 100ms;</li> <li>Several data protection regulations require it.</li> </ul> <p>Be aware of the following:</p> <ul> <li>Some regulations require synchronization with at least two clock sources. This could be two different NTP servers which can be traced to two different atomic clocks.</li> <li>Some regulations require synchronization with a specific time source. For example, MSBFS 2020:7 4 kap. 13 \u00a7 specifically requires synchronization with ntp.se.</li> </ul>","tags":["ISO 27001 A.12.4.4 Clock Synchronization","MSBFS 2020:7 4 kap. 13 \u00a7"]},{"location":"operator-manual/cluster-sizing/","title":"Cluster Sizing","text":"<p>A full Compliant Kubernetes deployment requires a cluster with at least 40 CPUs and 82 GB of memory in total.</p>"},{"location":"operator-manual/cluster-sizing/#monitoring","title":"Monitoring","text":"<p>Monitoring stack (Thanos) can handle 6000 metrics per second in our standard configuration. This can be increased on demand.</p>"},{"location":"operator-manual/cluster-sizing/#logging","title":"Logging","text":"<p>Logging stack (OpenSearch) can take 100 records per second while provisioned with 12 CPUs and 24 GB of memory.</p>"},{"location":"operator-manual/common/","title":"Common","text":""},{"location":"operator-manual/common/#deploy-rook","title":"Deploy Rook","text":"<p>To deploy Rook, please go to the <code>compliantkubernetes-kubespray</code> repo root directory and run the following.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops --decrypt ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml &gt; $CLUSTER.yaml\n    export KUBECONFIG=$CLUSTER.yaml\n    ./rook/deploy-rook.sh\n    shred -zu $CLUSTER.yaml\ndone\n</code></pre> <p>Please restart the operator pod, <code>rook-ceph-operator*</code>, if some pods stalls in initialization state as shown below:</p> <pre><code>rook-ceph     rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg    0/1     Init:0/2   0          24m\nrook-ceph     rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh   0/1     Init:0/2   0          36m\nrook-ceph     rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55   0/1     Init:0/2   0          14m\n</code></pre> <p>Important</p> <p>Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances.</p>"},{"location":"operator-manual/common/#test-rook","title":"Test Rook","text":"<p>To test Rook, proceed as follows:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml 'kubectl --kubeconfig {} apply -f https://raw.githubusercontent.com/rook/rook/release-1.5/cluster/examples/kubernetes/ceph/csi/rbd/pvc.yaml';\ndone\n\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml 'kubectl --kubeconfig {} get pvc';\ndone\n</code></pre> <p>You should see PVCs in Bound state. If you want to clean the previously created PVCs:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml 'kubectl --kubeconfig {} delete pvc rbd-pvc';\ndone\n</code></pre> <p>Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps.</p>"},{"location":"operator-manual/common/#clone-compliantkubernetes-apps-and-install-pre-requisites","title":"Clone <code>compliantkubernetes-apps</code> and Install Pre-requisites","text":"<p>If you haven't done so already, clone the <code>compliantkubernetes-apps</code> repo and install pre-requisites.</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes-apps.git\ncd compliantkubernetes-apps\nansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127.0.0.1, get-requirements.yaml\n</code></pre>"},{"location":"operator-manual/common/#initialize-the-apps-configuration","title":"Initialize the apps configuration","text":"<pre><code>export CK8S_ENVIRONMENT_NAME=my-environment-name\n#export CK8S_FLAVOR=[dev|prod] # defaults to dev\nexport CK8S_CONFIG_PATH=~/.ck8s/my-cluster-path\nexport CK8S_CLOUD_PROVIDER=# [exoscale|safespring|citycloud|aws|baremetal]\nexport CK8S_PGP_FP=&lt;your GPG key fingerprint&gt;  # retrieve with gpg --list-secret-keys\n\n./bin/ck8s init\n</code></pre> <p>This will initialise the configuration in the <code>${CK8S_CONFIG_PATH}</code> directory. Generating configuration files <code>sc-config.yaml</code> and <code>wc-config.yaml</code>, as well as secrets with randomly generated passwords in <code>secrets.yaml</code>. This will also generate read-only default configuration under the directory <code>defaults/</code> which can be used as a guide for available and suggested options.</p> <pre><code>ls -l $CK8S_CONFIG_PATH\n</code></pre>"},{"location":"operator-manual/common/#configure-the-apps","title":"Configure the apps","text":"<p>Edit the configuration files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code>, <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/secrets.yaml</code> and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted.</p> <pre><code>vim ${CK8S_CONFIG_PATH}/sc-config.yaml\nvim ${CK8S_CONFIG_PATH}/wc-config.yaml\nsops ${CK8S_CONFIG_PATH}/secrets.yaml\n</code></pre> <p>Tip</p> <p>The default configuration for the service cluster and workload cluster are available in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code> and can be used as a reference for available options.</p> <p>Warning</p> <p>Do not modify the read-only default configurations files found in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code>. Instead configure the cluster by modifying the regular files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> as they will override the default options.</p>"},{"location":"operator-manual/common/#install-compliant-kubernetes-apps","title":"Install Compliant Kubernetes apps","text":"<p>Start with the service cluster:</p> <pre><code>ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml\n./bin/ck8s apply sc  # Respond \"n\" if you get a WARN\n</code></pre> <p>Then the workload clusters:</p> <pre><code>for CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n    ./bin/ck8s apply wc  # Respond \"n\" if you get a WARN\ndone\n</code></pre>"},{"location":"operator-manual/common/#settling","title":"Settling","text":"<p>Important</p> <p>Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes.</p> <p>You can check if the system settled as follows:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces pods'\ndone\n</code></pre> <p>Check the output of the command above. All Pods needs to be Running or Completed.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates'\ndone\n</code></pre> <p>Check the output of the command above. All resources need to have the Ready column True.</p>"},{"location":"operator-manual/common/#testing","title":"Testing","text":"<p>After completing the installation step you can test if the apps are properly installed and ready using the commands below.</p> <p>Start with the service cluster:</p> <pre><code>ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml\n./bin/ck8s test sc  # Respond \"n\" if you get a WARN\n</code></pre> <p>Then the workload clusters:</p> <pre><code>for CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n    ./bin/ck8s test wc  # Respond \"n\" if you get a WARN\ndone\n</code></pre> <p>Done. Navigate to the endpoints, for example <code>grafana.$BASE_DOMAIN</code>, <code>kibana.$BASE_DOMAIN</code>, <code>harbor.$BASE_DOMAIN</code>, etc. to discover Compliant Kubernetes's features.</p>"},{"location":"operator-manual/common/#removing-compliant-kubernetes-apps-from-your-cluster","title":"Removing Compliant Kubernetes Apps from your cluster","text":"<p>To remove the applications added by compliant kubernetes you can use the two scripts <code>clean-sc.sh</code> and <code>clean-wc.sh</code>, they are located here in the scripts folder.</p> <p>They perform the following actions:</p> <ol> <li>Delete the added helm charts</li> <li>Delete the added namespaces</li> <li>Delete any remaining PersistentVolumes</li> <li>Delete the added CustomResourceDefinitions</li> </ol> <p>Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.</p>"},{"location":"operator-manual/common/#create-s3-buckets","title":"Create S3 buckets","text":"<p>You can use the following script to create required S3 buckets. The script uses <code>s3cmd</code> in the background and gets configuration and credentials for your S3 provider from <code>${HOME}/.s3cfg</code> file.</p> <pre><code># Use your default s3cmd config file: ${HOME}/.s3cfg\nscripts/S3/entry.sh create\n</code></pre> <p>Important</p> <p>You should not use your own credentials for S3. Rather create a new set of credentials with write-only access, when supported by the object storage provider (check a feature matrix).</p>"},{"location":"operator-manual/common/#test-s3","title":"Test S3","text":"<p>To ensure that you have configured S3 correctly, run the following snippet:</p> <pre><code>(\naccess_key=$(sops exec-file ${CK8S_CONFIG_PATH}/secrets.yaml 'yq r {} \"objectStorage.s3.accessKey\"')\nsecret_key=$(sops exec-file ${CK8S_CONFIG_PATH}/secrets.yaml 'yq r {} \"objectStorage.s3.secretKey\"')\nsc_config=$(yq m ${CK8S_CONFIG_PATH}/defaults/sc-config.yaml ${CK8S_CONFIG_PATH}/sc-config.yaml -a overwrite -x)\nregion=$(echo ${sc_config} | yq r - 'objectStorage.s3.region')\nhost=$(echo ${sc_config} | yq r -  'objectStorage.s3.regionEndpoint')\n\nfor bucket in $(echo ${sc_config} | yq r -  'objectStorage.buckets.*'); do\ns3cmd --access_key=${access_key} --secret_key=${secret_key} \\\n--region=${region} --host=${host} \\\nls s3://${bucket} &gt; /dev/null\n        [ ${?} = 0 ] &amp;&amp; echo \"Bucket ${bucket} exists!\"\ndone\n)\n</code></pre> <p>This page is out of date</p> <p>We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date.</p> <p>Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly.</p>"},{"location":"operator-manual/configure/","title":"Advanced Configuration","text":"<p>You have already been exposed to some of Compliant Kubernetes's configuration options while creating a cluster. If not, read that section first (e.g., On Exoscale).</p> <p>This section will outline some advanced configuration topics.</p>"},{"location":"operator-manual/configure/#overview","title":"Overview","text":"<p>Compliant Kubernetes is composed of two layers, the Kubernetes layer and the apps layer. Each is configured slightly differently.</p>"},{"location":"operator-manual/configure/#kubernetes-layer","title":"Kubernetes Layer","text":"<p>To find all configuration option of the Kubernetes layer, please read the upstream Kubespray documentation. Compliant Kubernetes overrides some of Kubespray's defaults, as shown here.</p>"},{"location":"operator-manual/configure/#apps-layer","title":"Apps Layer","text":"<p>The configuration of the apps layer is documented as comments in the <code>sc-config.yaml</code>, <code>wc-config.yaml</code> and <code>secrets.yaml</code> files generated by <code>./bin/ck8s init</code>. You can find the default here. If you find a configuration key is insufficiently documented, please open a PR.</p>"},{"location":"operator-manual/credentials/","title":"Use of Credentials","text":"<p>Compliant Kubernetes interacts with a lot of credentials. This document captures all of them in an orderly fashion, layer-by-layer.</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#terminology","title":"Terminology","text":"<ul> <li>Purpose: Why are these credentials necessary, what can be done with them.</li> <li>Owner: The person (e.g., John Smith) or computing system (e.g., control plane Node, Pod) who controls the credentials, and is responsible for their safe storage and usage.</li> <li>Type: Individual credentials identify a person, while service accounts identify a computing system.</li> <li>Use for: What should these credentials be used for.</li> <li>Do not use for: When should these credentials NOT be used, although they technically could.</li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#single-sign-on-sso-credentials","title":"Single Sign-On (SSO) Credentials","text":"<ul> <li>Example: Company Google Accounts</li> <li>Purpose: authenticate a person with various system, in particular<ul> <li>Kubernetes API via Dex</li> <li>Grafana via Dex</li> <li>OpenSearch Dashboards via Dex</li> <li>Harbor via Dex</li> </ul> </li> <li>Owner: individual person (user or administrator)</li> <li>Type: individual credentials</li> <li>Use for: identifying yourself</li> <li>Do not use for:<ul> <li>These credentials are super valuable and should not be shared with anyone, not even family, friends, workmates, etc., even if requested. Report such sharing requests.</li> </ul> </li> <li>Misc:<ul> <li>Protect using 2FA</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#cloud-provider-infrastructure-credentials","title":"Cloud Provider (Infrastructure) Credentials","text":"<ul> <li>Purpose: create infrastructure, e.g., VMs, load balancers, networks, buckets.</li> <li>Owner: administrator</li> <li>Type: individual credentials</li> <li>Use for:<ul> <li>Terraform layer in Kubespray</li> <li>Creating and destroying buckets via helper scripts</li> </ul> </li> <li>Do not use for:<ul> <li>Kubernetes cloud-controller integration, use Cloud Controller Credentials instead.</li> <li>Access to object storage / S3 bucket, use backup credentials instead.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#ssh-keys","title":"SSH Keys","text":"<ul> <li>Purpose: access Nodes for setup, break glass or disaster recovery</li> <li>Owner: administrator</li> <li>Type: individual credentials</li> <li>Use for:<ul> <li>Accessing Nodes via SSH</li> </ul> </li> <li>Do not use for:<ul> <li>Giving a system access to a Git repository. Create a separate SSH key only for that purpose instead.</li> </ul> </li> <li>Important considerations:<ul> <li>When generating an SSH key, see Cryptography.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#pgp-keys","title":"PGP Keys","text":"<ul> <li>Purpose: encrypt/decrypt sensitive information, e.g., service account credentials, customer names, incident reports, financial information, etc.</li> <li>Owner: administrator</li> <li>Type: individual credentials</li> <li>Use for:<ul> <li>Encrypting/decrypting sensitive information</li> </ul> </li> <li>Do not use for:<ul> <li>Encrypting/decrypting individual credentials. These are meant to be individual and never shared.</li> <li>Encrypting/decrypting SSH key. These are meant to be individual and never shared. Prefer protecting your SSH key with a passphrase or storing it on a YubiKey.</li> <li>Encrypting non-sensitive information. This leads to a culture of \"security by obscurity\" in which people over-rely on encryption. Prefer being mindful about what data you store and why. If unsure, prefer not storing credentials, as Cloud Provider Credentials and SSH keys should be enough to restore any access.</li> </ul> </li> <li>Important considerations:<ul> <li>When generating a GPG key, see Cryptography.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#cloud-controller-integration-credentials","title":"Cloud Controller (Integration) Credentials","text":"<ul> <li>Purpose: allow Kubernetes control Nodes, specifically the cloud-controller-manager, to create LoadBalancers and PersistentVolumes</li> <li>Owner: each Kubernetes cluster should have their own</li> <li>Type: service account</li> <li>Use for:<ul> <li>Configuring Kubespray to set up a Kubernetes cluster with cloud integration</li> </ul> </li> <li>Do not use for:<ul> <li>AWS. Use AWS IAM Node Roles instead.</li> <li>Exoscale. We currently don't integrate with Exoscale for LoadBalancer or PersistentVolumes.</li> <li>Terraform layer in Kubespray</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#backup-and-long-term-logging-credentials","title":"Backup and Long-Term Logging Credentials","text":"<ul> <li>Purpose:<ul> <li>Allow backup of various components, e.g., PVCs via Velero, Thanos metrics, OpenSearch Indexes, PostgreSQL databases.</li> <li>Allow long-term logging, e.g., Service Cluster logs</li> </ul> </li> <li>Owner: each Compliant Kubernetes cluster should have their own</li> <li>Type: service account</li> <li>Use for:<ul> <li>Backup</li> <li>Logging</li> </ul> </li> <li>Do not use for:<ul> <li>Other object storage, e.g., Harbor container images</li> <li>Disaster recovery, investigations. Use Cloud Provider credentials instead.</li> </ul> </li> <li>Misc:<ul> <li>Ensure these credentials are write-only, if supported by the underlying cloud provider, to comply with ISO 27001 A.12.3.1 Information Backup and ISO 27001 A.12.4.2 Protection of Log Information. As of 2021-05-20, this is supported by AWS S3, Exoscale S3, GCP and SafeSpring S3.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#opsgenie-credentials","title":"OpsGenie Credentials","text":"<ul> <li>Purpose:<ul> <li>Allow the Cluster to issue alerts to OpsGenie.</li> </ul> </li> <li>Owner: each Compliant Kubernetes cluster should have their own</li> <li>Type: service account</li> <li>Use for: alerting</li> <li>Do not use for:<ul> <li>Operator access to OpsGenie. Prefer Single Sign-On (SSO).</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#dex-openid-client-secret","title":"Dex OpenID Client Secret","text":"<ul> <li>Purpose:<ul> <li>Complete the \"OAuth dance\" between Grafana, OpenSearch Dashboard, Harbor and kubectl, on one side, and Dex, on the other side.</li> <li>Used both by administrators and users.</li> </ul> </li> <li>Owner: each Compliant Kubernetes cluster should have their own</li> <li>Type: not secret</li> <li>Misc:<ul> <li>We have determined that the OpenID client secret should not be treated as a secret. See risk analysis here and here.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#kubeconfig-with-openid-authentication","title":"Kubeconfig with OpenID Authentication","text":"<ul> <li>Purpose: access the Kubernetes API in normal situations</li> <li>Owner: shared between administrators and users</li> <li>Type: not secret</li> <li>Use for:<ul> <li>Routine checks</li> <li>Routine maintenance</li> <li>Investigations</li> <li>\"Simple\" recovery</li> </ul> </li> <li>Misc:<ul> <li>If these credentials become unusable, you are in a \"break glass\" situation. Use cloud provider credentials or SSH keys to initiate disaster recovery.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#kubeconfig-with-client-certificate-key","title":"Kubeconfig with Client Certificate Key","text":"<ul> <li>Purpose: access the Kubernetes API for disaster recovery, break glass or initial setup</li> <li>Owner: shared between administrators</li> <li>Type: special</li> <li>Use for:<ul> <li>Initial setup</li> <li>Break glass</li> <li>Disaster recovery</li> </ul> </li> <li>Do not use for:<ul> <li>Routine maintenance or investigation. Use Kubeconfig with OpenID Authentication</li> </ul> </li> <li>Misc:<ul> <li>Such a Kubeconfig is available on all control plane Nodes at <code>/etc/kubernetes/admin.conf</code>. SSH into a control plane Node then type <code>sudo su</code> and you can readily use <code>kubectl</code> commands.</li> <li>Unless absolutely necessary, avoid storing this file outside the control plane Nodes.</li> <li>If, for some good reason, you downloaded this file, <code>shred</code> it after usage.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/cryptography/","title":"Use of Cryptography","text":"<p>Compliant Kubernetes recommends the ECRYPT-CSA \"near term use\". The key cryptographic parameters are listed below.</p>","tags":["MSBFS 2020:7 4 kap. 9 \u00a7","ISO 27001 A.10 Cryptography","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)"]},{"location":"operator-manual/cryptography/#recommended-strengths","title":"Recommended Strengths","text":"Cryptographic Structure Size Symmetric 128 Factoring Modulus 3072 Discrete Logarithm 256/3072 Elliptic Group 256 Hash 256","tags":["MSBFS 2020:7 4 kap. 9 \u00a7","ISO 27001 A.10 Cryptography","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)"]},{"location":"operator-manual/cryptography/#recommended-algorithms","title":"Recommended Algorithms","text":"Function Algorithm Block Ciphers AESCamelliaSerpent Hash Functions SHA-2 (256, 384, 512, 512/256)SHA-3 (256, 384, 512, SHAKE128, SHAKE256)Whirlpool (512)BLAKE (256, 584, 512) Public Key Primitive RSA (&gt;3072)  DSA (&gt;256/3072)  ECDSA (&gt;256)","tags":["MSBFS 2020:7 4 kap. 9 \u00a7","ISO 27001 A.10 Cryptography","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)"]},{"location":"operator-manual/cryptography/#recommended-implementation","title":"Recommended Implementation","text":"<p>Ubuntu 22.04 already generates SSH and GPG keys conforming to this recommendation, as evidenced below:</p> <pre><code>$ ssh-keygen\nGenerating public/private rsa key pair.\n[...]\n+---[RSA 3072]----+\n|           o+.=++|\n|           +o..= |\n|        = =...o  |\n|       O @.    o |\n|      . S +.  . .|\n|       + B  .. .E|\n|      . O o ..o  |\n|       o + +o... |\n|          +oo=o  |\n+----[SHA256]-----+\n$ gpg --generate-key\ngpg (GnuPG) 2.2.27; Copyright (C) 2021 Free Software Foundation, Inc.\n[...]\npub   rsa3072 2023-03-24 [SC] [expires: 2025-03-23]\n      41E32D8838ADA81B4D57333E79797753D349F087\nuid                      Cristian Klein &lt;cristian.klein@example.com&gt;\nsub   rsa3072 2023-03-24 [E] [expires: 2025-03-23]\n</code></pre>","tags":["MSBFS 2020:7 4 kap. 9 \u00a7","ISO 27001 A.10 Cryptography","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)"]},{"location":"operator-manual/cryptography/#notes-on-https-traffic","title":"Notes on HTTPS Traffic","text":"<p>For HTTPS traffic, Compliant Kubernetes allows either TLS 1.2 or TLS 1.3. TLS 1.3 mandates forward secrecy. TLS 1.2 makes forward secrecy optional, however, the default cipher list in Compliant Kubernetes prioritizes algorithms that provide perfect forward secrecy. In brief, you can rely on forward secrecy with most browsers in use today.</p> <p>Forward secrecy addresses the \"store now, decrypt later\" attack. In essence, an attacker cannot decrypt past HTTPS transmissions even if the TLS certificate (private key) is compromised.</p> <p>Compliant Kubernetes uses RSA 2048 when provisioning HTTPS certificates, which is lower than the present recommendation. However, these certificates have a short expiration time of 3 months. Hence, with short certificate expiration time and forward secrecy, usage of RSA 2048 for HTTPS certificates does not add a security risk.</p> <p>We recommend you to regularly run the Qualys SSL Server Test against the application HTTPS endpoints to make sure encrypted-in-transit sufficiently protects your data. At the time of this writing, Compliant Kubernetes receives A+ overall rating.</p>","tags":["MSBFS 2020:7 4 kap. 9 \u00a7","ISO 27001 A.10 Cryptography","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)"]},{"location":"operator-manual/cryptography/#further-reading","title":"Further Reading","text":"<ul> <li>ECRYPT\u2013CSA D5.4 Algorithms, Key Size and Protocols Report (2018)</li> <li>BlueCrypt Cryptographic Key Length Recommendation</li> <li>Ingress Nginx SSL Ciphers</li> <li>Mozilla SSL recommendations</li> <li>Forward secrecy</li> </ul>","tags":["MSBFS 2020:7 4 kap. 9 \u00a7","ISO 27001 A.10 Cryptography","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)"]},{"location":"operator-manual/disaster-recovery/","title":"Disaster Recovery","text":"<p>This document details disaster recovery procedures for Compliant Kubernetes. These procedures must be executed by the administrator.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#compliant-need","title":"Compliant Need","text":"<p>Disaster recovery is mandated by several regulations and information security standards. For example, in ISO 27001:2013, the annexes that mostly concerns disaster recovery are:</p> <ul> <li>A.12.3.1 Information Backup</li> <li>A.17.1.1 Planning Information Security Continuity</li> </ul>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#object-storage-providers","title":"Object storage providers","text":"","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#feature-matrix","title":"Feature matrix","text":"Provider Write-only credentials AWS S3 Yes Citycloud S3 No Exoscale S3 Yes GCP Yes Safespring S3 Yes","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#off-site-backups","title":"Off-site backups","text":"<p>Backups can be set up to be replicated off-site using CronJobs.</p> <p>In version v0.23 these can be encrypted before they are sent off-site, which means they must first be restored to be usable.  It is possible to restore serviecs directly from unencrypted off-site backups with some additional steps.</p> <p>See the instructions in <code>compliantkubernetes-apps</code> for how to restore off-site backups.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#when-a-new-regioncloud-provider-is-used","title":"When a new region/cloud provider is used","text":"<ul> <li>Configure and set base ck8s-configs:</li> </ul> <p>sc-config.yaml:</p> <p><code>harbor.persistence.swift.*</code>,<code>objectStorage.sync.*</code></p> <p>common-config.yaml:</p> <p><code>objectStorage.s3.region</code>, <code>objectStorage.s3.regionEndpoint</code></p> <p>secrets.yaml:</p> <p><code>dex.connectors.*</code>, <code>harbor.persistence.swift.username</code>, <code>harbor.persistence.swift.password</code>, <code>objectStorage.s3.accessKey</code>, <code>objectStorage.s3.secretKey</code></p> <p>.state/s3cfg.ini:</p> <p><code>access_key</code>, <code>secret_key</code>, <code>host_base</code>, <code>host_bucket</code></p> <ul> <li>Configure and set custom ck8s-configs:</li> </ul> <p>Examples can be files containing Identity Provider, Cloud Provider, or DNS critical information.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#opensearch","title":"OpenSearch","text":"<p>Note</p> <p>For Open Distro for Elasticsearch used in v0.18 and earlier, the same approach for backup and restore can be used with different naming, using <code>elasticsearch</code> for the CronJob and <code>elastic</code> for the namespace instead of <code>opensearch</code>.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#backup","title":"Backup","text":"<p>OpenSearch is set up to store backups in an S3 bucket. There is a CronJob called <code>opensearch-backup</code> in the cluster that is invoking the snapshot process in OpenSearch.</p> <p>To take a snapshot on-demand, execute</p> <pre><code>./bin/ck8s ops kubectl sc -n opensearch-system create job --from=cronjob/opensearch-backup &lt;name-of-job&gt;\n</code></pre>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#restore","title":"Restore","text":"<p>Set the following variables</p> <ul> <li><code>user</code> - OpenSearch user with permissions to manage snapshots, usually <code>admin</code></li> <li><code>password</code> - password for the above user</li> <li><code>os_url</code> - URL to OpenSearch</li> </ul> <p>Restoring from off-site backup</p> <ul> <li> <p>To restore from an encrypted off-site backup:</p> <p>First import the backup into the main S3 service and register the restored bucket as a new snapshot repository: <pre><code>curl -kL -u \"${user}:${password}\" -X PUT \"${os_url}/_snapshot/backup-repository?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"type\": \"s3\",\n  \"settings\": {\n    \"bucket\": \"&lt;restored-bucket&gt;\",\n    \"readonly\": true\n  }\n}\n'\n</code></pre> Then restore from this snapshot repository (<code>backup-repositroy</code>) in OpenSearch.</p> </li> <li> <p>To restore from an unencrypted off-site backup:</p> <p>Configure the remote and bucket as the main S3 service and bucket for apps and OpenSearch respectively, then update the OpenSearch Helm releases and perform the restore. It is recommended to either suspend or remove the OpenSearch backup CronJob to prevent it from running while restoring.</p> <p>Remember to revert to the regular S3 service afterwards and reactivate the backup CronJob!  Replace the previous snapshot repository if it is unusable.</p> </li> </ul> <p>List snapshot repositories</p> <pre><code># Simple\n\u276f curl -kL -u \"${user}:${password}\" \"${os_url}/_cat/repositories?v\"\nid                   type\nopensearch-snapshots   s3\n\n# Detailed\n\u276f curl -kL -u \"${user}:${password}\" \"${os_url}/_snapshot/?pretty\"\n{\n\"opensearch-snapshots\" : {\n\"type\" : \"s3\",\n    \"settings\" : {\n\"bucket\" : \"opensearch-backup\",\n      \"client\" : \"default\"\n}\n}\n}\n</code></pre> <p>List available snapshots</p> <pre><code>snapshot_repo=&lt;name/id from previous step&gt;\n\n# Simple\n\u276f curl -kL -u \"${user}:${password}\" \"${os_url}/_cat/snapshots/${snapshot_repo}?v&amp;s=id\"\nid                         status start_epoch start_time end_epoch  end_time duration indices successful_shards failed_shards total_shards\nsnapshot-20211231_120002z SUCCESS 1640952003  12:00:03   1640952082 12:01:22     1.3m      54                54             0           54\nsnapshot-20220101_000003z SUCCESS 1640995203  00:00:03   1640995367 00:02:47     2.7m      59                59             0           59\nsnapshot-20220101_120002z SUCCESS 1641038403  12:00:03   1641038533 12:02:13     2.1m      57                57             0           57\n...\n\n# Detailed list of all snapshots\ncurl -kL -u \"${user}:${password}\" \"${os_url}/_snapshot/${snapshot_repo}/_all?pretty\"\n\n# Detailed list of specific snapshot\n\u276f curl -kL -u \"${user}:${password}\" \"${os_url}/_snapshot/${snapshot_repo}/snapshot-20220104_120002z?pretty\"\n{{\n\"snapshots\" : [\n{\n\"snapshot\" : \"snapshot-20220104_120002z\",\n      \"uuid\" : \"oClQdNAyTeiEmZb5dVh0SQ\",\n      \"version_id\" : 135238127,\n      \"version\" : \"1.2.3\",\n      \"indices\" : [\n\"authlog-default-2021.12.20-000001\",\n        \"authlog-default-2021.12.30-000011\",\n        \"authlog-default-2022.01.03-000015\",\n        \"other-default-2021.12.30-000011\",\n        ...\n      ],\n      \"data_streams\" : [ ],\n      \"include_global_state\" : false,\n      \"state\" : \"SUCCESS\",\n      \"start_time\" : \"2022-01-04T12:00:02.596Z\",\n      \"start_time_in_millis\" : 1641297602596,\n      \"end_time\" : \"2022-01-04T12:01:07.833Z\",\n      \"end_time_in_millis\" : 1641297667833,\n      \"duration_in_millis\" : 65237,\n      \"failures\" : [ ],\n      \"shards\" : {\n\"total\" : 66,\n        \"failed\" : 0,\n        \"successful\" : 66\n}\n}\n]\n}\n</code></pre> <p>You usually select the latest snapshot containing the indices you want to restore. Restore one or multiple indices from a snapshot</p> <p>Note</p> <p>You cannot restore a write index (the latest index) if you already have a write index connected to the same index alias (which will happen if you have started to receive logs).</p> <pre><code>snapshot_name=&lt;Snapshot name from previous step&gt;\nindices=\"&lt;list of comma separated indices/index patterns&gt;\"\n\ncurl -kL -u \"${user}:${password}\" -X POST \"${os_url}/_snapshot/${snapshot_repo}/${snapshot_name}/_restore?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"indices\": \"'${indices}'\"\n}\n'\n</code></pre> <p>Read the documentation to see the API, all parameters and their explanations.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#restoring-opensearch-dashboards-data","title":"Restoring OpenSearch Dashboards data","text":"<p>Data in OpenSearch Dashboards (saved searches, visualizations, dashboards, etc) is stored in the index <code>.opensearch_dashboards_x</code>. To restore that data you first need to delete the index and then do a restore.</p> <p>This will overwrite anything in the current <code>.opensearch_dashboards_x</code> index. If there is something new that should be saved, then export the saved objects and import them after the restore.</p> <p>There can be multiple <code>.opensearch_dashboards</code> indices in Opensearch, the current index should be the one you want to restore. To view your dashboard indices, follow these steps.</p> <pre><code>snapshot_name=&lt;Snapshot name from previous step&gt;\n\ncurl -kL -u \"${user}:${password}\" -X GET ${os_url}'/.opensearch_dashboard*?pretty' | jq 'keys'\n</code></pre> <p>If multiple <code>.opensearch_dashboards_x</code> indices show up, run this to see the index that the alias is currently looking at.</p> <pre><code>curl -kL -u \"${user}:${password}\" -X GET ${os_url}'/_alias/.opensearch_dashboard*?pretty' | jq 'keys'\n</code></pre> <p>Make sure that the index you want to restore also exists on the snapshot. (May be an issue if you are using an old snapshot)</p> <pre><code>curl -kL -u \"${user}:${password}\" -X GET \"${os_url}/_snapshot/${snapshot_repo}/${snapshot_name}?pretty\" | jq '.snapshots[].indices' | grep .opensearch_dashboards\n</code></pre> <p>Note</p> <p>If you visit the <code>\"&lt;os_url&gt;/app/dashboards\"</code> page in the Opensearch GUI after deleting the index and before restoring the index, another empty index <code>.opensearch_dashboards</code> will be created. You need to delete this manually, which can be done with <pre><code>`curl -kL -u \"${user}:${password}\" -X DELETE \"${os_url}/.opensearch_dashboards?pretty\"`\n</code></pre></p> <pre><code>index_to_restore=&lt;Index name from previous step&gt;\n\ncurl -kL -u \"${user}:${password}\" -X DELETE \"${os_url}/${index_to_restore}?pretty\"\n\ncurl -kL -u \"${user}:${password}\" -X POST \"${os_url}/_snapshot/${snapshot_repo}/${snapshot_name}/_restore?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"indices\": \"'${index_to_restore}'\"\n}\n'\n</code></pre> <p>Note</p> <p>For Open Distro for Elasticsearch and Kibana used in v0.18 and earlier, the same approach can be used with different naming, using <code>.kibana_1</code> for the index name instead of <code>.opensearch_dashboards_1</code>.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#start-new-cluster-from-snapshot","title":"Start new cluster from snapshot","text":"<p>This process is very similar to the one described above, but there are a few extra steps to carry out.</p> <p>Before you install OpenSearch you can preferably disable the initial index creation to make the restore process leaner by setting the following configuration option:</p> <pre><code>opensearch.createIndices: false\n</code></pre> <p>Install the OpenSearch suite:</p> <pre><code>./bin/ck8s ops helmfile sc -l group=opensearch apply\n</code></pre> <p>Wait for the the installation to complete.</p> <p>After the installation, go back up to the Restore section to proceed with the restore. If you want to restore all indices, use the following <code>indices</code> variable</p> <pre><code>indices=\"kubernetes-*,kubeaudit-*,other-*\"\n</code></pre> <p>Note</p> <p>This process assumes that you are using the same S3 bucket as your previous cluster. If you aren't:</p> <ul> <li>Register a new S3 snapshot repository to the old bucket as described here</li> <li>Use the newly registered snapshot repository in the restore process</li> </ul>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#harbor","title":"Harbor","text":"","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#backup_1","title":"Backup","text":"<p>Harbor is set up to store backups of the database in an S3 bucket (note that this does not include the actual images, since those are already stored in S3 by default). There is a CronJob called <code>harbor-backup-cronjob</code> in the cluster that is taking a database dump and uploading it to a S3 bucket.</p> <p>To take a backup on-demand, execute</p> <pre><code>./bin/ck8s ops kubectl sc -n harbor create job --from=cronjob/harbor-backup-cronjob &lt;name-of-job&gt;\n</code></pre>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#restore_1","title":"Restore","text":"<p>Restoring from off-site backup</p> <p>Since Harbor stores both database backups and images in the same bucket it is recommended to restore the off-site backup into the main S3 service first, reconfigure Harbor to use it, then restore the database from it.</p> <p>Instructions for how to restore Harbor can be found in <code>compliantkubernetes-apps</code>: https://github.com/elastisys/compliantkubernetes-apps/tree/main/scripts/restore#restore-harbor</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#velero","title":"Velero","text":"<p>These instructions make use of the Velero CLI, you can download it here: https://github.com/vmware-tanzu/velero/releases/tag/v1.7.1 (version 1.7.1). The CLI needs the env variable <code>KUBECONFIG</code> set to the path of a decrypted kubeconfig. Read more about Velero here: https://compliantkubernetes.io/user-guide/backup/</p> <p>Note</p> <p>This documentation uses the Velero CLI, as opposed to Velero CRDs, since that is what is encouraged by upstream documentation.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#backup_2","title":"Backup","text":"<p>Velero is set up to take daily backups and store them in an S3 bucket. The daily backup will not take backups of everything in a kubernetes cluster, it will instead look for certain labels and annotations. Read more about those labels and annotations here: https://compliantkubernetes.io/user-guide/backup/#backing-up</p> <p>It is also possible to take on-demand backups. Then you can freely chose what to backup and do not have to base it on the same labels. A basic example with the Velero CLI would be <code>velero backup create manual-backup</code>, which would take a backup of all kubernetes resources (though not the data in the volumes by default).</p> <p>If you want to create a latest backup from existing schedule , Velero CLI would be <code>velero backup create --from-schedule velero-daily-backup --wait</code>.</p> <p>Check which arguments you can use by running <code>velero backup create --help</code>.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#restore_2","title":"Restore","text":"<p>Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first.</p> <p>To restore the state from the latest daily backup, run:</p> <pre><code>velero restore create --from-schedule velero-daily-backup --wait\n</code></pre> <p>This command will wait until the restore has finished. You can also do partial restorations, e.g. just restoring one namespace, by using different arguments. You can also restore from manual backups by using the flag <code>--from-backup &lt;backup-name&gt;</code></p> <p>Persistent Volumes are only restored if a Pod with the backup annotation is restored. Multiple Pods can have an annotation for the same Persistent Volume. When restoring the Persistent Volume it will overwrite any existing files with the same names as the files to be restored. Any other files will be left as they were before the restoration started. So a restore will not wipe the volume clean and then restore. If a clean wipe is the desired behavior, then the volume must be wiped manually before restoring.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#restore-from-off-site-backup","title":"Restore from off-site backup","text":"<ul> <li> <p>Restoring from encrypted off-site backup:</p> <p>Recover the encrypted bucket into the main S3 service and reconfigure Velero to use this bucket, then follow the regular instructions.</p> <p>The references in Kubernetes might need to be deleted so Velero can resync from the bucket:</p> <pre><code># Note that this is only backup metadata\n./bin/ck8s ops kubectl sc -n velero delete backups.velero.io --all\n\n./bin/ck8s ops kubectl wc -n velero delete backups.velero.io --all\n</code></pre> </li> <li> <p>Restoring from unencrypted off-site backup:</p> <p>To recover directly from off-site backup the backup-location must be reconfigured:</p> <pre><code>export S3_BUCKET=\"&lt;off-site-s3-bucket&gt;\"\nexport S3_PREFIX=\"&lt;service-cluster|workload-cluster&gt;\"\nexport S3_ACCESS_KEY=$(sops -d --extract '[\"objectStorage\"][\"sync\"][\"s3\"][\"accessKey\"]' \"$CK8S_CONFIG_PATH/secrets.yaml\")\nexport S3_SECRET_KEY=$(sops -d --extract '[\"objectStorage\"][\"sync\"][\"s3\"][\"secretKey\"]' \"$CK8S_CONFIG_PATH/secrets.yaml\")\nexport S3_REGION=$(yq r \"$CK8S_CONFIG_PATH/sc-config.yaml\" \"objectStorage.sync.s3.region\")\nexport S3_ENDPOINT=$(yq r \"$CK8S_CONFIG_PATH/sc-config.yaml\" \"objectStorage.sync.s3.regionEndpoint\")\nexport S3_PATH_STYLE=$(yq r \"$CK8S_CONFIG_PATH/sc-config.yaml\" \"objectStorage.sync.s3.forcePathStyle\")\n\n# Delete default backup location\nvelero backup-location delete default\n\n# Delete backups from default backup location, note that this is only the backup metadata\n./bin/ck8s ops kubectl sc -n velero delete backups.velero.io --all\n\n./bin/ck8s ops kubectl wc -n velero delete backups.velero.io --all\n\n# Create off-site credentials\nkubectl -n velero create secret generic velero-backup \\\n--from-literal=cloud=\"$(echo -e \"[default]\\naws_access_key_id: ${S3_ACCESS_KEY}\\naws_secret_access_key: ${S3_SECRET_KEY}\\n\")\"\n\n# Create off-site backup location\nvelero backup-location create backup \\\n--access-mode ReadOnly \\\n--provider aws \\\n--bucket \"${S3_BUCKET}\" \\\n--prefix \"${S3_PREFIX}\" \\\n--config=\"region=${S3_REGION},s3Url=${S3_ENDPOINT},s3ForcePathStyle=${S3_PATH_STYLE}\" \\\n--credential=velero-backup=cloud\n</code></pre> <p>Check that the backup-location becomes available: <pre><code>$ velero backup-location get\nNAME     PROVIDER   BUCKET/PREFIX       PHASE       LAST VALIDATED   ACCESS MODE   DEFAULT\nbackup   aws        &lt;bucket&gt;/&lt;prefix&gt;   Available   &lt;timestamp&gt;      ReadOnly\n</code></pre></p> <p>Then check that the backups becomes available using <code>velero backup get</code>. When they are available restore one of them using <code>velero restore create &lt;name-of-restore&gt; --from-backup &lt;name-of-backup&gt;</code>.</p> <p>After the restore is complete Velero should be reconfigured to use the main S3 service again, with a new bucket if the previous one is unusable. Updating or syncing the Helm chart: <pre><code>./bin/ck8s ops helmfile sc -f helmfile -l app=velero -i apply\n\n./bin/ck8s ops helmfile wc -f helmfile -l app=velero -i apply\n</code></pre></p> <p>The secret and the backup metadata from the off-site backups can be deleted: <pre><code>./bin/ck8s ops kubectl sc -n velero delete secret velero-backup\n./bin/ck8s ops kubectl sc -n velero delete backups.velero.io --all\n./bin/ck8s ops kubectl sc -n velero delete backupstoragelocations.velero.io backup\n\n./bin/ck8s ops kubectl wc -n velero delete secret velero-backup\n./bin/ck8s ops kubectl wc -n velero delete backups.velero.io --all\n./bin/ck8s ops kubectl wc -n velero delete backupstoragelocations.velero.io backup\n</code></pre></p> </li> </ul>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#grafana","title":"Grafana","text":"<p>This refers to the user Grafana, not the ops Grafana.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#backup_3","title":"Backup","text":"<p>Grafana is set up to be included in the daily Velero backup. We then include the Grafana deployment, pod, and PVC (including the data). Manual backups can be taken using velero (include the same resources).</p>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/disaster-recovery/#restore_3","title":"Restore","text":"<p>To restore the Grafana backup you must:</p> <ul> <li>Have Grafana installed</li> <li> <p>Delete the grafana deployment, PVC and PV</p> <pre><code>./bin/ck8s ops kubectl sc delete deploy -n monitoring user-grafana\n./bin/ck8s ops kubectl sc delete pvc -n monitoring user-grafana\n</code></pre> </li> <li> <p>Restore the velero backup</p> <pre><code>velero restore create --from-schedule velero-daily-backup --wait\n</code></pre> </li> </ul>","tags":["ISO 27001 A.12.3.1 Information Backup","ISO 27001 A.17.1.1 Planning Information Security Continuity","HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering"]},{"location":"operator-manual/eksd/","title":"Eksd","text":"<p>This page is out of date</p> <p>We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date.</p> <p>Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly.</p>"},{"location":"operator-manual/eksd/#compliant-kubernetes-on-eks-d-based-clusters","title":"Compliant Kubernetes on EKS-D based clusters","text":"<p>This document contains instructions on how to install Compliant Kubernetes on AWS using EKS-D.</p> <p>Note</p> <p>This guide is written for compliantkubernetes-apps v0.13.0</p>"},{"location":"operator-manual/eksd/#requirements","title":"Requirements","text":"<ul> <li>An AWS account with billing enabled.</li> <li>A hosted zone in Route53.</li> <li><code>yq v3.4.1</code> installed on you machine.</li> <li><code>gpg2</code> installed on your machine with at least one key available.</li> <li><code>kubectl</code> installed on your machine.</li> </ul>"},{"location":"operator-manual/eksd/#infrastructure-and-kubernetes","title":"Infrastructure and Kubernetes","text":""},{"location":"operator-manual/eksd/#get-eks-d","title":"Get EKS-D","text":"<pre><code>git clone https://github.com/aws/eks-distro.git\ncd eks-distro/development/kops\ngit checkout v1-19-eks-1\n</code></pre>"},{"location":"operator-manual/eksd/#configure-your-aws-environment","title":"Configure your AWS environment","text":"<p>Follow the instructions in Getting Started with kOps on AWS up until you reach Creating your first cluster. Unless you have very specific requirements you shouldn't need to take any action when it comes to the DNS configuration.</p> <p>If you followed the instructions you should have:</p> <ul> <li>An IAM user for kOps with the correct permissions.</li> <li>Set AWS credentials and any other AWS environment variables you require in your shell.</li> <li>An S3 bucket for storing the kOps cluster state.</li> </ul>"},{"location":"operator-manual/eksd/#create-initial-kops-cluster-configurations","title":"Create initial kOps cluster configurations","text":"<pre><code>export AWS_REGION=&lt;region where you want the infrastructure to be created&gt;\nexport KOPS_STATE_STORE=s3://&lt;name of the bucket you created in previous step&gt;\n\nSERVICE_CLUSTER=\"&lt;xyz, e.g. test-sc&gt;.&lt;your hosted zone in Route53, e.g. example.com&gt;\"\nWORKLOAD_CLUSTER=\"&lt;xyz, e.g. test-wc&gt;.&lt;your hosted zone in Route53, e.g. example.com&gt;\"\n\nfor CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do\n    export KOPS_CLUSTER_NAME=${CLUSTER}\n    ./create_values_yaml.sh\n    ./create_configuration.sh\ndone\n</code></pre>"},{"location":"operator-manual/eksd/#modify-kops-cluster-configurations","title":"Modify kOps cluster configurations","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do\necho '\n---\n- command: update\n  path: spec.etcdClusters[0].manager\n  value:\n    env:\n    - name: ETCD_LISTEN_METRICS_URLS\n      value: http://0.0.0.0:8081\n    - name: ETCD_METRICS\n      value: basic\n\n- command: update\n  path: spec.networking\n  value:\n    calico:\n      encapsulationMode: ipip\n\n- command: update\n  path: spec.metricsServer.enabled\n  value:\n    false\n\n- command: update\n  path: spec.kubeAPIServer\n  value:\n    image: public.ecr.aws/eks-distro/kubernetes/kube-apiserver:v1.19.6-eks-1-19-1\n    auditLogMaxAge: 7\n    auditLogMaxBackups: 1\n    auditLogMaxSize: 100\n    auditLogPath: /var/log/kubernetes/audit/kube-apiserver-audit.log\n    auditPolicyFile: /srv/kubernetes/audit/policy-config.yaml\n    enableAdmissionPlugins:\n    - \"PodSecurityPolicy\"\n    - \"NamespaceLifecycle\"\n    - \"LimitRanger\"\n    - \"ServiceAccount\"\n    - \"DefaultStorageClass\"\n    - \"DefaultTolerationSeconds\"\n    - \"MutatingAdmissionWebhook\"\n    - \"ValidatingAdmissionWebhook\"\n    - \"ResourceQuota\"\n    - \"NodeRestriction\"\n\n- command: update\n  path: spec.fileAssets\n  value:\n  - name: audit-policy-config\n    path: /srv/kubernetes/audit/policy-config.yaml\n    roles:\n    - Master\n    content: |\n      apiVersion: audit.k8s.io/v1\n      kind: Policy\n      rules:\n      - level: RequestResponse\n        resources:\n        - group: \"\"\n          resources: [\"pods\"]\n      - level: Metadata\n        resources:\n        - group: \"\"\n          resources: [\"pods/log\", \"pods/status\"]\n      - level: None\n        resources:\n        - group: \"\"\n          resources: [\"configmaps\"]\n          resourceNames: [\"controller-leader\"]\n      - level: None\n        users: [\"system:kube-proxy\"]\n        verbs: [\"watch\"]\n        resources:\n        - group: \"\" # core API group\n          resources: [\"endpoints\", \"services\"]\n      - level: None\n        userGroups: [\"system:authenticated\"]\n        nonResourceURLs:\n        - \"/api*\" # Wildcard matching.\n        - \"/version\"\n      - level: Request\n        resources:\n        - group: \"\" # core API group\n          resources: [\"configmaps\"]\n        namespaces: [\"kube-system\"]\n      - level: Metadata\n        resources:\n        - group: \"\" # core API group\n          resources: [\"secrets\", \"configmaps\"]\n      - level: Request\n        resources:\n        - group: \"\" # core API group\n        - group: \"extensions\" # Version of group should NOT be included.\n      - level: Metadata\n        omitStages:\n          - \"RequestReceived\"\n' | yq w -i -s - ${CLUSTER}/${CLUSTER}.yaml\ndone\n\n# Configure OIDC flags for kube-apiserver.\nfor CLUSTER in ${WORKLOAD_CLUSTERS}; do\n    yq w -i ${CLUSTER}/${CLUSTER}.yaml 'spec.kubeAPIServer.oidcIssuerURL' https://dex.${SERVICE_CLUSTER}\n    yq w -i ${CLUSTER}/${CLUSTER}.yaml 'spec.kubeAPIServer.oidcUsernameClaim' email\n    yq w -i ${CLUSTER}/${CLUSTER}.yaml 'spec.kubeAPIServer.oidcClientID' kubelogin\ndone\n\n# Use bigger machines for service cluster worker nodes.\nyq w -i -d2 ${SERVICE_CLUSTER}/${SERVICE_CLUSTER}.yaml 'spec.machineType' t3.large\n\n# Update kOps cluster configurations in state bucket.\nfor CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do\n    ./bin/kops-1-19 replace -f \"./${CLUSTER}/${CLUSTER}.yaml\"\ndone\n</code></pre>"},{"location":"operator-manual/eksd/#create-clusters","title":"Create clusters","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do\n    export KOPS_CLUSTER_NAME=${CLUSTER}\n    ./create_cluster.sh\ndone\n</code></pre> <p>The creation of the clusters might take anywhere from 5 minutes to 20 minutes. You should run the <code>./cluster_wait.sh</code> script against all of your clusters as it creates a configmap needed by the <code>aws-iam-authenticator</code> pod, e.g.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do\n    export KOPS_CLUSTER_NAME=${CLUSTER}\n    kubectl config use-context ${CLUSTER}\n    timeout 600 ./cluster_wait.sh\ndone\n</code></pre>"},{"location":"operator-manual/eksd/#compliant-kubernetes-apps","title":"Compliant Kubernetes Apps","text":""},{"location":"operator-manual/eksd/#get-compliant-kubernetes-apps","title":"Get Compliant Kubernetes Apps","text":"<pre><code>git clone git@github.com:elastisys/compliantkubernetes-apps\ncd compliantkubernetes-apps\ngit checkout v0.13.0\n</code></pre>"},{"location":"operator-manual/eksd/#install-requirements","title":"Install requirements","text":"<pre><code>ansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127.0.0.1, get-requirements.yaml\n</code></pre>"},{"location":"operator-manual/eksd/#initialize-configuration","title":"Initialize configuration","text":"<pre><code>export CK8S_ENVIRONMENT_NAME=aws-eks-d\n#export CK8S_FLAVOR=[dev|prod] # defaults to dev\nexport CK8S_CONFIG_PATH=~/.ck8s/aws-eks-d\nexport CK8S_CLOUD_PROVIDER=aws\nexport CK8S_PGP_FP=&lt;your GPG key ID&gt;  # retrieve with gpg --list-secret-keys\n./bin/ck8s init\n</code></pre> <p>Three files, <code>sc-config.yaml</code> and <code>wc-config.yaml</code>, and <code>secrets.yaml</code>, were generated in the <code>${CK8S_CONFIG_PATH}</code> directory.</p> <pre><code>ls -l ${CK8S_CONFIG_PATH}\n</code></pre>"},{"location":"operator-manual/eksd/#edit-configuration-files","title":"Edit configuration files","text":"<p>Edit the configuration files <code>sc-config.yaml</code>, <code>wc-config.yaml</code> and <code>secrets.yaml</code> and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted.</p> <pre><code>vim ${CK8S_CONFIG_PATH}/sc-config.yaml\nvim ${CK8S_CONFIG_PATH}/wc-config.yaml\nsops ${CK8S_CONFIG_PATH}/secrets.yaml\n</code></pre> <p>You should perform the following changes:</p> <pre><code># sc-config.yaml\nglobal:\n  baseDomain: \"set-me\"     # Set to ${SERVICE_CLUSTER}\n  opsDomain: \"set-me\"      # Set to ops.${SERVICE_CLUSTER}\n  issuer: letsencrypt-prod\n  verifyTls: true\n  clusterDNS: 100.64.0.10\n\nstorageClasses:\n  default: kops-ssd-1-17\n  nfs:\n    enabled: false\n  cinder:\n    enabled: false\n  local:\n    enabled: false\n  ebs:\n    enabled: false\n\nobjectStorage:\n  type: \"s3\"\n  s3:\n    region: \"set-me\"          # e.g. eu-north-1\n    regionEndpoint: \"set-me\"  # e.g. https://s3.eu-north-1.amazonaws.com\n\nissuers:\n  letsencrypt:\n    prod:\n      email: \"set-me\"  # Set to a valid email address\n    staging:\n      email: \"set-me\"  # Set to a valid email address\n</code></pre> <pre><code># wc-config.yaml\nglobal:\n  baseDomain: \"set-me\"     # Set to ${WORKLOAD_CLUSTER}\n  opsDomain: \"set-me\"      # Set to ops.${SERVICE_CLUSTER}\n  issuer: letsencrypt-prod\n  verifyTls: true\n  clusterDNS: 100.64.0.10\n\nstorageClasses:\n  default: kops-ssd-1-17\n  nfs:\n    enabled: false\n  cinder:\n    enabled: false\n  local:\n    enabled: false\n  ebs:\n    enabled: false\n\nobjectStorage:\n  type: \"s3\"\n  s3:\n    region: \"set-me\"          # e.g. eu-north-1\n    regionEndpoint: \"set-me\"  # e.g. https://s3.eu-north-1.amazonaws.com\n\n\nopa:\n  enabled: false # Does not work with k8s 1.19+\n</code></pre> <pre><code># secrets.yaml\nobjectStorage:\n  s3:\n    accessKey: \"set-me\" # Set to your AWS S3 accesskey\n    secretKey: \"set-me\" # Set to your AWS S3 secretKey\n</code></pre>"},{"location":"operator-manual/eksd/#psp-and-rbac","title":"PSP and RBAC","text":"<p>Since we've enabled the PodSecurityPolicy admission plugin in the kube-apiserver we'll need to create some basic PSPs and RBAC rules that both you and Compliant Kubernetes Apps will need to run workloads.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do\nkubectl config use-context ${CLUSTER}\n\n# Install 'restricted' and 'privileged' podSecurityPolicies.\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/privileged-psp.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/restricted-psp.yaml\n\n# Install RBAC so authenticated users are be able to use the 'restricted' psp.\necho '\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n  name: psp:restricted\nrules:\n- apiGroups:\n  - policy\n  resourceNames:\n  - restricted\n  resources:\n  - podsecuritypolicies\n  verbs:\n  - use\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: psp:any:restricted\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: psp:restricted\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:authenticated\n' | kubectl apply -f -\ndone\n</code></pre>"},{"location":"operator-manual/eksd/#create-placeholder-dns-records","title":"Create placeholder DNS records","text":"<p>To avoid negative caching and other surprises. Create the following records using your favorite tool or you can use the Import zone file feature in Route53:</p> <pre><code>echo \"\n*.${SERVICE_CLUSTER}     60s A 203.0.113.123\n*.${WORKLOAD_CLUSTER}    60s A 203.0.113.123\n*.ops.${SERVICE_CLUSTER} 60s A 203.0.113.123\n\"\n</code></pre>"},{"location":"operator-manual/eksd/#create-s3-buckets","title":"Create S3 buckets","text":"<p>Depending on you configuration you may want to create S3 buckets. Create the following buckets using your favorite tool or via the AWS console:</p> <pre><code># List bucket names.\n{ yq r ${CK8S_CONFIG_PATH}/wc-config.yaml 'objectStorage.buckets.*';\\\n    yq r ${CK8S_CONFIG_PATH}/sc-config.yaml 'objectStorage.buckets.*'; } | sort | uniq\n\n# Create buckets using the AWS CLI.\n# Assumes that the same bucket is used for velero in both service and workload cluster.\nfor BUCKET in $(yq r ${CK8S_CONFIG_PATH}/sc-config.yaml 'objectStorage.buckets.*'); do\n    aws s3api create-bucket\\\n        --bucket ${BUCKET} \\\n        --create-bucket-configuration LocationConstraint=${AWS_REGION}\ndone\n</code></pre>"},{"location":"operator-manual/eksd/#prepare-kubeconfigs","title":"Prepare kubeconfigs","text":"<p>Compliant Kubernetes Apps demands that the kube contexts for the workload and service cluster are found in separate files encrypted with sops.</p> <pre><code>kubectl config view --minify --flatten --context=${SERVICE_CLUSTER} &gt; ${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml\nsops -e -i --config ${CK8S_CONFIG_PATH}/.sops.yaml ${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml\n\nkubectl config view --minify --flatten --context=${WORKLOAD_CLUSTER} &gt; ${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml\nsops -e -i --config ${CK8S_CONFIG_PATH}/.sops.yaml ${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml\n</code></pre>"},{"location":"operator-manual/eksd/#install-apps","title":"Install apps","text":"<p>You can install apps in parallel, although it is recommended to install the service cluster before the workload cluster.</p> <pre><code># Service cluster\n./bin/ck8s apply sc # Respond \"n\" if you get a WARN\n\n# Workload cluster\n./bin/ck8s apply wc # Respond \"n\" if you get a WARN\n</code></pre> <p>Run the following to get metrics from etcd-manager <pre><code># Service cluster\n./bin/ck8s ops helmfile sc -l app=kube-prometheus-stack apply --skip-deps --set kubeEtcd.service.selector.k8s-app=etcd-manager-main --set kubeEtcd.service.targetPort=8081\n\n# Workload cluster\n./bin/ck8s ops helmfile wc -l app=kube-prometheus-stack apply --skip-deps --set kubeEtcd.service.selector.k8s-app=etcd-manager-main --set kubeEtcd.service.targetPort=8081\n</code></pre></p>"},{"location":"operator-manual/eksd/#update-dns-records","title":"Update DNS records","text":"<p>Now that we've installed all applications, the loadbalancer fronting the ingress controller should be ready. Run the following commands and update the A records in Route53.</p> <pre><code>sc_lb=$(./bin/ck8s ops kubectl sc -n ingress-nginx get svc ingress-nginx-controller -ojsonpath={.status.loadBalancer.ingress[0].hostname})\nwc_lb=$(./bin/ck8s ops kubectl wc -n ingress-nginx get svc ingress-nginx-controller -ojsonpath={.status.loadBalancer.ingress[0].hostname})\nsc_lb_ip=$(dig +short ${sc_lb} | head -1)\nwc_lb_ip=$(dig +short ${wc_lb} | head -1)\n\necho \"\n*.${SERVICE_CLUSTER}     60s A ${sc_lb_ip}\n*.${WORKLOAD_CLUSTER}    60s A ${wc_lb_ip}\n*.ops.${SERVICE_CLUSTER} 60s A ${sc_lb_ip}\n\"\n</code></pre>"},{"location":"operator-manual/eksd/#teardown","title":"Teardown","text":""},{"location":"operator-manual/eksd/#compliant-kubernetes-apps_1","title":"Compliant Kubernetes Apps","text":"<p>This step is optional. If this is not run you'll have to check and manually remove any leftover cloud resources like S3 buckets, ELBs, and EBS volumes.</p> <pre><code>git checkout 6f2e386\ntimeout 180 ./scripts/clean-wc.sh\ntimeout 180 ./scripts/clean-sc.sh\n\n# Delete buckets\nfor BUCKET in $(yq r ${CK8S_CONFIG_PATH}/sc-config.yaml 'objectStorage.buckets.*'); do\n    aws s3 rb --force s3://${BUCKET}\ndone\n\n# Delete config repo\nrm -rf ${CK8S_CONFIG_PATH}\n</code></pre> <p>Remember to also remove the A records from Route53.</p>"},{"location":"operator-manual/eksd/#infrastructure-and-kubernetes_1","title":"Infrastructure and Kubernetes","text":"<p>Enter <code>eks-distro/development/kops</code> and run:</p> <pre><code># Destroy clusters and local cluster configurations.\nfor CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do\n    export KOPS_CLUSTER_NAME=${CLUSTER}\n    ./delete_cluster.sh\n    rm -rf ${CLUSTER}\ndone\n</code></pre> <p>You'll have to manually remove the leftover kOps A records from Route53.</p> <pre><code># Get names of the A records to be removed.\nfor CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTER}; do\n    echo kops-controller.internal.${CLUSTER}\ndone\n</code></pre> <p>Finally, you'll also need to remove the <code>${KOPS_STATE_STORE}</code> from S3 and the IAM user that you used for this guide.</p>"},{"location":"operator-manual/exoscale/","title":"Exoscale","text":"<p>This page is out of date</p> <p>We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date.</p> <p>Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly.</p>"},{"location":"operator-manual/exoscale/#compliant-kubernetes-deployment-on-exoscale","title":"Compliant Kubernetes Deployment on Exoscale","text":"<p>This document contains instructions on how to setup a service cluster and a workload cluster in Exoscale. The following are the main tasks addressed in this document:</p> <ol> <li>Infrastructure setup for two clusters: one service and one workload cluster</li> <li>Deploying Compliant Kubernetes on top of the two clusters.</li> <li>Creating DNS Records</li> <li>Deploying Rook Storage Orchestration Service</li> <li>Deploying Compliant Kubernetes apps</li> </ol> <p>The instructions below are just samples, you need to update them according to your requirements. Besides, the exoscale cli is used to manage DNS. If you are using any other DNS service provider for managing your DNS you can skip it.</p> <p>Before starting, make sure you have all necessary tools.</p> <p>Note</p> <p>This guide is written for compliantkubernetes-apps v0.17.0</p>"},{"location":"operator-manual/exoscale/#setup","title":"Setup","text":"<p>Choose names for your service cluster and workload cluster, as well as a name for your environment:</p> <pre><code>SERVICE_CLUSTER=\"testsc\"\nWORKLOAD_CLUSTERS=( \"testwc0\" )\nCK8S_ENVIRONMENT_NAME=my-environment-name\n</code></pre>"},{"location":"operator-manual/exoscale/#infrastructure-setup-using-terraform","title":"Infrastructure Setup using Terraform","text":"<p>Before trying any of the steps, clone the Elastisys Compliant Kubernetes Kubespray repo as follows:</p> <pre><code>git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray\ncd compliantkubernetes-kubespray/kubespray\n</code></pre>"},{"location":"operator-manual/exoscale/#expose-exoscale-credentials-to-terraform","title":"Expose Exoscale credentials to Terraform","text":"<p>For authentication create the file  <code>~/.cloudstack.ini</code> and put your Exoscale credentials in it. The file should look like something like this:</p> <pre><code>[cloudstack]\nkey = &lt;API key&gt;\nsecret = &lt;API secret&gt;\n</code></pre>"},{"location":"operator-manual/exoscale/#customize-your-infrastructure","title":"Customize your infrastructure","text":"<p>Create a configuration for the service and the workload clusters:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\ncp -r inventory/sample inventory/$CLUSTER\ncp contrib/terraform/exoscale/default.tfvars inventory/$CLUSTER/\ndone\n</code></pre> <p>Review and, if needed, adjust the files in <code>inventory/$CLUSTER/default.tfvars</code>, where <code>$CLUSTER</code> is the cluster name:</p> <ul> <li>Use different value for the <code>prefix</code> field in <code>/default.tfvars</code> for the two clusters.     Failing to do so will result in a name conflict.</li> <li>Set a non-zero value for <code>ceph_partition_size</code> field, e.g., <code>\"ceph_partition_size\": 50</code>, as it will be used by Rook storage service to provide local disk storage.</li> <li>To security harden your cluster, set <code>ssh_whitelist</code> and <code>api_server_whitelist</code> to the IP addresses from which you expect to operate the cluster.</li> <li>Make sure you configure your SSH keys in <code>ssh_public_keys</code>.</li> </ul> <p>Important</p> <p>The <code>Linux Ubuntu 20.04 LTS 64-bit</code> image on Exoscale is regularly upgraded, which might cause unexpected changes during <code>terraform apply</code>. Consider uploading your own dated Ubuntu image to reduce the risk of downtime.</p>"},{"location":"operator-manual/exoscale/#initialize-and-apply-terraform","title":"Initialize and Apply Terraform","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\npushd contrib/terraform/exoscale\n    terraform init\n    terraform apply \\\n-var-file=../../../inventory/$CLUSTER/default.tfvars \\\n-state=../../../inventory/$CLUSTER/tfstate-$CLUSTER.tfstate\n    cp inventory.ini ../../../inventory/$CLUSTER/\n    popd\ndone\n</code></pre> <p>Important</p> <p>The Terraform state is stored in <code>inventory/$CLUSTER/tfstate-$CLUSTER.tfstate</code>, where <code>$CLUSTER</code> is the cluster name. It is precious. Consider backing it up or using Terraform Cloud.</p> <p>You should now have inventory file named <code>inventory/$CLUSTER/inventory.ini</code> for each cluster that you can use with kubespray.</p>"},{"location":"operator-manual/exoscale/#test-access-to-all-nodes","title":"Test access to all nodes","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\npushd inventory/$CLUSTER\nANSIBLE_HOST_KEY_CHECKING=False ansible all -i inventory.ini -m ping\n    popd\ndone\n</code></pre>"},{"location":"operator-manual/exoscale/#deploying-vanilla-kubernetes-clusters-using-kubespray","title":"Deploying vanilla Kubernetes clusters using Kubespray","text":"<p>With the infrastructure provisioned, we can now deploy Kubernetes using kubespray. First, if you haven't done so already, install the pre-requisites and change to the <code>compliantkubernetes-kubespray</code> root directory. <pre><code>pip3 install -r requirements.txt\n\ncd ..\n</code></pre></p>"},{"location":"operator-manual/exoscale/#init-the-kubespray-config-in-your-config-path","title":"Init the Kubespray config in your config path","text":"<pre><code>export DOMAIN=&lt;your_domain&gt; # DNS domain to expose the services inside the service cluster i.e. \"example.com\"\nexport CK8S_CONFIG_PATH=~/.ck8s/exoscale\nexport CK8S_PGP_FP=&lt;your GPG key fingerprint&gt;  # retrieve with gpg --list-secret-keys\n\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\n./bin/ck8s-kubespray init $CLUSTER default $CK8S_PGP_FP\ndone\n</code></pre>"},{"location":"operator-manual/exoscale/#copy-the-generated-inventory-files-in-the-right-location","title":"Copy the generated inventory files in the right location","text":"<p>Please copy the two inventory files, <code>kubespray/inventory/$CLUSTER/inventory.ini</code>, generated by Terraform to <code>${CK8S_CONFIG_PATH}/$CLUSTER-config/</code>, where $CLUSTER the name of each cluster (i.e., <code>testsc</code>, <code>testwc0</code>).</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\n    cp kubespray/inventory/$CLUSTER/inventory.ini ${CK8S_CONFIG_PATH}/$CLUSTER-config/\ndone\n</code></pre>"},{"location":"operator-manual/exoscale/#run-kubespray-to-deploy-the-kubernetes-clusters","title":"Run kubespray to deploy the Kubernetes clusters","text":"<p><pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\n./bin/ck8s-kubespray apply $CLUSTER --flush-cache\ndone\n</code></pre> This may take up to 10 minutes for each cluster, 20 minutes in total.</p>"},{"location":"operator-manual/exoscale/#correct-the-kubernetes-api-ip-addresses","title":"Correct the Kubernetes API IP addresses","text":"<p>Locate the encrypted kubeconfigs in <code>${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml</code> and edit them using sops. Copy the public IP address of the load balancer from inventory files <code>${CK8S_CONFIG_PATH}/*-config/inventory.ini</code> and replace the private IP address for the <code>server</code> field in <code>${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml</code>.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml\ndone\n</code></pre>"},{"location":"operator-manual/exoscale/#test-access-to-the-clusters-as-follows","title":"Test access to the clusters as follows","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get nodes'\ndone\n</code></pre>"},{"location":"operator-manual/exoscale/#create-the-dns-records","title":"Create the DNS Records","text":"<p>You will need to setup a number of DNS entries for traffic to be routed correctly. Determine the public IP of the load-balancer fronting the Ingress controller of the clusters from the Terraform state file generated during infrastructure setup.</p> <p>To get the load-balancer IP, run the following command:</p> <pre><code>SC_INGRESS_LB_IP_ADDRESS=$(terraform output -state kubespray/inventory/$SERVICE_CLUSTER/tfstate-$SERVICE_CLUSTER.tfstate -raw ingress_controller_lb_ip_address)\necho $SC_INGRESS_LB_IP_ADDRESS\n</code></pre> <p>Configure the exoscale CLI:</p> <pre><code>exo config\n</code></pre> <p>Then point these domains to the service cluster using 'exoscale cli' as follows:</p> <pre><code>exo dns add A $DOMAIN -a $SC_INGRESS_LB_IP_ADDRESS -n *.ops.$CK8S_ENVIRONMENT_NAME\nexo dns add A $DOMAIN -a $SC_INGRESS_LB_IP_ADDRESS -n *.$CK8S_ENVIRONMENT_NAME\n</code></pre>"},{"location":"operator-manual/exoscale/#deploy-rook","title":"Deploy Rook","text":"<p>To deploy Rook, please go to the <code>compliantkubernetes-kubespray</code> repo root directory and run the following.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops --decrypt ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml &gt; $CLUSTER.yaml\n    export KUBECONFIG=$CLUSTER.yaml\n    ./rook/deploy-rook.sh\n    shred -zu $CLUSTER.yaml\ndone\n</code></pre> <p>Please restart the operator pod, <code>rook-ceph-operator*</code>, if some pods stalls in initialization state as shown below:</p> <pre><code>rook-ceph     rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg    0/1     Init:0/2   0          24m\nrook-ceph     rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh   0/1     Init:0/2   0          36m\nrook-ceph     rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55   0/1     Init:0/2   0          14m\n</code></pre> <p>Important</p> <p>Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances.</p>"},{"location":"operator-manual/exoscale/#test-rook","title":"Test Rook","text":"<p>To test Rook, proceed as follows:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml 'kubectl --kubeconfig {} apply -f https://raw.githubusercontent.com/rook/rook/release-1.5/cluster/examples/kubernetes/ceph/csi/rbd/pvc.yaml';\ndone\n\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml 'kubectl --kubeconfig {} get pvc';\ndone\n</code></pre> <p>You should see PVCs in Bound state. If you want to clean the previously created PVCs:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml 'kubectl --kubeconfig {} delete pvc rbd-pvc';\ndone\n</code></pre>"},{"location":"operator-manual/exoscale/#deploying-compliant-kubernetes-apps","title":"Deploying Compliant Kubernetes Apps","text":"<p>Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps.</p>"},{"location":"operator-manual/exoscale/#clone-compliantkubernetes-apps-and-install-pre-requisites","title":"Clone <code>compliantkubernetes-apps</code> and Install Pre-requisites","text":"<p>If you haven't done so already, clone the <code>compliantkubernetes-apps</code> repo and install pre-requisites.</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes-apps.git\ncd compliantkubernetes-apps\nansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127.0.0.1, get-requirements.yaml\n</code></pre>"},{"location":"operator-manual/exoscale/#initialize-the-apps-configuration","title":"Initialize the apps configuration","text":"<pre><code>export CK8S_ENVIRONMENT_NAME=my-environment-name\n#export CK8S_FLAVOR=[dev|prod] # defaults to dev\nexport CK8S_CONFIG_PATH=~/.ck8s/my-cluster-path\nexport CK8S_CLOUD_PROVIDER=# [exoscale|safespring|citycloud|aws|baremetal]\nexport CK8S_PGP_FP=&lt;your GPG key fingerprint&gt;  # retrieve with gpg --list-secret-keys\n\n./bin/ck8s init\n</code></pre> <p>This will initialise the configuration in the <code>${CK8S_CONFIG_PATH}</code> directory. Generating configuration files <code>sc-config.yaml</code> and <code>wc-config.yaml</code>, as well as secrets with randomly generated passwords in <code>secrets.yaml</code>. This will also generate read-only default configuration under the directory <code>defaults/</code> which can be used as a guide for available and suggested options.</p> <pre><code>ls -l $CK8S_CONFIG_PATH\n</code></pre>"},{"location":"operator-manual/exoscale/#configure-the-apps","title":"Configure the apps","text":"<p>Edit the configuration files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code>, <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/secrets.yaml</code> and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted.</p> <pre><code>vim ${CK8S_CONFIG_PATH}/sc-config.yaml\nvim ${CK8S_CONFIG_PATH}/wc-config.yaml\nsops ${CK8S_CONFIG_PATH}/secrets.yaml\n</code></pre> <p>Tip</p> <p>The default configuration for the service cluster and workload cluster are available in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code> and can be used as a reference for available options.</p> <p>Warning</p> <p>Do not modify the read-only default configurations files found in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code>. Instead configure the cluster by modifying the regular files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> as they will override the default options.</p> <p>The following are the minimum change you should perform:</p> <pre><code># ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml\nglobal:\nbaseDomain: set-me # set to $CK8S_ENVIRONMENT_NAME.$DOMAIN\nopsDomain: set-me  # set to ops.$CK8S_ENVIRONMENT_NAME.$DOMAIN\nissuer: letsencrypt-prod\n\nobjectStorage:\n# type: s3 # set as default for prod flavor, defaults to \"none\" for dev\ns3:\nregion: set-me          # Region for S3 buckets, e.g. ch-gva-2\nregionEndpoint: set-me  # Region endpoint for S3 buckets, e.g. https://sos-ch-gva-2.exo.io\n# forcePathStyle: false # set as default\n\nclusterAdmin:\nusers: # set to the cluster admin users\n- set-me\n- admin@example.com\n</code></pre> <pre><code># ${CK8S_CONFIG_PATH}/sc-config.yaml (in addition to the changes above)\nuser:\ngrafana:\noidc:\nallowedDomains: # set to your domain(s), or unset using [] to deny all\n- set-me\n- example.com\nharbor:\n# persistence:\n#  type: objectStorage    # set as default for prod flavor, defaults to \"filesystem\" for dev\n#  disableRedirect: false # set as default\noidc:\ngroupClaimName: set-me # set to group claim name used by OIDC provider\nadminGroupName: set-me # name of the group that automatically will get admin\n\nelasticsearch:\nextraRoleMappings: # set to configure elasticsearch access, or unset using []\n- mapping_name: kibana_user\ndefinition:\nusers:\n- set-me\n- mapping_name: kubernetes_log_reader\ndefinition:\nusers:\n- set-me\n- mapping_name: all_access\ndefinition:\nusers:\n- set-me\n\nalerts:\nopsGenieHeartbeat:\n# enabled: true # set as default for prod flavour, defaults to \"false\" for dev\nname: set-me    # set to name the heartbeat if enabled\n\nissuers:\nletsencrypt:\nprod:\nemail: set-me # set this to an email to receive LetsEncrypt notifications\nstaging:\nemail: set-me # set this to an email to receive LetsEncrypt notifications\n</code></pre> <pre><code># ${CK8S_CONFIG_PATH}/wc-config.yaml (in addition to the changes above)\nuser:\nnamespaces: # set this to create user namespaces, or unset using []\n- set-me\n- production\n- staging\nadminUsers: # set this to create admins in the user namespaces, or unset using []\n- set-me\n- admin@example.com\nadminGroups: # set this to create admin groups in the user namespaces, or unset using []\n- set-me\n# alertmanager: # add this block to enable user accessible alertmanager\n#   enabled: true\n#   namespace: alertmanager # note that this namespace must be listed above under \"user.namespaces\"\n\nopa:\nimageRegistry:\nURL: # set this to the allowed image registry, or unset using [] to deny all\n- set-me\n- harbor.example.com\n</code></pre> <pre><code># ${CK8S_CONFIG_PATH}/secrets.yaml\nobjectStorage:\ns3:\naccessKey: set-me # set to your s3 accesskey\nsecretKey: set-me # set to your s3 secretKey\n</code></pre>"},{"location":"operator-manual/exoscale/#create-s3-buckets","title":"Create S3 buckets","text":"<p>You can use the following script to create required S3 buckets. The script uses <code>s3cmd</code> in the background and gets configuration and credentials for your S3 provider from <code>${HOME}/.s3cfg</code> file.</p> <pre><code># Use your default s3cmd config file: ${HOME}/.s3cfg\nscripts/S3/entry.sh create\n</code></pre> <p>Important</p> <p>You should not use your own credentials for S3. Rather create a new set of credentials with write-only access, when supported by the object storage provider (check a feature matrix).</p>"},{"location":"operator-manual/exoscale/#test-s3","title":"Test S3","text":"<p>To ensure that you have configured S3 correctly, run the following snippet:</p> <pre><code>(\naccess_key=$(sops exec-file ${CK8S_CONFIG_PATH}/secrets.yaml 'yq r {} \"objectStorage.s3.accessKey\"')\nsecret_key=$(sops exec-file ${CK8S_CONFIG_PATH}/secrets.yaml 'yq r {} \"objectStorage.s3.secretKey\"')\nsc_config=$(yq m ${CK8S_CONFIG_PATH}/defaults/sc-config.yaml ${CK8S_CONFIG_PATH}/sc-config.yaml -a overwrite -x)\nregion=$(echo ${sc_config} | yq r - 'objectStorage.s3.region')\nhost=$(echo ${sc_config} | yq r -  'objectStorage.s3.regionEndpoint')\n\nfor bucket in $(echo ${sc_config} | yq r -  'objectStorage.buckets.*'); do\ns3cmd --access_key=${access_key} --secret_key=${secret_key} \\\n--region=${region} --host=${host} \\\nls s3://${bucket} &gt; /dev/null\n        [ ${?} = 0 ] &amp;&amp; echo \"Bucket ${bucket} exists!\"\ndone\n)\n</code></pre>"},{"location":"operator-manual/exoscale/#install-compliant-kubernetes-apps","title":"Install Compliant Kubernetes apps","text":"<p>Start with the service cluster:</p> <pre><code>ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml\n./bin/ck8s apply sc  # Respond \"n\" if you get a WARN\n</code></pre> <p>Then the workload clusters:</p> <pre><code>for CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n    ./bin/ck8s apply wc  # Respond \"n\" if you get a WARN\ndone\n</code></pre>"},{"location":"operator-manual/exoscale/#settling","title":"Settling","text":"<p>Important</p> <p>Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes.</p> <p>You can check if the system settled as follows:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces pods'\ndone\n</code></pre> <p>Check the output of the command above. All Pods needs to be Running or Completed.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates'\ndone\n</code></pre> <p>Check the output of the command above. All resources need to have the Ready column True.</p>"},{"location":"operator-manual/exoscale/#testing","title":"Testing","text":"<p>After completing the installation step you can test if the apps are properly installed and ready using the commands below.</p> <p>Start with the service cluster:</p> <pre><code>ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml\n./bin/ck8s test sc  # Respond \"n\" if you get a WARN\n</code></pre> <p>Then the workload clusters:</p> <pre><code>for CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n    ./bin/ck8s test wc  # Respond \"n\" if you get a WARN\ndone\n</code></pre> <p>Done. Navigate to the endpoints, for example <code>grafana.$BASE_DOMAIN</code>, <code>kibana.$BASE_DOMAIN</code>, <code>harbor.$BASE_DOMAIN</code>, etc. to discover Compliant Kubernetes's features.</p>"},{"location":"operator-manual/exoscale/#teardown","title":"Teardown","text":""},{"location":"operator-manual/exoscale/#removing-compliant-kubernetes-apps-from-your-cluster","title":"Removing Compliant Kubernetes Apps from your cluster","text":"<p>To remove the applications added by compliant kubernetes you can use the two scripts <code>clean-sc.sh</code> and <code>clean-wc.sh</code>, they are located here in the scripts folder.</p> <p>They perform the following actions:</p> <ol> <li>Delete the added helm charts</li> <li>Delete the added namespaces</li> <li>Delete any remaining PersistentVolumes</li> <li>Delete the added CustomResourceDefinitions</li> </ol> <p>Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.</p>"},{"location":"operator-manual/exoscale/#remove-infrastructure","title":"Remove infrastructure","text":"<p>To teardown the infrastructure, please switch to the root directory of the exoscale branch of the Kubespray repo (see the Terraform section).</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\npushd contrib/terraform/exoscale\n    terraform init\n    terraform destroy \\\n-var-file=../../../inventory/$CLUSTER/default.tfvars \\\n-state=../../../inventory/$CLUSTER/tfstate-$CLUSTER.tfstate\n    popd\ndone\n\n# Remove DNS records\nexo dns remove $DOMAIN *.ops.$CK8S_ENVIRONMENT_NAME\nexo dns remove $DOMAIN *.$CK8S_ENVIRONMENT_NAME\n</code></pre>"},{"location":"operator-manual/exoscale/#further-reading","title":"Further Reading","text":"<ul> <li>Elastisys Compliant Kubernetes Kubespray</li> <li>Kubernetes on Exoscale with Terraform</li> <li>Compliant Kubernetes apps repo</li> <li>Configurations option</li> </ul>"},{"location":"operator-manual/faq/","title":"Kubernetes Administrator FAQ","text":""},{"location":"operator-manual/faq/#i-updated-some-opensearch-options-but-it-didnt-work-now-what","title":"I updated some OpenSearch options but it didn't work, now what?","text":"<p>If you update the OpenSearch <code>securityConfig</code> you will have to make sure that the master Pod(s) are restarted so that they pick up the new Secret and then run the <code>securityadmin.sh</code> script. This happens for example if you switch from non-SSO to SSO.</p> <p>To reload the configuration you need to run the following commands:</p> <pre><code># Make the script executable\nkubectl -n opensearch-system exec opensearch-master-0 -- chmod +x ./plugins/opensearch-security/tools/securityadmin.sh\n# Run the script to update the configuration\nkubectl -n opensearch-system exec opensearch-master-0 -- ./plugins/opensearch-security/tools/securityadmin.sh \\\n-f plugins/opensearch-security/securityconfig/config.yml \\\n-icl -nhnv \\\n-cacert config/admin/ca.crt \\\n-cert config/admin/tls.crt \\\n-key config/admin/tls.key\n</code></pre> <p>Note that the above only reloads the <code>config.yml</code> (as specified with the <code>-f</code>). If you made changes to other parts of the system you will need to point to the relevant file to reload, or reload everything like this:</p> <pre><code># Run the script to update \"everything\" (internal users, roles, configuration, etc.)\nkubectl -n opensearch-system exec opensearch-master-0 -- ./plugins/opensearch-security/tools/securityadmin.sh \\\n-cd plugins/opensearch-security/securityconfig/ \\\n-icl -nhnv \\\n-cacert config/admin/ca.crt \\\n-cert config/admin/tls.crt \\\n-key config/admin/tls.key\n</code></pre> <p>When you update things other than <code>config.yml</code> you will also need to rerun the Configurer Job by syncing the <code>opensearch-configurer</code> chart.</p>"},{"location":"operator-manual/gcp/","title":"Gcp","text":"<p>This page is out of date</p> <p>We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date.</p> <p>Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly.</p>"},{"location":"operator-manual/gcp/#compliant-kubernetes-deployment-on-gcp","title":"Compliant Kubernetes Deployment on GCP","text":"<p>This document contains instructions on how to set up a Compliant Kubernetes environment (consisting of a service cluster and one or more workload clusters) on GCP. The document is split into two parts:</p> <ul> <li>Cluster setup (Setting up infrastructure and the Kubernetes clusters)</li> <li>Apps setup (including information about limitations)</li> </ul> <p>Before starting, make sure you have all necessary tools. In addition to these general tools, you will also need:</p> <ul> <li>A GCP project</li> <li>A JSON keyfile for running Terraform.</li> <li>SSH key that you will use to access GCP, which you have added to the metadata in your GCP project.</li> <li>(Optional) Another JSON keyfile for the GCP Persistent Disk CSI Driver. It is possible (but not recommended) to reuse the same JSON keyfile as you use for Terraform.</li> </ul> <p>Note</p> <p>This guide is written for compliantkubernetes-apps v0.13.0</p>"},{"location":"operator-manual/gcp/#initial-setup","title":"Initial setup","text":"<p>Choose names for your service cluster and workload cluster(s):</p> <pre><code>SERVICE_CLUSTER=\"testsc\"\nWORKLOAD_CLUSTERS=( \"testwc0\" )\n</code></pre>"},{"location":"operator-manual/gcp/#cluster-setup","title":"Cluster setup","text":"<ol> <li>Clone the compliantkubernetes-kubespray repository: <pre><code>git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray\ncd compliantkubernetes-kubespray\n</code></pre> For all commands in the cluster setup part of this guide, your working directory is assumed to be the root directory of this repository.</li> <li>In <code>config/gcp/group_vars/all/ck8s-gcp.yml</code>, set the value of <code>gcp_pd_csi_sa_cred_file</code> to the path of your JSON keyfile for GCP Persistent Disk CSI Driver.</li> <li>Modify <code>kubespray/contrib/terraform/gcp/tfvars.json</code> in the following way:<ul> <li>Set <code>gcp_project_id</code> to the ID of your GCP project.</li> <li>Set <code>keyfile_location</code> to the location of your JSON keyfile. This will be used as credentials for accessing the GCP API when running Terraform.</li> <li>Set <code>ssh_pub_key</code> to the path of your public SSH key.</li> <li>In <code>ssh_whitelist</code>, <code>api_server_whitelist</code> and <code>nodeport_whitelist</code>, add IP address(es) that you want to be able to access the cluster.</li> </ul> </li> <li> <p>Set up the nodes by performing the following steps:</p> <ol> <li>Make copies of the Terraform variables, one for each cluster: <pre><code>pushd kubespray/contrib/terraform/gcp\nfor CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTERS[@]}; do\ncp tfvars.json $CLUSTER-tfvars.json\ndone\npopd\n</code></pre></li> <li> <p>Set up the nodes with Terraform. If desired, first modify <code>\"machines\"</code> in <code>kubespray/contrib/terraform/gcp/$CLUSTER-tfvars.json</code> to add/remove nodes, change node sizes, etc. (For setting up compliantkubernetes-apps in the service cluster, one <code>n1-standard-8</code> worker and one <code>n1-standard-4</code> worker is enough.) <pre><code>pushd kubespray/contrib/terraform/gcp\nfor CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTERS[@]}; do\nterraform init\n    terraform apply -var-file $CLUSTER-tfvars.json -auto-approve -state $CLUSTER.tfstate -var prefix=$CLUSTER\ndone\npopd\n</code></pre> Save the outputs from <code>apply</code>. (Alternatively, get them later by running <code>terraform output -state $CLUSTER.tfstate</code> in the folder with the state file)</p> </li> <li> <p>Generate inventory file: <pre><code>pushd kubespray/contrib/terraform/gcp\nfor CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTERS[@]}; do\n./generate-inventory.sh $CLUSTER.tfstate &gt; $CLUSTER-inventory.ini\ndone\npopd\n</code></pre></p> </li> <li>Initialize the config: <pre><code>export CK8S_CONFIG_PATH=~/.ck8s/&lt;environment-name&gt;\n./bin/ck8s-kubespray init $CLUSTER gcp &lt;path to SSH key&gt; [&lt;SOPS fingerprint&gt;]\n</code></pre></li> <li><code>path to SSH key</code> should point to your private SSH key. It will be copied into your config path and encrypted with SOPS, the original file left as it were.</li> <li><code>SOPS fingerprint</code> is the gpg fingerprint that will be used for SOPS encryption. You need to set this or the environment variable <code>CK8S_PGP_FP</code> the first time SOPS is used in your specified config path.</li> <li>Copy the inventory files: <pre><code>pushd kubespray/contrib/terraform/gcp\nfor CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTERS[@]}; do\nmv $CLUSTER-inventory.ini $CK8S_CONFIG_PATH/$CLUSTER-config/inventory.ini\ndone\npopd\n</code></pre></li> <li>Run kubespray to set up the kubernetes cluster:     <pre><code>./bin/ck8s-kubespray apply $CLUSTER\n</code></pre></li> <li>In your config path, open <code>.state/kube_config_$CLUSTER.yaml</code> with SOPS and change <code>clusters.cluster.server</code> to the <code>control_plane_lb_ip_address</code> you got from <code>terraform apply</code>.</li> </ol> </li> </ol>"},{"location":"operator-manual/gcp/#apps-setup","title":"Apps setup","text":"<p>The following instructions were made for release v0.13.0 of compliantkubernetes-apps. There may be discrepancies with newer versions.</p>"},{"location":"operator-manual/gcp/#limitations","title":"Limitations","text":"<p>Note that there are a few limitations when using compliantkubernetes-apps on GCP at the moment, due to lack of support for certain features:</p> <ul> <li>Backup retention for InfluxDB is disabled due to it only being supported with S3 as object storage. If you want to circumvent this, consider using S3 as object storage or deploying Minio as a gateway.</li> <li>Fluentd does not work due to a missing output plugin. If you want to circumvent this, consider using S3 as object storage or deploying Minio as a gateway. Alternatively, Fluentd can be disabled in the compliantkubernetes-apps configuration, which has the consequence of no logs being saved from the service cluster.</li> </ul> <p>For information on how to modify the configuration to use S3 as object storage, refer to the administrator manual for AWS or Exoscale, in the section for apps configuration.</p>"},{"location":"operator-manual/gcp/#setup","title":"Setup","text":"<ol> <li> <p>Set up your DNS entries on a provider of your choice, using the <code>ingress_controller_lb_ip_address</code> from <code>terraform apply</code> as your loadbalancer IPs. You need the following entries:     <pre><code>*.ops.&lt;environment_name&gt;.$DOMAIN            A &lt;service_cluster_lb_ip&gt;\ngrafana.&lt;environment_name&gt;.$DOMAIN          A &lt;service_cluster_lb_ip&gt;\nharbor.&lt;environment_name&gt;.$DOMAIN           A &lt;service_cluster_lb_ip&gt;\nkibana.&lt;environment_name&gt;.$DOMAIN           A &lt;service_cluster_lb_ip&gt;\ndex.&lt;environment_name&gt;.$DOMAIN              A &lt;service_cluster_lb_ip&gt;\nnotary.harbor.&lt;environment_name&gt;.$DOMAIN    A &lt;service_cluster_lb_ip&gt;\n</code></pre></p> <p>Optionally, if alertmanager is enabled in the workload cluster, create the following DNS record:</p> <pre><code>*.&lt;environment_name&gt;.$DOMAIN    A &lt;workload_cluster_lb_ip&gt;\n</code></pre> </li> <li> <p>In <code>compliantkubernetes-apps</code>, run:     <pre><code>export CK8S_ENVIRONMENT_NAME=&lt;environment-name&gt;\nexport CK8S_CLOUD_PROVIDER=baremetal\n./bin/ck8s init\n</code></pre></p> </li> <li> <p>You will need to modify <code>secrets.yaml</code>, <code>sc-config.yaml</code> and <code>wc-config.yaml</code> in your config path.</p> <ul> <li><code>secrets.yaml</code><ul> <li>Uncomment <code>objectStorage.gcs.keyfileData</code> and paste the contents of your JSON keyfile as the value.</li> </ul> </li> <li><code>sc-config.yaml</code> AND <code>wc-config.yaml</code><ul> <li>Set <code>global.baseDomain</code> to <code>&lt;environment-name&gt;.&lt;dns-domain&gt;</code> and <code>global.opsDomain</code> to <code>ops.&lt;environment-name&gt;.&lt;dns-domain&gt;</code>.</li> <li>Set <code>global.issuer</code> to <code>letsencrypt-prod</code>.</li> <li>Set <code>storageClasses.default</code> to <code>csi-gce-pd</code>. Also set all <code>storageClasses.*.enabled</code> to <code>false</code>.</li> <li>Set <code>objectStorage.type</code> to <code>\"gcs\"</code>.</li> <li>Uncomment <code>objectStorage.gcs.project</code> and set it to the name of your GCP project.</li> </ul> </li> <li><code>sc-config.yaml</code><ul> <li>Set <code>influxDB.backupRetention.enabled</code> to <code>false</code>.</li> <li>Set <code>ingressNginx.controller.service.type</code> to <code>this-is-not-used</code></li> <li>Set <code>ingressNginx.controller.service.annotations</code> to <code>this-is-not-used</code></li> <li>Set <code>harbor.oidc.groupClaimName</code> to  <code>set-me</code></li> <li>Set <code>issuers.letsencrypt.prod.email</code> and <code>issuers.letsencrypt.staging.email</code> to email addresses of choice.</li> </ul> </li> </ul> </li> <li> <p>Create buckets for storage on GCP (found under \"Storage\"). The names must match the bucket names found in your <code>sc-config.yaml</code> and <code>wc-config.yaml</code> in the config path.</p> </li> <li>Set the default storageclass by running the following command:     <pre><code>bin/ck8s ops kubectl sc \"patch storageclass csi-gce-pd -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\"\nbin/ck8s ops kubectl wc \"patch storageclass csi-gce-pd -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\"\n</code></pre></li> <li>Apply the apps:     <pre><code>bin/ck8s apply sc\nbin/ck8s apply wc\n</code></pre></li> </ol> <p>Done. You should now have a functioning Compliant Kubernetes environment.</p>"},{"location":"operator-manual/getting-started/","title":"Getting Started","text":"<p>Setting up Compliant Kubernetes consists of two parts: setting up at least two vanilla Kubernetes clusters and deploying <code>compliantkubernetes-apps</code> on top of them.</p>"},{"location":"operator-manual/getting-started/#pre-requisites-for-creating-vanilla-kubernetes-clusters","title":"Pre-requisites for Creating Vanilla Kubernetes clusters","text":"<p>In theory, any vanilla Kubernetes cluster can be used for Compliant Kubernetes. We suggest the kubespray way. To this end, you need:</p> <ul> <li>Git</li> <li>Python3 pip</li> <li>Terraform</li> <li>Ansible</li> <li>pwgen</li> </ul> <p>Ansible is best installed as follows:</p> <pre><code>git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray\ncd compliantkubernetes-kubespray\npip3 install -r kubespray/requirements.txt\n</code></pre> <p>Optional: For debugging, you may want CLI tools to interact with your chosen cloud provider:</p> <ul> <li>AWS CLI</li> <li>Exoscale CLI</li> <li>OpenStack Client</li> <li>VMware vSphere CLI (govmomi)</li> </ul>"},{"location":"operator-manual/getting-started/#pre-requisites-for-compliantkubernetes-apps","title":"Pre-requisites for compliantkubernetes-apps","text":"<p>Using Ansible, these can be retrieved as follows:</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes-apps\ncd compliantkubernetes-apps\nansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127.0.0.1, get-requirements.yaml\n</code></pre>"},{"location":"operator-manual/getting-started/#misc","title":"Misc","text":"<p>Compliant Kubernetes relies on SSH for accessing nodes. If you haven't already done so, generate an SSH key as follows:</p> <pre><code>ssh-keygen\n</code></pre> <p>Configuration secrets in Compliant Kubernetes are encrypted using SOPS. We currently only support using PGP when encrypting secrets. If you haven't already done so, generate your own PGP key as follows:</p> <pre><code>gpg --full-generate-key\n</code></pre>"},{"location":"operator-manual/ingress/","title":"Ingress","text":"<p>Compliant Kubernetes (CK8s) uses the Nginx Ingress controller to route external traffic to the correct Service inside the cluster. CK8s can configure the Ingress controller in two different ways depending on the underlying infrastructure.</p>"},{"location":"operator-manual/ingress/#using-a-service-of-type-loadbalancer","title":"Using a Service of type LoadBalancer","text":"<p>When using a cloud provider with a Kubernetes cloud integration such as AWS, Azure and Google cloud the Ingress controller can be exposed with a Service of type LoadBalancer. This will create an external load balancer in the cloud provider with an external ip-address. Any dns records should be pointed to the ip-address of the load balancer.</p> <p>Note</p> <p>This is only currently supported in CK8s for AWS. It is however possible to configure this for Azure and Google cloud as well but it's not done by default</p>"},{"location":"operator-manual/ingress/#using-the-host-network","title":"Using the host network","text":"<p>For any cloud provider (or bare metal) not supporting these kind of public load balancers the Ingress controller uses the host network instead. This is done by configuring the Ingress controller as a DaemonSet so one Pod is created on each node. The Pods are configured to use the host network, so all traffic received on the node on port 80 and 443 will be intercepted by the Ingress controller Pod and then routed to the desired Service.</p> <p>On some clouds providers there is load balancing available for the worker nodes. For example Exoscale uses an \"elastic ip\" which provides one external ip which load balances to the available worker nodes. For these cloud providers this external ip of the load balancers should be used as the entry point in the dns.</p> <p>For the cloud providers where this is not available the easiest option is to just point the dns to the ip of any, or all, of the worker nodes. This is of course not a optimal solution because it adds a single point of failure on the worker node which is selected by the dns. Another option is to use any existing load balancer service if this is available.</p>"},{"location":"operator-manual/ingress/#installation","title":"Installation","text":"<p>The Nginx ingress is currently configured and installed by the compliantkubernetes-apps repository. The configuration is set in sc-config.yaml and wc-config.yaml under: <pre><code>ingressNginx:\nuseHostPort: \"\"\nservice:\nenabled: \"\"\ntype: \"\"\n</code></pre> If the apps repository is initiated with the correct cloud provider these config options will get the correct defaults.</p> <p>For more ways to install the Nginx Ingress controller see https://kubernetes.github.io/ingress-nginx/deploy</p>"},{"location":"operator-manual/ingress/#ingress-resource","title":"Ingress resource","text":"<p>The Ingress resource is used to later route traffic to the desired Service. For more information about this see the official documentation.</p>"},{"location":"operator-manual/maintenance/","title":"Maintaining and Upgrading your Compliant Kubernetes environment.","text":"<p>In order to keep your Compliant Kubernetes environment running smoothly, and to assure that you are up to date with the latest patches you need to perform regular maintenance on it.</p> <p>This guide assumes that:</p> <ul> <li>Your Compliant Kubernetes environment is running normally, if not, please see the Troubleshooting guide.</li> <li>Your Compliant Kubernetes environment is properly sized</li> <li>You have performed the actions in the Go-live Checklist as failure to do so might cause downtime during maintenance.</li> </ul>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7"]},{"location":"operator-manual/maintenance/#compliance-needs","title":"Compliance needs","text":"<p>Many regulations require you to secure your information system against unauthorized access, data loss, and breaches. An important part of this is keeping your Compliant Kubernetes environment up to date with the latest security patches not run outdated versions of components that are no longer supported. This maps to objectives in ISO Annex A.12.6.1 Management of Technical Vulnerabilities.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7"]},{"location":"operator-manual/maintenance/#what-maintenance-do-i-need-to-do-and-how","title":"What maintenance do I need to do and how?","text":"<p>In short, there are three levels of maintenance that should be performed on a regular basis.</p> <ul> <li>Patching the underlying OS on the nodes</li> <li>Upgrading the Compliant Kubernetes application stack</li> <li>Upgrading Compliant Kubernetes-Kubespray</li> </ul> <p>Let's go through them one by one.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7"]},{"location":"operator-manual/maintenance/#patching-the-nodes","title":"Patching the nodes","text":"<p>Security patches for the underlying OS on the nodes is constantly being released, and to ensure your environment is secured, the nodes that run Compliant Kubernetes must be updated with these patches. We recommend that you use the AutomaticSecurityUpdates feature that is available in Ubuntu (similar feature exist in other distros) to install these updates. Note that the nodes still need to be rebooted for some of these updates to be applied. In order to reboot the nodes, you can either use a tool like kured or you can do it manually by logging on to the nodes and rebooting them manually. When doing that, reboot one node at the time and make sure that the rebooted node is 'Ready' and that pods are scheduled to it before you move on to the next, or you risk downtime.</p> <p>There is a playbook in the compliantkubernetes-kubespray repo that can assist with the reboot of nodes. It will cordon and reboot the nodes one by one.</p> <pre><code>./bin/ck8s-kubespray reboot-nodes &lt;prefix&gt; [--extra-vars manual_prompt=true] [&lt;options&gt;]\n</code></pre>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7"]},{"location":"operator-manual/maintenance/#upgrading-the-compliant-kubernetes-application-stack","title":"Upgrading the Compliant Kubernetes application stack","text":"<p>Compliant Kubernetes consists of a multitude of open source components that interact to form a smooth end user experience. In order to free you of the burden of keeping track of when to upgrade the various components, new versions of Complaint Kubernetes are regularly release. When a new version is released, it becomes available as a tagged release in the github repo.</p> <p>Before upgrading to a new release, please review the changelog if possible, apply the upgrade to a staging environment before upgrading any environments with production data.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7"]},{"location":"operator-manual/maintenance/#prerequisites","title":"Prerequisites","text":"<ul> <li> Notify the users (if any) before the upgrade starts;</li> <li> Check if there are any pending changes to the environment;</li> <li> Check the state of the environment, pods, nodes and backup jobs:</li> </ul> <p>NOTE: the below steps should be run from compliantkubernetes-apps root directory</p> <pre><code>./bin/ck8s test sc|wc\n./bin/ck8s ops kubectl sc|wc get pods -A -o custom-columns=NAMESPACE:metadata.namespace,POD:metadata.name,READY-false:status.containerStatuses[*].ready,REASON:status.containerStatuses[*].state.terminated.reason | grep false | grep -v Completed\n./bin/ck8s ops kubectl sc|wc get nodes\n./bin/ck8s ops kubectl sc|wc get jobs -A\nvelero get backup\n</code></pre> <ul> <li> Silence the notifications for the alerts. e.g you can use alertmanager silences;</li> </ul>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7"]},{"location":"operator-manual/maintenance/#upgrading-compliantkubernetes-apps","title":"Upgrading compliantkubernetes-apps","text":"<p>For security, compliance, and support reasons, environments should stay up to date with the latest version of compliankubernetes-apps.</p> <p>Note what version of compliantkubernetes-apps that is currently used and the version that you want to upgrade to. Then check the release notes for each version in between to see if there are anything that might cause any problems, if so then consult the rest of the operations team before proceeding. You should never upgrade more than one minor version of compliantkubernetes-apps at a time.</p> <ol> <li> <p>Pull the latest changes and switch to the correct branch:</p> <pre><code>git pull\ngit switch -d &lt;next-version&gt;\n</code></pre> </li> <li> <p>Update apps configuration:</p> <p>This will take a backup into <code>backups/</code> before modifying any files.</p> <pre><code>./bin/ck8s init\n</code></pre> </li> <li> <p>Check if there is a migration document for the release you want to upgrade to, (e.g. for upgrade to 0.11.0 ) and follow the instructions there. Note that you should check the documentation at the release tag instead of <code>main</code> to be sure that it's correct.</p> </li> <li> <p>If there is no relevant migration document, first do a dry-run.</p> <pre><code>./bin/ck8s dry-run sc\n./bin/ck8s dry-run wc\n</code></pre> </li> <li> <p>If dry-run reports no errors, proceed with the upgrade.</p> <pre><code>./bin/ck8s apply sc\n./bin/ck8s apply wc\n</code></pre> </li> <li> <p>Verify that everything is running after the upgrade. At the minimum, at least run the tests in compliantkubernetes-apps.</p> <pre><code>./bin/ck8s test sc\n./bin/ck8s test wc\n</code></pre> </li> <li> <p>Go back to step 1 and repeat one new release of compliantkubernetes-apps at a time until you are at the latest release.</p> </li> </ol>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7"]},{"location":"operator-manual/maintenance/#upgrading-kubespraykubernetes","title":"Upgrading Kubespray/Kubernetes","text":"<p>All clusters should stay up to date with the latest Kubespray version used in compliantkubernetes-kubespray.</p> <ol> <li> <p>Note what version of Kubespray that is currently used in the cluster and the Kubespray version we want to upgrade to. Then check the release notes for each version in between to see if there are anything that might cause any problems, if so then consult the rest of the operations team before proceeding. Also check if the newer Kubespray version would upgrade Kubernetes to a new minor version, if so then the customer should get a notice of x weeks before proceeding to let them check for any deprecated APIs that they might be using. You should never upgrade more than one patch version of Kubespray at a time. E.g. if you are at Kubespray version 2.13.3 and are going to 2.15.0 then the upgrade path would be 2.13.3 -&gt; 2.13.4 -&gt; 2.14.0 -&gt; 2.14.1 -&gt; 2.14.2 -&gt; 2.15.0. Patches that are released to an older minor version can be skipped, e.g. new patches to 2.14 after 2.15 has been released. Read more about Kubespray upgrades in their documentation.</p> </li> <li> <p>Checkout the next Kubespray version by checking out the last compliantkubernetes-kubespray commit (the commit is <code>next-version</code> in the snippet below) that used that version and updating the submodule.</p> <pre><code># you should be in the root folder of compliantkubernetes-kubespray\ngit switch -d &lt;next-version&gt;\ngit submodule sync\ngit submodule update --init --recursive\n</code></pre> </li> <li> <p>Upgrade compliantkubernetes-kubespray by following the relevant documentation (e.g. for upgrade to v2.17.x-ck8s1).</p> </li> <li> <p>Download the required files on the nodes</p> <pre><code>./bin/ck8s-kubespray run-playbook sc upgrade-cluster.yml -b --tags=download\n./bin/ck8s-kubespray run-playbook wc upgrade-cluster.yml -b --tags=download\n</code></pre> </li> <li> <p>Upgrade the cluster to a new kubernetes version:</p> <pre><code>./bin/ck8s-kubespray run-playbook sc upgrade-cluster.yml -b --skip-tags=download\n./bin/ck8s-kubespray run-playbook wc upgrade-cluster.yml -b --skip-tags=download\n</code></pre> </li> </ol>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7"]},{"location":"operator-manual/maintenance/#after-doing-any-upgrades-or-maintenance","title":"After doing any upgrades or maintenance","text":"<ul> <li> Check the state of the environment, pods and nodes:</li> </ul> <p>NOTE: the below steps should be run from compliantkubernetes-apps root directory</p> <pre><code>./bin/ck8s test sc|wc\n./bin/ck8s ops kubectl sc|wc get pods -A -o custom-columns=NAMESPACE:metadata.namespace,POD:metadata.name,READY-false:status.containerStatuses[*].ready,REASON:status.containerStatuses[*].state.terminated.reason | grep false | grep -v Completed\n./bin/ck8s ops kubectl sc|wc get nodes\n</code></pre> <ul> <li> Check if any alerts generated by the upgrade didn't close;</li> <li> Check if you can login to Grafana, Opensearch or Harbor;</li> <li> Enable the notifications for the alerts;</li> <li> Notify the users (if any) when the upgrade is complete;</li> <li> Check that you can see fresh metrics and logs.</li> </ul>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7"]},{"location":"operator-manual/on-prem-standard/","title":"Standard Template for on-prem Environment","text":"<p>This document contains instructions on how to set-up a new Compliant Kubernetes on-prem environment.</p>"},{"location":"operator-manual/on-prem-standard/#prerequisites","title":"Prerequisites","text":"<p>Decision to be taken</p> <p>Decisions regarding the following items should be made before venturing on deploying Compliant Kubernetes.</p> <ul> <li>Overall architecture, i.e., VM sizes, load-balancer configuration, storage configuration, etc.</li> <li>Identity Provider (IdP) choice and configuration. See this blog post.</li> <li>On-call Management Tool (OMT) choice and configuration</li> </ul> <ol> <li> <p>Prepare Ubuntu-based VMs:     If you are using public clouds, you can create VMs using the scripts included in Kubespray:</p> <ul> <li>For Azure, use AzureRM scripts.</li> <li>For other clouds, use their respective Terraform scripts.</li> </ul> </li> <li> <p>Create a git working folder to store Compliant Kubernetes configurations in a version-controlled manner. Run the following commands from the root of the config repo.</p> <p>Note</p> <p>The following steps are done from the root of the git repository you created for the cofigurations.</p> <p>Note</p> <p>You can choose names for your service cluster and workload cluster by changing the values for <code>SERVICE_CLUSTER</code> and <code>WORKLOAD_CLUSTERS</code> respectively.</p> <pre><code>export CK8S_CONFIG_PATH=./\nexport CK8S_ENVIRONMENT_NAME=&lt;my-ck8s-cluster&gt;\nexport CK8S_CLOUD_PROVIDER=[exoscale|safespring|citycloud|elastx|aws|baremetal]\nexport CK8S_FLAVOR=[dev|prod] # defaults to dev\nexport CK8S_PGP_FP=&lt;PGP-fingerprint&gt; # retrieve with gpg --list-secret-keys\nSERVICE_CLUSTER=\"sc\"\nWORKLOAD_CLUSTERS=\"wc\"\n</code></pre> </li> <li> <p>Add the Elastisys Compliant Kubernetes Kubespray repo as <code>git submodule</code> to the configuration repo as follows:</p> <pre><code>git submodule add  https://github.com/elastisys/compliantkubernetes-kubespray\ngit submodule update --init --recursive\n</code></pre> </li> <li> <p>Add compliantkubernetes-apps as <code>git submodule</code> to the configuration repo and install pre-requisites as follows:</p> <pre><code>https://github.com/elastisys/compliantkubernetes-apps.git\ncd compliantkubernetes-apps\nansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127.0.0.1, get-requirements.yaml\n</code></pre> </li> <li> <p>Create the domain name.     You need to create a domain name to access the different services in your environment. You will need to set up the following DNS entries (replace <code>example.com</code> with your domain name).</p> <ul> <li>Point these domains to the workload cluster ingress controller (this step is done during Compliant Kubernetes app installation):<ul> <li><code>*.example.com</code></li> </ul> </li> <li>Point these domains to the service cluster ingress controller (this step is done during Compliant Kubernetes app installation):<ul> <li><code>*.ops.example.com</code></li> <li><code>dex.example.com</code></li> <li><code>grafana.example.com</code></li> <li><code>harbor.example.com</code></li> <li><code>opensearch.example.com</code></li> </ul> </li> </ul> If both service and workload clusters are in the same subnet <p>If both the service and workload clusters are in the same subnet, it would be great to configure the following domain names to the private IP addresses of service cluster's worker nodes. (Replace <code>example.com</code> with your domain name.)</p> <ul> <li><code>*.thanos.ops.example.com</code></li> <li><code>*.opensearch.ops.example.com</code></li> </ul> </li> <li> <p>Create S3 credentials and add them to <code>.state/s3cfg.ini</code>.</p> </li> <li> <p>Set up load balancer</p> <p>You need to set up two load balancers, one for the workload cluster and one for the service cluster.</p> </li> <li> <p>Make sure you have all necessary tools.</p> </li> </ol>"},{"location":"operator-manual/on-prem-standard/#deploying-compliant-kubernetes-using-kubespray","title":"Deploying Compliant Kubernetes using Kubespray","text":"How to change Default Kubernetes Subnet Address <p>If  the default IP block ranges used for Docker and Kubernetes are the same as the internal IP ranges used in the company, you can change the values  to resolve the conflict as follows. Note that you can use any valid private IP address range, the values below are put as an example.</p> For KubernetesFor Docker <pre><code>* For service cluster: Add `kube_service_addresses: 10.178.0.0/18` and `kube_pods_subnet: 10.178.120.0/18` in `${CK8S_CONFIG_PATH}/sc-config/group_vars/k8s_cluster/ck8s-k8s-cluster.yaml` file.\n* For workload cluster:  Add `kube_service_addresses: 10.178.0.0/18` and `kube_pods_subnet: 10.178.120.0/18` in `${CK8S_CONFIG_PATH}/wc-config/group_vars/k8s_cluster/ck8s-k8s-cluster.yaml` file.\n</code></pre> <pre><code>* For service cluster: Added `docker_options: \"--default-address-pool base=10.179.0.0/24,size=24\"` in `${CK8S_CONFIG_PATH}/sc-config/group_vars/all/docker.yml` file.\n* For workload cluster:  Added `docker_options: \"--default-address-pool base=10.179.4.0/24,size=24\"` in `${CK8S_CONFIG_PATH}/wc-config/group_vars/all/docker.yml` file.\n</code></pre>"},{"location":"operator-manual/on-prem-standard/#init-kubespray-config-in-your-config-path","title":"Init Kubespray config in your config path.","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"{WORKLOAD_CLUSTERS}\"; do\ncompliantkubernetes-kubespray/ck8s-kubespray init $CLUSTER $CK8S_CLOUD_PROVIDER $CK8S_PGP_FP\ndone\n</code></pre>"},{"location":"operator-manual/on-prem-standard/#configure-oidc","title":"Configure OIDC","text":"<p>To configure OpenID access for Kubernetes API and other services, Dex should be configured with your identity provider. Check what Dex needs from your identity provider.</p>"},{"location":"operator-manual/on-prem-standard/#configure-oidc-endpoint","title":"Configure OIDC endpoint","text":"<p>Set <code>kube_oidc_url</code> in <code>${CK8S_CONFIG_PATH}/sc-config/group_vars/k8s_cluster/ck8s-k8s-cluster.yaml</code> and <code>${CK8S_CONFIG_PATH}/wc-config/group_vars/k8s_cluster/ck8s-k8s-cluster.yaml</code> based on your cluster. For example, if your domain is <code>example.com</code> then kube_oidc_url is set as <code>kube_oidc_url: https://dex.example.com</code> in both files.</p>"},{"location":"operator-manual/on-prem-standard/#copy-the-vms-information-to-the-inventery-files","title":"Copy the VMs information to the inventery files","text":"<p>Add the host name, user and IP address of each VM that you prepared above in <code>${CK8S_CONFIG_PATH}/sc-config/inventory.ini</code>for service cluster and <code>${CK8S_CONFIG_PATH}/sc-config/inventory.ini</code> for workload cluster. Moreover, you also need to add the host names of the master nodes under <code>[kube_control_plane]</code>, etdc nodes under <code>[etcd]</code> and worker nodes under <code>[kube_node]</code>.</p> <p>Note</p> <p>Make sure that the user has SSH access to the VMs.</p>"},{"location":"operator-manual/on-prem-standard/#run-kubespray-to-deploy-the-kubernetes-clusters","title":"Run Kubespray to deploy the Kubernetes clusters","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTERS}; do\ncompliantkubernetes-kubespray/bin/ck8s-kubespray apply $CLUSTER --flush-cache\ndone\n</code></pre> <p>Note</p> <p>The kubeconfig for wc <code>.state/kube_config_wc.yaml</code> will not be usable until you have installed dex in the service cluster (by deploying apps).</p>"},{"location":"operator-manual/on-prem-standard/#set-up-rook","title":"Set up Rook","text":"<p>Only for cloud providers that doesn't natively support storage kubernetes.</p> <p>Run the following command to set up Rook.</p> <pre><code> for CLUSTER in  sc wc; do\nsops --decrypt ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml &gt; $CLUSTER.yaml\n     export KUBECONFIG=$CLUSTER.yaml\n     compliantkubernetes-kubespray/rook/deploy-rook.sh\n done\n</code></pre> <p>Please restart the operator pod, <code>rook-ceph-operator*</code>, if some pods stall in the initialization state as shown below:</p> <pre><code>rook-ceph     rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg    0/1     Init:0/2   0          24m\nrook-ceph     rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh   0/1     Init:0/2   0          36m\nrook-ceph     rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55   0/1     Init:0/2   0          14m\n</code></pre> <p>Important</p> <p>Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances.</p>"},{"location":"operator-manual/on-prem-standard/#test-rook","title":"Test Rook","text":"<p>To test Rook, proceed as follows:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml 'kubectl --kubeconfig {} apply -f https://raw.githubusercontent.com/rook/rook/release-1.5/cluster/examples/kubernetes/ceph/csi/rbd/pvc.yaml';\ndone\n\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml 'kubectl --kubeconfig {} get pvc';\ndone\n</code></pre> <p>You should see PVCs in <code>Bound</code> state. If you want to clean the previously created PVCs:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml 'kubectl --kubeconfig {} delete pvc rbd-pvc';\ndone\n</code></pre>"},{"location":"operator-manual/on-prem-standard/#deploying-compliant-kubernetes-apps","title":"Deploying Compliant Kubernetes Apps","text":"How to change local DNS IP if you change the default Kubebernetes subnet address <p>You need to change the default coreDNS default IP address in <code>common-config.yaml</code> file if  you change the default IP block  used for Kubernetes services above.  To get the coreDNS IP address, run the following commands.</p> <p><pre><code>${CK8S_CONFIG_PATH}/compliantkubernetes-apps/bin/ck8s ops kubectl sc get svc -n kube-system coredns\n</code></pre> Once you get the IP address edit <code>${CK8S_CONFIG_PATH}/scommon-config.yaml</code> file  and set  the value  to <code>global.clusterDns</code> field.</p> Configure the load balancer IP on the loopback interface for each worker node <p>The Kubernetes data planenodes (i.e., worker nodes) cannot connect to themselves with the IP address of the load balancer that fronts them. The easiest is to configure the load balancer's IP address on the loopback interface of each nodes. Create <code>/etc/netplan/20-eip-fix.yaml</code> file and add the following to it. <code>${loadblancer_ip_address}</code> should be replaced with the IP address of the load balancer for each cluster.</p> <p><pre><code>network:\nversion: 2\nethernets:\n\"lo:0\":\nmatch:\nname: lo\ndhcp4: false\naddresses:\n- ${loadblancer_ip_address}/32\n</code></pre> After adding the above content, run the following command in each worker node:</p> <pre><code>sudo netplan apply\n</code></pre>"},{"location":"operator-manual/on-prem-standard/#initialize-the-apps-configuration","title":"Initialize the apps configuration","text":"<pre><code>compliantkubernetes-apps/bin/ck8s init\n</code></pre> <p>This will initialise the configuration in the <code>${CK8S_CONFIG_PATH}</code> directory. Generating configuration files <code>common-config.yaml</code>, <code>sc-config.yaml</code> and <code>wc-config.yaml</code>, as well as secrets with randomly generated passwords in <code>secrets.yaml</code>. This will also generate read-only default configuration under the directory <code>defaults/</code> which can be used as a guide for available and suggested options.</p>"},{"location":"operator-manual/on-prem-standard/#configure-the-apps-and-secrets","title":"Configure the apps and secrets","text":"<p>The configuration files contain some predefined values. You may want to check and edit based on your current environment requirements. The configuration files that require editing are <code>${CK8S_CONFIG_PATH}/common-config.yaml</code>, <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code>, <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/secrets.yaml</code> and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted.</p> <pre><code>vim ${CK8S_CONFIG_PATH}/sc-config.yaml\n\nvim ${CK8S_CONFIG_PATH}/wc-config.yaml\n\nvim ${CK8S_CONFIG_PATH}/common-config.yaml\n</code></pre> <p>Edit the secrets.yaml file and add the credentials for:</p> <ul> <li>s3 - used for backup storage</li> <li>dex - connectors -- check your indentiy provider.</li> <li>On-call management tool configurations-- Check supported on-call management tools</li> </ul> <pre><code>sops ${CK8S_CONFIG_PATH}/secrets.yaml\n</code></pre> <p>Tip</p> <p>The default configuration for the service cluster and workload cluster are available in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code> and can be used as a reference for available options.</p> <p>Warning</p> <p>Do not modify the read-only default configurations files found in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code>. Instead configure the cluster by modifying the regular files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> as they will override the default options.</p>"},{"location":"operator-manual/on-prem-standard/#create-s3-buckets","title":"Create S3 buckets","text":"<p>You can use the following command to create the required S3 buckets. The command uses <code>s3cmd</code> in the background and gets configuration and credentials for your S3 provider from the <code>~/.s3cfg</code> file.</p> <pre><code>compliantkubernetes-apps/bin/ck8s s3cmd create\n</code></pre>"},{"location":"operator-manual/on-prem-standard/#install-compliant-kubernetes-apps","title":"Install Compliant Kubernetes apps","text":"<p>This will set up apps, first in the service cluster and then in the workload cluster:</p> <pre><code>compliantkubernetes-apps/bin/ck8s apply sc\ncompliantkubernetes-apps/bin/ck8s apply wc\n</code></pre>"},{"location":"operator-manual/on-prem-standard/#settling","title":"Settling","text":"<p>Info</p> <p>Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes.</p> <p>Check if all helm charts succeeded.</p> <pre><code>compliantkubernetes-apps/bin/ck8s ops helm wc list -A --all\n</code></pre> <p>You can check if the system settled as follows.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\ncompliantkubernetes-apps/bin/ck8s ops kubectl $CLUSTER get --all-namespaces pods\ndone\n</code></pre> <p>Check the output of the command above. All Pods need to be <code>Running</code> or <code>Completed</code> status.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\ncompliantkubernetes-apps/bin/ck8s ops kubectl $CLUSTER get --all-namespaces issuers,clusterissuers,certificates\ndone\n</code></pre> <p>Check the output of the command above. All resources need to have the <code>Ready</code> column <code>True</code>.</p>"},{"location":"operator-manual/on-prem-standard/#testing","title":"Testing","text":"<p>After completing the installation step you can test if the apps are properly installed and ready using the commands below.</p> <p>Start with the service cluster:</p> <pre><code>compliantkubernetes-apps/bin/ck8s test sc\n</code></pre> <p>Then the workload clusters:</p> <pre><code>compliantkubernetes-apps/bin/ck8s test wc\n</code></pre>"},{"location":"operator-manual/openstack/","title":"Openstack","text":"<p>This page is out of date</p> <p>We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date.</p> <p>Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly.</p>"},{"location":"operator-manual/openstack/#compliant-kubernetes-on-openstack","title":"Compliant Kubernetes on Openstack","text":"<p>This document contains instructions on how to set up a Compliant Kubernetes environment (consisting of a service cluster and one or more workload clusters) on Openstack.</p> <ol> <li>Infrastructure setup for two clusters: one service and one workload cluster</li> <li>Deploying Compliant Kubernetes on top of the two clusters.</li> <li>Creating DNS Records</li> <li>Deploying Compliant Kubernetes apps</li> </ol> <p>Before starting, make sure you have all necessary tools. In addition to these general tools, you will also need: - Openstack credentials (either using <code>openrc</code> or the <code>clouds.yaml</code> configuration file) for setting up the infrastructure.</p> <p>Note</p> <p>Although recommended OpenStack authentication method is <code>clouds.yaml</code>, it is more convenient to use the <code>openrc</code> method with Compliant Kubernetes as it works both with Kubespray and Terraform. If you are using the <code>clouds.yaml</code> method, at the moment, Kubespray will still expect you to set a few environment variables.</p> <p>Note</p> <p>This guide is written for compliantkubernetes-apps v0.17.0</p>"},{"location":"operator-manual/openstack/#setup","title":"Setup","text":"<p>Choose names for your service cluster and workload cluster(s):</p> <pre><code>SERVICE_CLUSTER=\"sc\"\nWORKLOAD_CLUSTERS=( \"wc0\" \"wc1\" )\n\nexport CK8S_CONFIG_PATH=~/.ck8s/&lt;environment-name&gt;\nexport SOPS_FP=&lt;PGP-fingerprint&gt; # retrieve with gpg --list-secret-keys\n</code></pre>"},{"location":"operator-manual/openstack/#infrastructure-setup-using-terraform","title":"Infrastructure setup using Terraform","text":"<p>Before trying any of the steps, clone the Elastisys Compliant Kubernetes Kubespray repo as follows: <pre><code>git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray\n</code></pre></p>"},{"location":"operator-manual/openstack/#expose-openstack-credentials-to-terraform","title":"Expose Openstack credentials to Terraform","text":"<p>Terraform will need access to Openstack credentials in order to create the infrastructure. More details can be found here. We will be using the declarative option with the <code>open.rc</code> file.</p> <p>Expose Openstack credentials to Terraform For authentication create or download, from your provider, the file openstack-rc and <code>source path/to/your/openstack-rc</code>. The file should contain the following variables: <pre><code>export OS_USERNAME=\nexport OS_PASSWORD=\nexport OS_AUTH_URL=\nexport OS_USER_DOMAIN_NAME=\nexport OS_PROJECT_DOMAIN_NAME=\nexport OS_REGION_NAME=\nexport OS_PROJECT_NAME=\nexport OS_TENANT_NAME=\nexport OS_AUTH_VERSION=\nexport OS_IDENTITY_API_VERSION=\nexport OS_PROJECT_ID=\n</code></pre></p>"},{"location":"operator-manual/openstack/#customize-your-infrastructure","title":"Customize your infrastructure","text":"<p>Start by initializing a Compliant Kubernetes environment using Compliant Kubernetes Kubespray. All of this is done from the root of the <code>compliantkubernetes-kubespray</code> repository.</p> <pre><code>for CLUSTER in \"${SERVICE_CLUSTER}\" \"${WORKLOAD_CLUSTERS[@]}\"; do\n./bin/ck8s-kubespray init \"${CLUSTER}\" openstack \"${SOPS_FP}\"\ndone\n</code></pre> <p>Configure Terraform by creating a <code>cluster.tfvars</code> file for each cluster. The available options can be seen in <code>kubespray/contrib/terraform/openstack/variables.tf</code>. There is a sample file that can be copied to get something to start from.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTERS[@]}; do\ncp kubespray/contrib/terraform/openstack/sample-inventory/cluster.tfvars \"${CK8S_CONFIG_PATH}/${CLUSTER}-config/cluster.tfvars\"\ndone\n</code></pre> <p>Note</p> <p>You really must edit the values in these files. There is no way to set sane defaults for what flavor to use, what availability zones or networks are available across providers. In the section below some guidance and samples are provided but remember that they might be useless to you depending on your needs and setup.</p>"},{"location":"operator-manual/openstack/#infrastructure-guidance","title":"Infrastructure guidance","text":"<p>The minimum infrastructure sizing requirements are at least three worker nodes with 4 cores and 8 GB memory each, and we recommend you to have at least 2 cores and 4 GB for your control plane nodes.</p> <p>Note</p> <p>A recommended production infrastructure sizing is available in the architecture diagram.</p> <p>Below is example <code>cluster.tfvars</code> for a few select openstack providers. The examples are copy-pastable, but you might want to change <code>cluster_name</code> and <code>network_name</code> (if neutron is used!).</p> Citycloud Fra1Citycloud Kna1Safespring sto1 <pre><code># your Kubernetes cluster name here\ncluster_name = \"your-cluster-name\"\n\n# list of availability zones available in your OpenStack cluster\n#az_list = [\"nova\"]\n\n# SSH key to use for access to nodes\npublic_key_path = \"~/.ssh/id_rsa.pub\"\n\n# image to use for bastion, masters, standalone etcd instances, and nodes\nimage = \"Ubuntu 20.04 Focal Fossa 20200423\"\n\n# user on the node (ex. core on Container Linux, ubuntu on Ubuntu, etc.)\nssh_user = \"ubuntu\"\n\n# 0|1 bastion nodes\nnumber_of_bastions = 0\n\n# standalone etcds\nnumber_of_etcd = 0\n\n# masters\nnumber_of_k8s_masters = 1\n\nnumber_of_k8s_masters_no_etcd = 0\n\nnumber_of_k8s_masters_no_floating_ip = 0\n\nnumber_of_k8s_masters_no_floating_ip_no_etcd = 0\n\n# Flavor depends on your openstack installation\n# you can get available flavor IDs through `openstack flavor list`\nflavor_k8s_master = \"89afeed0-9e41-4091-af73-727298a5d959\"\"\n\n# nodes\nnumber_of_k8s_nodes = 3\n\nnumber_of_k8s_nodes_no_floating_ip = 0\n\n# Flavor depends on your openstack installation\n# you can get available flavor IDs through `openstack flavor list`\nflavor_k8s_node = \"ecd976c3-c71c-4096-b138-e4d964c0b27f\"\n\n# networking\n# ssh access to nodes\nk8s_allowed_remote_ips = [\"0.0.0.0/0\"]\n\n# List of CIDR blocks allowed to initiate an API connection\nmaster_allowed_remote_ips = [\"0.0.0.0/0\"]\n\nworker_allowed_ports = [\n  { # Node ports\n    \"protocol\"         = \"tcp\"\n    \"port_range_min\"   = 30000\n    \"port_range_max\"   = 32767\n    \"remote_ip_prefix\" = \"0.0.0.0/0\"\n  },\n  { # HTTP\n    \"protocol\"         = \"tcp\"\n    \"port_range_min\"   = 80\n    \"port_range_max\"   = 80\n    \"remote_ip_prefix\" = \"0.0.0.0/0\"\n  },\n  { # HTTPS\n    \"protocol\"         = \"tcp\"\n    \"port_range_min\"   = 443\n    \"port_range_max\"   = 443\n    \"remote_ip_prefix\" = \"0.0.0.0/0\"\n  }\n]\n\n# use `openstack network list` to list the available external networks\nnetwork_name = \"name-of-your-network\"\n\n# UUID of the external network that will be routed to\nexternal_net = \"your-external-network-uuid\"\nfloatingip_pool = \"ext-net\"\n\n# If 1, nodes with floating IPs will transmit internal cluster traffic via floating IPs; if 0 private IPs will be used instead. Default value is 1.\nuse_access_ip = 0\n\n# Create and use openstack nova servergroups, default: false\nuse_server_groups = true\n\nsubnet_cidr = \"172.16.0.0/24\"\n</code></pre> <pre><code># your Kubernetes cluster name here\ncluster_name = \"your-cluster-name\"\n\n# list of availability zones available in your OpenStack cluster\n#az_list = [\"nova\"]\n\n# SSH key to use for access to nodes\npublic_key_path = \"~/.ssh/id_rsa.pub\"\n\n# image to use for bastion, masters, standalone etcd instances, and nodes\nimage = \"Ubuntu 20.04 Focal Fossa 20200423\"\n\n# user on the node (ex. core on Container Linux, ubuntu on Ubuntu, etc.)\nssh_user = \"ubuntu\"\n\n# 0|1 bastion nodes\nnumber_of_bastions = 0\n\n# standalone etcds\nnumber_of_etcd = 0\n\n# masters\nnumber_of_k8s_masters = 1\n\nnumber_of_k8s_masters_no_etcd = 0\n\nnumber_of_k8s_masters_no_floating_ip = 0\n\nnumber_of_k8s_masters_no_floating_ip_no_etcd = 0\n\n# Flavor depends on your openstack installation\n# you can get available flavor IDs through `openstack flavor list`\nflavor_k8s_master = \"96c7903e-32f0-421d-b6a2-a45c97b15665\"\n\n# nodes\nnumber_of_k8s_nodes = 3\n\nnumber_of_k8s_nodes_no_floating_ip = 0\n\n# Flavor depends on your openstack installation\n# you can get available flavor IDs through `openstack flavor list`\nflavor_k8s_node = \"572a3b2e-6329-4053-b872-aecb1e70d8a6\"\n\n# networking\n# ssh access to nodes\nk8s_allowed_remote_ips = [\"0.0.0.0/0\"]\n\n# List of CIDR blocks allowed to initiate an API connection\nmaster_allowed_remote_ips = [\"0.0.0.0/0\"]\n\nworker_allowed_ports = [\n  { # Node ports\n    \"protocol\"         = \"tcp\"\n    \"port_range_min\"   = 30000\n    \"port_range_max\"   = 32767\n    \"remote_ip_prefix\" = \"0.0.0.0/0\"\n  },\n  { # HTTP\n    \"protocol\"         = \"tcp\"\n    \"port_range_min\"   = 80\n    \"port_range_max\"   = 80\n    \"remote_ip_prefix\" = \"0.0.0.0/0\"\n  },\n  { # HTTPS\n    \"protocol\"         = \"tcp\"\n    \"port_range_min\"   = 443\n    \"port_range_max\"   = 443\n    \"remote_ip_prefix\" = \"0.0.0.0/0\"\n  }\n]\n# use `openstack network list` to list the available external networks\nnetwork_name = \"name-of-your-network\"\n\n# UUID of the external network that will be routed to\nexternal_net = \"your-external-network-uuid\"\nfloatingip_pool = \"ext-net\"\n\n# If 1, nodes with floating IPs will transmit internal cluster traffic via floating IPs; if 0 private IPs will be used instead. Default value is 1.\nuse_access_ip = 0\n\n# Create and use openstack nova servergroups, default: false\nuse_server_groups = true\n\nsubnet_cidr = \"172.16.0.0/24\"\n</code></pre> <pre><code># your Kubernetes cluster name here\ncluster_name = \"your-cluster-name\"\n\n# SSH key to use for access to nodes\npublic_key_path = \"~/.ssh/id_rsa.pub\"\n\n# image to use for bastion, masters, standalone etcd instances, and nodes\nimage = \"ubuntu-20.04\"\n\n# user on the node (ex. core on Container Linux, ubuntu on Ubuntu, etc.)\nssh_user = \"ubuntu\"\n\n# 0|1 bastion nodes\nnumber_of_bastions = 0\n\nuse_neutron = 0\n\n# standalone etcds\nnumber_of_etcd = 0\n\n# masters\nnumber_of_k8s_masters = 0\n\nnumber_of_k8s_masters_no_etcd = 0\n\nnumber_of_k8s_masters_no_floating_ip = 1\n\nnumber_of_k8s_masters_no_floating_ip_no_etcd = 0\n\n# Flavor depends on your openstack installation\n# you can get available flavor IDs through `openstack flavor list`\nflavor_k8s_master = \"8a707999-0bce-4f2f-8243-b4253ba7c473\"\n\n# nodes\nnumber_of_k8s_nodes = 0\n\nnumber_of_k8s_nodes_no_floating_ip = 3\n\n# Flavor depends on your openstack installation\n# you can get available flavor IDs through `openstack flavor list`\nflavor_k8s_node = \"5b40af67-9d11-45ed-a44f-e876766160a5\"\n\n# networking\n# ssh access to nodes\nk8s_allowed_remote_ips = [\"0.0.0.0/0\"]\n\n# List of CIDR blocks allowed to initiate an API connection\nmaster_allowed_remote_ips = [\"0.0.0.0/0\"]\n\nworker_allowed_ports = [\n  { # Node ports\n    \"protocol\"         = \"tcp\"\n    \"port_range_min\"   = 30000\n    \"port_range_max\"   = 32767\n    \"remote_ip_prefix\" = \"0.0.0.0/0\"\n  },\n  { # HTTP\n    \"protocol\"         = \"tcp\"\n    \"port_range_min\"   = 80\n    \"port_range_max\"   = 80\n    \"remote_ip_prefix\" = \"0.0.0.0/0\"\n  },\n  { # HTTPS\n    \"protocol\"         = \"tcp\"\n    \"port_range_min\"   = 443\n    \"port_range_max\"   = 443\n    \"remote_ip_prefix\" = \"0.0.0.0/0\"\n  }\n]\n\n# use `openstack network list` to list the available external networks\nnetwork_name = \"public\"\n\n# UUID of the external network that will be routed to\nexternal_net = \"your-external-network-uuid\"\n\n# If 1, nodes with floating IPs will transmit internal cluster traffic via floating IPs; if 0 private IPs will be used instead. Default value is 1.\nuse_access_ip = 1\n\n# Create and use openstack nova servergroups, default: false\nuse_server_groups = true\n\nsubnet_cidr = \"172.16.0.0/24\"\n</code></pre>"},{"location":"operator-manual/openstack/#initialize-and-apply-terraform","title":"Initialize and apply Terraform","text":"<pre><code>MODULE_PATH=\"$(pwd)/kubespray/contrib/terraform/openstack\"\n\nfor CLUSTER in \"${SERVICE_CLUSTER}\" \"${WORKLOAD_CLUSTERS[@]}\"; do\npushd \"${MODULE_PATH}\"\nterraform init\n  terraform apply -var-file=\"${CK8S_CONFIG_PATH}/${CLUSTER}-config/cluster.tfvars\" -state=\"${CK8S_CONFIG_PATH}/${CLUSTER}-config/terraform.tfstate\"\npopd\ndone\n</code></pre> <p>Warning</p> <p>The above will not work well if you are using a bastion host. This is due to some hard coded paths. This is fixed in kubespray release-2.17. If you are using an older version of kubespray, you may link the <code>kubespray/contrib</code> folder to the correct relative path, or make sure your <code>CK8S_CONFIG_PATH</code> is already at a proper place relative to the same.</p>"},{"location":"operator-manual/openstack/#deploying-compliant-kubernetes-using-kubespray","title":"Deploying Compliant Kubernetes using Kubespray","text":"<p>Before we can run Kubespray, we will need to go through the relevant variables. Additionally we will need to expose some credentials so that Kubespray can set up cloud provider integration.</p> <p>You will need to change at least one value: <code>kube_oidc_url</code> in <code>${CK8S_CONFIG_PATH}/${CLUSTER}-config/group_vars/k8s_cluster/ck8s-k8s-cluster.yaml</code>, normally this should be set to <code>https://dex.BASE_DOMAIN</code>.</p> <p>For cloud provider integration, you have a few options as described here. We will be going with the external cloud provider and simply source the Openstack credentials.</p>"},{"location":"operator-manual/openstack/#setting-up-kubespray-variables","title":"Setting up Kubespray variables","text":"<p>Below are some examples for <code>${CK8S_CONFIG_PATH}/${CLUSTER}-config/group_vars/k8s_cluster/ck8s-k8s-cluster-openstack.yaml</code> for a few selected openstack providers. The examples are copy-pastable, but you will have to change some of the values.</p> Citycloud Fra1Citycloud Kna1 <pre><code>etcd_kubeadm_enabled: true\n\ncloud_provider: external\nexternal_cloud_provider: openstack\ncalico_mtu: 1480\n\ncinder_csi_enabled: true\npersistent_volumes_enabled: true\nexpand_persistent_volumes: true\nopenstack_blockstorage_ignore_volume_az: true\n\n## Cinder CSI is enabled by default along with the configuration options to enable persistent volumes and the expansion of these volumes.\n## It is also set to ignore the volume availability zone to allow volumes to attach to nodes in different or mismatching zones. The default works well with both CityCloud and SafeSpring.\nstorage_classes:\n  - name: cinder-csi\n    is_default: true\n    parameters:\n      availability: nova\n      allowVolumeExpansion: true\n      ## openstack volume type list\n      type: default_encrypted\n\n\n## If you want to set up LBaaS in your cluster, you can add the following config:\nexternal_openstack_cloud_controller_extra_args:\n  ## Must be different for every cluster in the same openstack project\n  cluster-name: \"&lt;your-cluster-name&gt;.cluster.local\"\n\n## use `openstack subnet list` to list the available subnets\nexternal_openstack_lbaas_subnet_id: \"your-cluster-subnet-uuid\"\n\n## use `openstack network list` to list the available external networks\nexternal_openstack_lbaas_floating_network_id: \"your-external-network-uuid\"\n\nexternal_openstack_lbaas_method: \"ROUND_ROBIN\"\nexternal_openstack_lbaas_provider: \"octavia\"\nexternal_openstack_lbaas_use_octavia: true\nexternal_openstack_lbaas_create_monitor: true\nexternal_openstack_lbaas_monitor_delay: \"1m\"\nexternal_openstack_lbaas_monitor_timeout: \"30s\"\nexternal_openstack_lbaas_monitor_max_retries: \"3\"\nexternal_openstack_network_public_networks:\n  - \"ext-net\"\n\n## if you have use_access_ip = 0 in cluster.tfvars, you should add the public ip address of the master nodes to this variable\nsupplementary_addresses_in_ssl_keys: [\"master-ip-address1\", \"master-ip-address2\", ...]\n</code></pre> <pre><code>etcd_kubeadm_enabled: true\n\ncloud_provider: external\nexternal_cloud_provider: openstack\ncalico_mtu: 1480\n\ncinder_csi_enabled: true\npersistent_volumes_enabled: true\nexpand_persistent_volumes: true\nopenstack_blockstorage_ignore_volume_az: true\n\nstorage_classes:\n  - name: cinder-csi\n    is_default: true\n    parameters:\n      availability: nova\n      allowVolumeExpansion: true\n      ## openstack volume type list\n      type: ceph_hdd_encrypted\n\nexternal_openstack_cloud_controller_extra_args:\n  ## Must be different for every cluster in the same openstack project\n  cluster-name: \"&lt;your-cluster-name&gt;.cluster.local\"\n\n## use `openstack subnet list` to list the available subnets\nexternal_openstack_lbaas_subnet_id: \"your-cluster-subnet-uuid\"\n\n## use `openstack network list` to list the available external networks\nexternal_openstack_lbaas_floating_network_id: \"your-external-network-uuid\"\n\nexternal_openstack_lbaas_method: \"ROUND_ROBIN\"\nexternal_openstack_lbaas_provider: \"octavia\"\nexternal_openstack_lbaas_use_octavia: true\nexternal_openstack_lbaas_create_monitor: true\nexternal_openstack_lbaas_monitor_delay: \"1m\"\nexternal_openstack_lbaas_monitor_timeout: \"30s\"\nexternal_openstack_lbaas_monitor_max_retries: \"3\"\nexternal_openstack_network_public_networks:\n  - \"ext-net\"\n\n## if you have use_access_ip = 0 in cluster.tfvars, you should add the public ip address of the master nodes to this variable\nsupplementary_addresses_in_ssl_keys: [\"master-ip-address1\", \"master-ip-address2\", ...]\n</code></pre> <p>Note</p> <p>At this point if the cluster is running on Safespring and you are using <code>kubespray v2.17.0+</code> it is possible to create an application credential. Which will give the cluster its own set of credentials instead of using your own.</p> <p>To create a set of credentials use the following command: <code>openstack application credential create &lt;name&gt;</code></p> <p>And set the following environment variables</p> <pre><code>export OS_APPLICATION_CREDENTIAL_NAME: &lt;name&gt;\nexport OS_APPLICATION_CREDENTIAL_ID: &lt;project_id&gt;\nexport OS_APPLICATION_CREDENTIAL_SECRET: &lt;secret&gt;\n</code></pre>"},{"location":"operator-manual/openstack/#run-kubespray","title":"Run Kubespray","text":"<p>Copy the script for generating dynamic ansible inventories:</p> <pre><code>for CLUSTER in \"${SERVICE_CLUSTER}\" \"${WORKLOAD_CLUSTERS[@]}\"; do\ncp kubespray/contrib/terraform/terraform.py \"${CK8S_CONFIG_PATH}/${CLUSTER}-config/inventory.ini\"\nchmod +x \"${CK8S_CONFIG_PATH}/${CLUSTER}-config/inventory.ini\"\ndone\n</code></pre> <p>Now it is time to run the Kubespray playbook!</p> <pre><code>for CLUSTER in \"${SERVICE_CLUSTER}\" \"${WORKLOAD_CLUSTERS[@]}\"; do\n./bin/ck8s-kubespray apply \"${CLUSTER}\" --flush-cache\ndone\n</code></pre>"},{"location":"operator-manual/openstack/#correct-the-kubernetes-api-ip-addresses","title":"Correct the Kubernetes API IP addresses","text":"<p>Locate the encrypted kubeconfigs in <code>${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml</code> and edit them using sops. Copy the public IP address of the load balancer (usually one of the masters public IP address) and replace the private IP address for the server field in <code>${CK8S_CONFIG_PATH}/.state/kube_config_*.yaml</code>. <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml\ndone\n</code></pre></p>"},{"location":"operator-manual/openstack/#test-access-to-the-clusters-as-follows","title":"Test access to the clusters as follows","text":"<p>You should now have an encrypted kubeconfig file for each cluster under <code>$CK8S_CONFIG_PATH/.state</code>. Check that they work like this:</p> <pre><code>for CLUSTER in \"${SERVICE_CLUSTER}\" \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file \"${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml\" \\\n'kubectl --kubeconfig {} get nodes'\ndone\n</code></pre>"},{"location":"operator-manual/openstack/#deploying-compliant-kubernetes-apps","title":"Deploying Compliant Kubernetes Apps","text":"<p>Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps.</p>"},{"location":"operator-manual/openstack/#clone-compliantkubernetes-apps-and-install-pre-requisites","title":"Clone <code>compliantkubernetes-apps</code> and Install Pre-requisites","text":"<p>If you haven't done so already, clone the <code>compliantkubernetes-apps</code> repo and install pre-requisites.</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes-apps.git\ncd compliantkubernetes-apps\nansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127.0.0.1, get-requirements.yaml\n</code></pre>"},{"location":"operator-manual/openstack/#initialize-the-apps-configuration","title":"Initialize the apps configuration","text":"<pre><code>export CK8S_ENVIRONMENT_NAME=my-environment-name\n#export CK8S_FLAVOR=[dev|prod] # defaults to dev\nexport CK8S_CONFIG_PATH=~/.ck8s/my-cluster-path\nexport CK8S_CLOUD_PROVIDER=# [exoscale|safespring|citycloud|aws|baremetal]\nexport CK8S_PGP_FP=&lt;your GPG key fingerprint&gt;  # retrieve with gpg --list-secret-keys\n\n./bin/ck8s init\n</code></pre> <p>This will initialise the configuration in the <code>${CK8S_CONFIG_PATH}</code> directory. Generating configuration files <code>sc-config.yaml</code> and <code>wc-config.yaml</code>, as well as secrets with randomly generated passwords in <code>secrets.yaml</code>. This will also generate read-only default configuration under the directory <code>defaults/</code> which can be used as a guide for available and suggested options.</p> <pre><code>ls -l $CK8S_CONFIG_PATH\n</code></pre>"},{"location":"operator-manual/openstack/#configure-the-apps","title":"Configure the apps","text":"<p>Edit the configuration files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code>, <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/secrets.yaml</code> and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted.</p> <pre><code>vim ${CK8S_CONFIG_PATH}/sc-config.yaml\nvim ${CK8S_CONFIG_PATH}/wc-config.yaml\nsops ${CK8S_CONFIG_PATH}/secrets.yaml\n</code></pre> <p>Tip</p> <p>The default configuration for the service cluster and workload cluster are available in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code> and can be used as a reference for available options.</p> <p>Warning</p> <p>Do not modify the read-only default configurations files found in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code>. Instead configure the cluster by modifying the regular files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> as they will override the default options.</p> <p>The following are the minimum change you should perform:</p> Citycloud Fra1, Kna1Safespring sto1 <pre><code># ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml\nglobal:\nbaseDomain: set-me         # set to $CK8S_ENVIRONMENT_NAME.$DOMAIN\nopsDomain: set-me          # set to ops.$CK8S_ENVIRONMENT_NAME.$DOMAIN\n# issuer: letsencrypt-prod # set as default for prod flavor, defaults to \"letsencrypt-staging\" for dev\n\nobjectStorage:\n# type: s3 # set as default for prod flavor, defaults to \"none\" for dev\ns3:\nregion: set-me         # Kna1 for Karlskrona/Sweden, Fra1 for Frankfurt/Germany\nregionEndpoint: set-me # https://s3-&lt;region&gt;.citycloud.com:8080 # kna1 or fra1\n# forcePathStyle: true # set as default\n\n## This block is set as default for using service load balancers\n# ingressNginx:\n#     controller:\n#       useHostPort: false\n#       service:\n#         enabled: true\n#         type: LoadBalancer\n#         annotations: \"\"\n\nclusterAdmin:\nusers: # set to the cluster admin users\n- set-me\n- admin@example.com\n</code></pre> <pre><code># ${CK8S_CONFIG_PATH}/sc-config.yaml (in addition to the changes above)\nuser:\ngrafana:\noidc:\nallowedDomains: # set to your domain(s), or unset using [] to deny all\n- set-me\n- example.com\n\nharbor:\npersistence:\n# type: swift           # set as default for prod flavor, defaults to \"filesystem\" for dev\n# disableRedirect: true # set as default\nswift:\nauthURL: set-me           # https://&lt;region&gt;.citycloud.com:5000 # kna1 or fra1\nregionName: set-me        # Kna1 for Karlskrona/Sweden, Fra1 for Frankfurt/Germany\nprojectDomainName: set-me\nuserDomainName: set-me\nprojectName: set-me\nprojectID: set-me\ntenantName: set-me\noidc:\ngroupClaimName: set-me # set to group claim name used by OIDC provider\nadminGroupName: set-me # name of the group that automatically will get admin\n\nelasticsearch:\nextraRoleMappings: # set to configure elasticsearch access, or unset using []\n- mapping_name: kibana_user\ndefinition:\nusers:\n- set-me\n- mapping_name: kubernetes_log_reader\ndefinition:\nusers:\n- set-me\n- mapping_name: all_access\ndefinition:\nusers:\n- set-me\n\nalerts:\nopsGenieHeartbeat:\n# enabled: true # set as default for prod flavour, defaults to \"false\" for dev\nname: set-me    # set to name the heartbeat if enabled\n\nissuers:\nletsencrypt:\nprod:\nemail: set-me # set this to an email to receive LetsEncrypt notifications\nstaging:\nemail: set-me # set this to an email to receive LetsEncrypt notifications\n</code></pre> <pre><code># ${CK8S_CONFIG_PATH}/wc-config.yaml (in addition to the changes above)\nuser:\nnamespaces: # set this to create user namespaces, or unset using []\n- set-me\n- production\n- staging\nadminUsers: # set this to create admins in the user namespaces, or unset using []\n- set-me\n- admin@example.com\nadminGroups: # set this to create admin groups in the user namespaces, or unset using []\n- set-me\n# alertmanager: # add this block to enable user accessible alertmanager\n#   enabled: true\n#   namespace: alertmanager # note that this namespace must be listed above under \"user.namespaces\"\n\nopa:\nimageRegistry:\nURL: # set this to the allowed image registry, or unset using [] to deny all\n- set-me\n- harbor.example.com\n</code></pre> <pre><code># ${CK8S_CONFIG_PATH}/secrets.yaml\nobjectStorage:\ns3:\naccessKey: set-me # set to your s3 accesskey\nsecretKey: set-me # set to your s3 secretKey\n</code></pre> <pre><code># ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml\nglobal:\nbaseDomain: set-me         # set to $CK8S_ENVIRONMENT_NAME.$DOMAIN\nopsDomain: set-me          # set to ops.$CK8S_ENVIRONMENT_NAME.$DOMAIN\n# issuer: letsencrypt-prod # set as default for prod flavor, defaults to \"letsencrypt-staging\" for dev\n\nobjectStorage:\n# type: s3 # set as default for prod flavor, defaults to \"none\" for dev\ns3:\nregion: sto2\nregionEndpoint: https://s3.sto2.safedc.net\n# forcePathStyle: true # set as default\n\nclusterAdmin:\nusers: # set to the cluster admin users\n- set-me\n- admin@example.com\n</code></pre> <pre><code># ${CK8S_CONFIG_PATH}/sc-config.yaml (in addition to the changes above)\nuser:\ngrafana:\noidc:\nallowedDomains: # set to your domain(s), or unset using [] to deny all\n- set-me\n- example.com\n\nharbor:\n# persistence:\n#   type: objectStorage   # set as default for prod flavor, defaults to \"filesystem\" for dev\n#   disableRedirect: true # set as default\noidc:\ngroupClaimName: set-me # set to group claim name used by OIDC provider\nadminGroupName: set-me # name of the group that automatically will get admin\n\nelasticsearch:\nextraRoleMappings: # set to configure elasticsearch access, or unset using []\n- mapping_name: kibana_user\ndefinition:\nusers:\n- set-me\n- mapping_name: kubernetes_log_reader\ndefinition:\nusers:\n- set-me\n- mapping_name: all_access\ndefinition:\nusers:\n- set-me\n\nalerts:\nopsGenieHeartbeat:\n# enabled: true # set as default for prod flavour, defaults to \"false\" for dev\nname: set-me    # set to name the heartbeat if enabled\n\nissuers:\nletsencrypt:\nprod:\nemail: set-me # set this to an email to receive LetsEncrypt notifications\nstaging:\nemail: set-me # set this to an email to receive LetsEncrypt notifications\n</code></pre> <pre><code># ${CK8S_CONFIG_PATH}/wc-config.yaml (in addition to the changes above)\nuser:\nnamespaces: # set this to create user namespaces, or unset using []\n- set-me\n- production\n- staging\nadminUsers: # set this to create admins in the user namespaces, or unset using []\n- set-me\n- admin@example.com\nadminGroups: # set this to create admin groups in the user namespaces, or unset using []\n- set-me\n# alertmanager: # add this block to enable user accessible alertmanager\n#   enabled: true\n#   namespace: alertmanager # note that this namespace must be listed above under \"user.namespaces\"\n\nopa:\nimageRegistry:\nURL: # set this to the allowed image registry, or unset using [] to deny all\n- set-me\n- harbor.example.com\n</code></pre> <pre><code># ${CK8S_CONFIG_PATH}/secrets.yaml\nobjectStorage:\ns3:\naccessKey: set-me # set to your s3 accesskey\nsecretKey: set-me # set to your s3 secretKey\n</code></pre>"},{"location":"operator-manual/openstack/#create-s3-buckets","title":"Create S3 buckets","text":"<p>You can use the following script to create required S3 buckets. The script uses s3cmd in the background and gets configuration and credentials for your S3 provider from <code>$CK8S_CONFIG_PATH/.state/s3cfg.ini</code> file.</p> Citycloud Fra1, Kna1 <pre><code># To get your s3 access and secret keys run:\n openstack --os-interface public ec2 credentials list\n# If you don't have any create them with:\n openstack --os-interface public ec2 credentials create\n\n# Use your default s3cmd config file: \"$CK8S_CONFIG_PATH/.state/s3cfg.ini\" that should contain:\naccess_key =\nsecret_key =\nhost_base = s3-&lt;region&gt;.citycloud.com:8080\nhost_bucket = s3-&lt;region&gt;.citycloud.com:8080\nsignurl_use_https = True\nuse_https = True\n\n./scripts/S3/entry.sh --s3cfg \"$CK8S_CONFIG_PATH/.state/s3cfg.ini\" create\n</code></pre>"},{"location":"operator-manual/openstack/#dns","title":"DNS","text":"<p>If are using service loadbalancers on citycloud you must provision it before you can setup the DNS. You can do that by running the following:</p> <pre><code># for the service cluster\nbin/ck8s bootstrap sc\n\nbin/ck8s ops helmfile sc -l app=common-psp-rbac -l app=service-cluster-psp-rbac apply\n\nbin/ck8s ops helmfile sc -l app=kube-prometheus-stack apply\n\nbin/ck8s ops helmfile sc -l app=ingress-nginx apply\n\nbin/ck8s ops kubectl sc get svc -n ingress-nginx\n\n# for the workload clusters\nfor CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n\n    bin/ck8s bootstrap wc\n\n    bin/ck8s ops helmfile wc -l app=common-psp-rbac -l app=workload-cluster-psp-rbac apply\n\n    bin/ck8s ops helmfile wc -l app=kube-prometheus-stack apply\n\n    bin/ck8s ops helmfile wc -l app=ingress-nginx apply\n\n    bin/ck8s ops kubectl wc get svc -n ingress-nginx\n\ndone\n</code></pre> <p>Now that we have the loadbalancer public IPs we can setup the DNS.</p> <ol> <li>If you are using Exoscale as your DNS provider make sure that your have Exoscale cli installed and you can follow this guide for more details.</li> <li>If you are using AWS make sure you have AWS cli installed and follow the below instructions: <pre><code>vim ${CK8S_CONFIG_PATH}/dns.json\n\n# add this lines\n{\n\"Comment\": \"Manage test cluster DNS records\",\n  \"Changes\": [\n{\n\"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n\"Name\": \"*.CK8S_ENVIRONMENT_NAME.DOMAIN\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{ \"Value\": \"&lt;wc_cluster_lb_ip&gt;\"}]\n}\n},\n    {\n\"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n\"Name\": \"*.ops.CK8S_ENVIRONMENT_NAME.DOMAIN\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{ \"Value\": \"&lt;sc_cluster_lb_ip&gt;\"}]\n}\n},\n    {\n\"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n\"Name\": \"grafana.CK8S_ENVIRONMENT_NAME.DOMAIN\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{ \"Value\": \"&lt;sc_cluster_lb_ip&gt;\"}]\n}\n},\n    {\n\"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n\"Name\": \"harbor.CK8S_ENVIRONMENT_NAME.DOMAIN\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{ \"Value\": \"&lt;sc_cluster_lb_ip&gt;\"}]\n}\n},\n    {\n\"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n\"Name\": \"notary.harbor.CK8S_ENVIRONMENT_NAME.DOMAIN\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{ \"Value\": \"&lt;sc_cluster_lb_ip&gt;\"}]\n}\n},\n    {\n\"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n\"Name\": \"kibana.CK8S_ENVIRONMENT_NAME.DOMAIN\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{ \"Value\": \"&lt;sc_cluster_lb_ip&gt;\"}]\n}\n},\n    {\n\"Action\": \"UPSERT\",\n      \"ResourceRecordSet\": {\n\"Name\": \"dex.CK8S_ENVIRONMENT_NAME.DOMAIN\",\n        \"Type\": \"A\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [{ \"Value\": \"&lt;sc_cluster_lb_ip&gt;\"}]\n}\n}\n]\n}\n\n\n# set your profile credentials\nAWS_ACCESS_KEY_ID='my-access-key'\nAWS_SECRET_ACCESS_KEY='my-secret-key'\n\naws --configure default ${AWS_ACCESS_KEY_ID} ${AWS_SECRET_ACCESS_KEY}\naws configure set region &lt;region_name&gt;\n\n# get your hosted zone id\naws route53 list-hosted-zones\n\n# apply the DNS changes\naws route53 change-resource-record-sets --hosted-zone-id &lt;hosted_zone_id&gt; --change-batch file://${CK8S_CONFIG_PATH}/dns.json\n</code></pre></li> </ol>"},{"location":"operator-manual/openstack/#install-compliant-kubernetes-apps","title":"Install Compliant Kubernetes apps","text":"<p>Start with the service cluster:</p> <pre><code>ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml\n./bin/ck8s apply sc  # Respond \"n\" if you get a WARN\n</code></pre> <p>Then the workload clusters:</p> <pre><code>for CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n    ./bin/ck8s apply wc  # Respond \"n\" if you get a WARN\ndone\n</code></pre>"},{"location":"operator-manual/openstack/#settling","title":"Settling","text":"<p>Important</p> <p>Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes.</p> <p>You can check if the system settled as follows:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces pods'\ndone\n</code></pre> <p>Check the output of the command above. All Pods needs to be Running or Completed.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates'\ndone\n</code></pre> <p>Check the output of the command above. All resources need to have the Ready column True.</p>"},{"location":"operator-manual/openstack/#testing","title":"Testing","text":"<p>After completing the installation step you can test if the apps are properly installed and ready using the commands below.</p> <p>Start with the service cluster:</p> <pre><code>ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml\n./bin/ck8s test sc  # Respond \"n\" if you get a WARN\n</code></pre> <p>Then the workload clusters:</p> <pre><code>for CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n    ./bin/ck8s test wc  # Respond \"n\" if you get a WARN\ndone\n</code></pre> <p>Done. Navigate to the endpoints, for example <code>grafana.$BASE_DOMAIN</code>, <code>kibana.$BASE_DOMAIN</code>, <code>harbor.$BASE_DOMAIN</code>, etc. to discover Compliant Kubernetes's features.</p>"},{"location":"operator-manual/openstack/#teardown","title":"Teardown","text":""},{"location":"operator-manual/openstack/#removing-compliant-kubernetes-apps-from-your-cluster","title":"Removing Compliant Kubernetes Apps from your cluster","text":"<p>To remove the applications added by compliant kubernetes you can use the two scripts <code>clean-sc.sh</code> and <code>clean-wc.sh</code>, they are located here in the scripts folder.</p> <p>They perform the following actions:</p> <ol> <li>Delete the added helm charts</li> <li>Delete the added namespaces</li> <li>Delete any remaining PersistentVolumes</li> <li>Delete the added CustomResourceDefinitions</li> </ol> <p>Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.</p>"},{"location":"operator-manual/openstack/#remove-infrastructure","title":"Remove infrastructure","text":"<p>To teardown the infrastructure, please switch to the root directory of the Kubespray repo (see the Terraform section). Make sure you remove all PersistentVolumes and Services with <code>type=LoadBalancer</code>. These objects may create cloud resources that are not managed by Terraform, and therefore would not be removed when we destroy the infrastructure.</p> <pre><code>MODULE_PATH=\"$(pwd)/kubespray/contrib/terraform/openstack\"\n\nfor CLUSTER in \"${SERVICE_CLUSTER}\" \"${WORKLOAD_CLUSTERS[@]}\"; do\npushd \"${MODULE_PATH}\"\nterraform init\n  terraform destroy -var-file=\"${CK8S_CONFIG_PATH}/${CLUSTER}-config/cluster.tfvars\" -state=\"${CK8S_CONFIG_PATH}/${CLUSTER}-config/terraform.tfstate\"\npopd\ndone\n\n# Remove DNS records\n</code></pre> <p>Don't forget to remove any DNS records and object storage buckets that you may have created.</p>"},{"location":"operator-manual/ovh-managed-kubernetes/","title":"Ovh managed kubernetes","text":"<p>This page is out of date</p> <p>We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date.</p> <p>Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly.</p>"},{"location":"operator-manual/ovh-managed-kubernetes/#compliant-kubernetes-deployment-on-ovh-managed-kubernetes","title":"Compliant Kubernetes Deployment on OVH Managed Kubernetes","text":"<p>This document contains instructions on how to setup a service cluster and a workload cluster in OVH. The following are the main tasks addressed in this document:</p> <ol> <li>Setting up Compliant Kubernetes for OVH Managed Kubernetes</li> <li>Deploying Compliant Kubernetes on top of two Kubernetes clusters.</li> </ol> <p>The instructions below are just samples, you need to update them according to your requirements.</p> <p>Before starting, make sure you have all necessary tools.</p> <p>Note</p> <p>This guide is written for compliantkubernetes-apps v0.13.0</p>"},{"location":"operator-manual/ovh-managed-kubernetes/#setup","title":"Setup","text":"<p>Create two Kubernetes clusters in OVH, follow this guide.</p> <p>Sizing hint</p> <p>For the service cluster you can start with creating 3 nodes of size <code>B2-7</code> and add a <code>B2-15</code> node.</p> <p>The workload cluster is fine with 3 <code>B2-7</code> nodes.</p>"},{"location":"operator-manual/ovh-managed-kubernetes/#configure-compliant-kubernetes","title":"Configure Compliant Kubernetes","text":"<p>Start by preparing your shell with some variables that will be used.</p> <pre><code>REGION=\"waw\" # Region for the cluster\nISSUER_MAIL=\"user@example.com\" # Mail that will be used for the LetsEncrypt certificate issuer\n\nexport CK8S_CONFIG_PATH=~/.ck8s/my-ovh-cluster # Path for the configuration\nexport CK8S_ENVIRONMENT_NAME=my-ovh-cluster # Name of the environment\nexport CK8S_PGP_FP=\"FOOBAR1234567\" # Fingerprint of your PGP key, retrieve with gpg --list-secret-keys\nexport CK8S_CLOUD_PROVIDER=baremetal # We don't have a OVH flavor, but baremetal is fine\nexport CK8S_FLAVOR=dev # Change to \"prod\" if it's a production cluster you're setting up\n\nS3_ACCESS_KEY=\"foo\" # Access key for S3, see https://docs.ovh.com/gb/en/public-cloud/getting_started_with_the_swift_S3_API/#create-ec2-credentials\nS3_SECRET_KEY=\"bar\" # Secret key for S3\n</code></pre> <p>Download the kubeconfig and set them up for Compliant Kubernetes by following these steps:</p>"},{"location":"operator-manual/ovh-managed-kubernetes/#create-the-path-where-theyre-going-to-be-stored","title":"Create the path where they're going to be stored","text":"<pre><code>mkdir -p \"${CK8S_CONFIG_PATH}/.state\"\n</code></pre>"},{"location":"operator-manual/ovh-managed-kubernetes/#download-the-kubeconfig-from-ovh-for-the-service-cluster-and-run","title":"Download the kubeconfig from OVH for the service cluster and run:","text":"<pre><code>mv ~/Downloads/kubeconfig.yml \"${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml\"\nsops --encrypt --in-place --pgp \"${CK8S_PGP_FP}\" \"${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml\"\n</code></pre>"},{"location":"operator-manual/ovh-managed-kubernetes/#download-the-kubeconfig-from-ovh-for-the-workload-cluster-and-run","title":"Download the kubeconfig from OVH for the workload cluster and run:","text":"<pre><code>mv ~/Downloads/kubeconfig.yml \"${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml\"\nsops --encrypt --in-place --pgp \"${CK8S_PGP_FP}\" \"${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml\"\n</code></pre>"},{"location":"operator-manual/ovh-managed-kubernetes/#prepare-dns-records","title":"Prepare DNS records","text":"<p>Set up DNS records in OVH.</p> <p>Run this snippet and append it into \"Change in text format\" in your domain in OVH.</p> <pre><code>IP=\"203.0.113.123\"\n\ncat &lt;&lt;EOF | envsubst\n*.ops.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP}\n*.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP}\ngrafana.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP}\nharbor.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP}\nkibana.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP}\ndex.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP}\nnotary.harbor.${CK8S_ENVIRONMENT_NAME} 60 IN A ${IP}\nEOF\n</code></pre> <p>Since we don't know the IP for the loadbalancer yet, you can set them to <code>203.0.113.123</code> (TEST-NET-3).</p> <p>Create required buckets that Compliant Kubernetes will use.</p> <pre><code>cat &lt;&lt;EOF | sops --encrypt --pgp \"${CK8S_PGP_FP}\" --output-type ini --input-type ini /dev/stdin &gt; s3cmd.ini\n[default]\nuse_https   = True\nhost_base   = s3.${REGION}.cloud.ovh.net\nhost_bucket = s3.${REGION}.cloud.ovh.net\naccess_key  = ${S3_ACCESS_KEY}\nsecret_key  = ${S3_SECRET_KEY}\nEOF\n\nsops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-harbor\"\nsops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-velero\"\nsops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-es-backup\"\nsops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-influxdb\"\nsops exec-file --no-fifo s3cfg.ini \"s3cmd --config {} mb s3://${CK8S_ENVIRONMENT_NAME}-sc-logs\"\n</code></pre> <p>Download Compliant Kubernetes and checkout the latest version.</p> <pre><code>git clone git@github.com:elastisys/compliantkubernetes-apps.git\ncd compliantkubernetes-apps\ngit checkout v0.13.0\n</code></pre> <p>Initialize the config.</p> <pre><code>bin/ck8s init\n</code></pre> <p>Update the service cluster configuration for OVH</p> <pre><code>yq write --inplace sc-config.yaml global.baseDomain \"${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\"\nyq write --inplace sc-config.yaml global.opsDomain \"ops.${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\"\nyq write --inplace sc-config.yaml global.issuer \"letsencrypt-prod\"\n\nyq write --inplace sc-config.yaml storageClasses.default \"csi-cinder-high-speed\"\nyq write --inplace sc-config.yaml storageClasses.local.enabled \"false\"\n\nyq write --inplace sc-config.yaml objectStorage.s3.region \"${REGION}\"\nyq write --inplace sc-config.yaml objectStorage.s3.regionEndpoint \"https://s3.${REGION}.cloud.ovh.net/\"\n\nyq write --inplace sc-config.yaml fluentd.forwarder.useRegionEndpoint \"false\"\n\nyq write --inplace sc-config.yaml nfsProvisioner.server \"not-used\"\n\nyq write --inplace sc-config.yaml ingressNginx.controller.useHostPort \"false\"\nyq write --inplace sc-config.yaml ingressNginx.controller.service.enabled \"true\"\nyq write --inplace sc-config.yaml ingressNginx.controller.service.type \"LoadBalancer\"\nyq write --inplace sc-config.yaml ingressNginx.controller.service.annotations \"\"\n\nyq write --inplace sc-config.yaml issuers.letsencrypt.prod.email \"${ISSUER_MAIL}\"\nyq write --inplace sc-config.yaml issuers.letsencrypt.staging.email \"${ISSUER_MAIL}\"\n\nyq write --inplace sc-config.yaml metricsServer.enabled \"false\"\n</code></pre> <p>Update the workload cluster configuration for OVH</p> <pre><code>yq write --inplace wc-config.yaml global.baseDomain \"${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\"\nyq write --inplace wc-config.yaml global.opsDomain \"ops.${CK8S_ENVIRONMENT_NAME}.${DOMAIN}\"\nyq write --inplace wc-config.yaml global.issuer \"letsencrypt-prod\"\n\nyq write --inplace wc-config.yaml storageClasses.default \"csi-cinder-high-speed\"\nyq write --inplace wc-config.yaml storageClasses.local.enabled \"false\"\n\nyq write --inplace wc-config.yaml objectStorage.s3.region \"${REGION}\"\nyq write --inplace wc-config.yaml objectStorage.s3.regionEndpoint \"https://s3.${REGION}.cloud.ovh.net/\"\n\nyq write --inplace wc-config.yaml ingressNginx.controller.useHostPort \"false\"\nyq write --inplace wc-config.yaml ingressNginx.controller.service.enabled \"true\"\nyq write --inplace wc-config.yaml ingressNginx.controller.service.type \"LoadBalancer\"\nyq write --inplace wc-config.yaml ingressNginx.controller.service.annotations \"\"\n\nyq write --inplace wc-config.yaml metricsServer.enabled \"false\"\n</code></pre> <p>Set s3 credentials</p> <pre><code>sops --set '[\"objectStorage\"][\"s3\"][\"accessKey\"] \"'\"${S3_ACCESS_KEY}\"'\"' secrets.yaml\nsops --set '[\"objectStorage\"][\"s3\"][\"secretKey\"] \"'\"${S3_SECRET_KEY}\"'\"' secrets.yaml\n</code></pre>"},{"location":"operator-manual/ovh-managed-kubernetes/#deploy-compliant-kubernetes","title":"Deploy Compliant Kubernetes","text":"<p>Now you're ready to deploy Compliant Kubernetes. When the apply command is done, fetch the external IP assigned to the loadbalancer service and update the DNS record to match this.</p> <pre><code>bin/ck8s apply sc\n\nbin/ck8s ops kubectl sc get svc -n ingress-nginx ingress-nginx-controller\n</code></pre> <p>Run this snippet and update the DNS records you added previously to match the external IP of the service cluster load balancer.</p> <pre><code>IP=\"SERVICE_CLUSTER_LB_IP\"\n\ncat &lt;&lt;EOF | envsubst\n*.ops.${CK8S_ENVIRONMENT_NAME} IN A ${IP}\ngrafana.${CK8S_ENVIRONMENT_NAME} IN A ${IP}\nharbor.${CK8S_ENVIRONMENT_NAME} IN A ${IP}\nkibana.${CK8S_ENVIRONMENT_NAME} IN A ${IP}\ndex.${CK8S_ENVIRONMENT_NAME} IN A ${IP}\nnotary.harbor.${CK8S_ENVIRONMENT_NAME} IN A ${IP}\nEOF\n</code></pre> <p>Next, do the same for the workload cluster.</p> <pre><code>bin/ck8s apply wc\n\nbin/ck8s ops kubectl wc get svc -n ingress-nginx ingress-nginx-controller\n</code></pre> <p>Run this snippet and update the DNS records you added previously to match the external IP of the workload cluster load balancer.</p> <pre><code>IP=\"WORKLOAD_CLUSTER_LB_IP\"\n\ncat &lt;&lt;EOF | envsubst\n*.${CK8S_ENVIRONMENT_NAME} IN A ${IP}\nEOF\n</code></pre>"},{"location":"operator-manual/ovh-managed-kubernetes/#limitations","title":"Limitations","text":"<p>At the time of writing, there's some issues with velero and fluentd.</p> <p>Workarounds for making this work until fix is out</p>"},{"location":"operator-manual/ovh-managed-kubernetes/#add-s3_endpoint-to-fluentd-fixed-with-this-issue","title":"Add <code>s3_endpoint</code> to fluentd, fixed with this issue","text":"<p>To add this, you need to edit the config map fluentd uses. Find the <code>&lt;match **&gt;</code> tag and add <code>s3_endpoint https://s3.REGION.cloud.ovh.net/</code> Where <code>REGION</code> matches what region you're using (<code>waw</code> in the example).</p> <p>After that, delete the fluentd pod to force it to reload the configuration.</p> <pre><code>$ bin/ck8s ops kubectl sc edit cm -n fluentd fluentd-aggregator-configmap\n\n    .\n    .\n    .\n    &lt;match **&gt;\n      @id output-s3\n      @type s3\n\n      aws_key_id \"#{ENV['AWS_ACCESS_KEY_ID']}\"\n      aws_sec_key \"#{ENV['AWS_ACCESS_SECRET_KEY']}\"\n      s3_endpoint https://s3.waw.cloud.ovh.net/ # &lt;--- Add this line\n      s3_region waw\n    .\n    .\n$ bin/ck8s ops kubectl sc delete pod -n fluentd fluentd-0\n</code></pre>"},{"location":"operator-manual/ovh-managed-kubernetes/#update-velero-to-use-130-instead-of-120-fixed-with-this-issue","title":"Update velero to use 1.3.0 instead of 1.2.0, fixed with this issue","text":"<p>For velero to work with OVH S3, velero needs to run with version <code>1.3.0</code>.</p> <pre><code>bin/ck8s ops kubectl sc patch deployment -n velero velero -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}'\nbin/ck8s ops kubectl sc patch daemonset -n velero restic -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}'\n\nbin/ck8s ops kubectl wc patch deployment -n velero velero -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}'\nbin/ck8s ops kubectl wc patch daemonset -n velero restic -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"velero\",\"image\":\"velero/velero:v1.3.0\"}]}}}}'\n</code></pre>"},{"location":"operator-manual/provider-audit/","title":"Cloud Provider Audit","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.15 Supplier Relationships</li> </ul> <p>This page will help you do your due diligence and ensure you choose a cloud provider that provides a solid foundation for Compliant Kubernetes and your application. Elastisys regularly uses this template to validate cloud partners, as required for ISO 27001 certification.</p>","tags":["ISO 27001 A.15 Supplier Relationships","HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor"]},{"location":"operator-manual/provider-audit/#rationale","title":"Rationale","text":"<p>Compliant Kubernetes is designed to build upon the security and compliance of the underlying cloud provider. If you cannot trust the underlying provider with controls such as physical security to the servers, safe disposal of hard drives, access control to infrastructure control plane, then no technical measure will help you achieve your security and compliance goals. Trying to take preventive measures in Compliant Kubernetes -- i.e., at the platform level -- is inefficient at best and downright dangerous at worst. Failing to due your due diligence will end up in security theatre, putting your reputation at risk.</p>","tags":["ISO 27001 A.15 Supplier Relationships","HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor"]},{"location":"operator-manual/provider-audit/#overview","title":"Overview","text":"<p>The remainder of this page contains open questions that you should ask your cloud provider. Notice the following:</p> <ul> <li>Make sure you ask open questions and note down the answers. Burden of proof lies with the provider that they do an excellent job with protecting data.</li> <li>Ask all questions, then evaluate the provider's suitability. It is unlikely that you'll find the perfect provider, but you'll likely find one that is sufficient for your present and future needs.</li> <li>The least expected the answer, the more \"digging\" is needed.</li> <li>\"You\" represents the cloud provider and \"I\" represents the Compliant Kubernetes administrator.</li> </ul>","tags":["ISO 27001 A.15 Supplier Relationships","HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor"]},{"location":"operator-manual/provider-audit/#technical-capability-questionnaire","title":"Technical Capability Questionnaire","text":"<ol> <li>Availability Zones:<ol> <li>Where are your data centers located?</li> <li>How are they presented, i.e., single API vs. multiple independent APIs?</li> </ol> </li> <li>Services:<ol> <li>What services do you offer? (e.g., VMs, object storage)</li> <li>Are all your services available in all zones?</li> </ol> </li> <li>Identity and Access Management (IAM):<ol> <li>Do you offer IAM?</li> <li>How can I create roles? Via an API? Via a UI? Via a Terraform provider?</li> <li>What services can I configure role-based access control for?</li> <li>Can IAM be configured via API? Can IAM be configured via Terraform?</li> <li>Can one single user be given access to multiple projects?</li> </ol> </li> <li> <p>Infrastructure aaS:</p> <ol> <li>Which IaaS engine do you use? (e.g., OpenStack, VMware, proprietary)</li> <li>Do you have a Terraform provider for your API?</li> <li>Do you have pre-uploaded Ubuntu images? Which?<ol> <li>Do these images have AutomaticSecurityUpdates by default?</li> <li>Do these images have NTP enabled by default?</li> </ol> </li> <li>Do you have a Kubernetes integration for your IaaS?<ol> <li>Can I use a cloud-controller for automatic discovery of Nodes and labeling Nodes with the right Zone?</li> </ol> </li> <li>Can you handle large diurnal capacity changes, a.k.a., auto-scaling? E.g., 40 VMs from 6.00 to 10.00, but only 10 VMs from 10.00-6.00.<ol> <li>Can I reserve VMs? How do you bill for reserved but unused VMs?</li> <li>What technical implementation do you recommend? E.g., pause/unpause VMs, stop/start VMs, terminate/recreate VMs.</li> </ol> </li> <li>Do you support anti-affinity?<ol> <li>If not, how can we ensure that VMs don't end up on the same physical servers?</li> </ol> </li> </ol> </li> <li> <p>Storage capabilities:</p> <ol> <li>Do you offer Object Storage as a Service (OSaaS)?<ol> <li>Can I use the object storage via an S3-compatible API?</li> <li>Can I create buckets via API?</li> <li>Can I create bucket credentials via API?</li> <li>Do you have a Terraform provider for your API?</li> <li>In which zones?</li> <li>Do you have immutable storage or object lock?</li> <li>Is OSaaS stretched across zones?</li> <li>Is object storage replicated across zones?</li> </ol> </li> <li>Do you offer Block storage as a Service (BLaaS)?<ol> <li>Which API (OpenStack, VMware)?</li> <li>In which zones?</li> <li>Can I use a Container Storage Interface (CSI) driver for automatic creating of PersistentVolumes?</li> <li>[For NFS] How did you configure User ID Mapping, specifically <code>root_squash</code>, <code>no_root_squash</code>, <code>all_squash</code>, <code>anonuid</code> and <code>anongid</code>? Mapping the root UID to values typically used by containers, e.g., 1000, will lead to permission denied errors. For example, OpenSearch's init containers do <code>chown 1000</code> which fails with <code>squash_root</code> and <code>anonuid=1000</code>.</li> <li>Is BSaaS stretched across zones?</li> <li>Is block storage replicated across zones?</li> </ol> </li> <li>Do you offer encryption-at-rest?<ol> <li>Encrypted object storage: Do you offer this by default?</li> <li>Encrypted block storage: Do you offer this by default?</li> <li>Encrypted boot discs: Do you offer this by default?</li> <li>If not, how do you dispose of media potentially containing personal data (e.g., hard drivers, backup tapes)?</li> </ol> </li> </ol> </li> <li> <p>Networking capabilities:</p> <ol> <li>Can the VMs be set up on a private network? Do you have a Terraform provider for your API?<ol> <li>Is your private network stretched across zones?</li> <li>Do you trust the network between your data centers?</li> <li>Does the private network overlap:<ol> <li>The default Docker network (<code>172.17.0.0/16</code>)?</li> <li>The default Kubernetes Service network (<code>10.233.0.0/18</code>)?</li> <li>The default Kubernetes Pod network (<code>10.233.64.0/18</code>)?</li> </ol> </li> </ol> </li> <li>Firewall aaS<ol> <li>Are Firewall aaS available?</li> <li>What API? (e.g., OpenStack, VMware)</li> <li>Do you have a Terraform provider for your API?</li> </ol> </li> <li>Do you offer Load Balancer aaS (LBaaS)?<ol> <li>Can I create a LB via API?</li> <li>Do you have a Terraform provider for your API?</li> <li>Can I use a cloud-controller for automatic creation of external LoadBalancers?</li> <li>Can I set up a LB across zones? Via API?</li> <li>Can VMs see themselves via the LB's IP? (If not, then VMs need a minor fix.)</li> <li>Do your LBs preserve source IPs? Usually, this involves clever DNAT or PROXY protocol support.</li> </ol> </li> <li>Do you offer IPv6 support? By default?</li> <li>Do you offer DNS as a Service? Which API?</li> </ol> </li> <li> <p>Network security:</p> <ol> <li>Do you allow NTP (UDP port 123) for clock synchronization to the Internet?<ol> <li>If not, do you have a private NTP server?</li> </ol> </li> <li>Do you allow ACME (TCP port 80) for automated certificate provisioning via LetsEncrypt?<ol> <li>If not, how will you provision certificates?</li> </ol> </li> </ol> </li> </ol>","tags":["ISO 27001 A.15 Supplier Relationships","HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor"]},{"location":"operator-manual/provider-audit/#organizational-capabilities","title":"Organizational capabilities","text":"<ol> <li>What regulations are your existing customers subject to? (e.g., GDPR, public sector regulations, some ISO 27001 profile)</li> <li>Can you show us your ISO-27001 certification?<ol> <li>Which profile?</li> <li>Which organization made the audit?</li> <li>Can we get a copy of the Statement of Applicability (SoA)?</li> </ol> </li> <li>Who is overall responsible with compliance in your organization?</li> <li>How do you implement regulatory and contractual requirements?</li> <li>How is a new requirement discovered?<ol> <li>What is the journey that a requirement takes from discovery, to updating policies, to training employees, to implementation, to evidence of implementation?</li> </ol> </li> <li>How is physical security handled?</li> <li>How do you handle incidents and deviations?<ol> <li>What response times / time to resolution do you offer?</li> <li>What are your actual response times / time to resolution?</li> </ol> </li> <li>What is your change management process?</li> <li>How do you handle technical vulnerabilities?</li> <li>How do you handle capacity management?</li> <li>In case of a breach, how long until you notify your customers?</li> <li>What SLA do you offer?<ol> <li>What uptime do you offer?</li> <li>What is your measured uptime?</li> <li>Do you have a public status page?</li> </ol> </li> <li>How do you handle access control?</li> <li>Does your operation team have individual accounts? How do you handle team member onboarding / offboarding?</li> <li>How do you communicate credentials to your customers?</li> <li>Do you have audit logs?<ol> <li>How long do you store audit logs? Who has access to them? How are they protected against disclosure and tampering?</li> </ol> </li> <li>How do you handle business continuity?<ol> <li>How often do you test fail-over? How did the last test go?</li> </ol> </li> <li>How do you handle disaster recovery?<ol> <li>How often do you test disaster recovery? How did the last test go?</li> </ol> </li> </ol>","tags":["ISO 27001 A.15 Supplier Relationships","HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor"]},{"location":"operator-manual/provider-audit/#legal-issues","title":"Legal issues","text":"<ol> <li>Do you fully operate under EU jurisdiction?</li> <li>Is your ownership fully under EU jurisdiction?</li> <li>Are your suppliers fully under EU jurisdiction?<ol> <li>Even the web fonts and analytics code on your front-page?</li> </ol> </li> <li>Do you have a DPO?<ol> <li>Is this an internal employee or outsourced?</li> </ol> </li> <li>Can you show us your Data Processing Agreement (DPA)?</li> <li>[HIPAA only] Are you familiar with Business Associate Agreements?<ol> <li>Are you ready to sign one with us?</li> </ol> </li> </ol>","tags":["ISO 27001 A.15 Supplier Relationships","HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor"]},{"location":"operator-manual/provider-audit/#collaboration","title":"Collaboration","text":"<ol> <li>How can we collaborate with your on-call team?<ol> <li>What collaboration channels do you offer? (e.g., Slack, Teams, phone, service desk)</li> <li>What response times can we expect?</li> <li>Is your on-call team available 24/7?</li> </ol> </li> <li>Are you open to having quarterly operations (engineering) retrospectives? Our engineering team wants to keep a close loop with vendors and regularly discuss what went well, what can be improved, and devise a concrete action plan.</li> <li>Are you open to having quarterly roadmap discussions?</li> </ol>","tags":["ISO 27001 A.15 Supplier Relationships","HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor"]},{"location":"operator-manual/provider-audit/#environment-management","title":"Environment Management","text":"<ol> <li>What environmental policies and certifications do you have?</li> <li>What energy sources are your datacenters using?</li> <li>How do you work to become more energy efficient?</li> <li>How do you recycle used/old equipment?</li> <li>Do you do any form of environmental compensation activities?</li> </ol>","tags":["ISO 27001 A.15 Supplier Relationships","HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor"]},{"location":"operator-manual/provider-audit/#evidence","title":"Evidence","text":"<p>The audit should conclude with gathering the following documents in an \"evidence package\":</p> <ol> <li>Filled questionnaire</li> <li>All relevant certificates, e.g., ISO 14001, ISO 27001, \u201cgreen cloud\u201d</li> <li>Latest version of the Terms of Service and Data Protection Agreement</li> <li>All relevant certificates from data-centre providers</li> <li>Signed and transparent ownership structure</li> </ol>","tags":["ISO 27001 A.15 Supplier Relationships","HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor"]},{"location":"operator-manual/qa/","title":"Quality Assurance","text":"<p>Compliant Kubernetes provides a stable and secure platform for containerized applications. To achieve this, quality assurance is an integral part of development. When we say \"quality\", we really refer to the following quality criteria.</p> <p>Feature can be delivered ...</p> <ul> <li> <p>... at scale</p> <ul> <li>Feature has good user-facing documentation</li> <li>Feature is self-serviced</li> <li>Feature is well-understood and aligned in marketing, sales, product and operations</li> <li>Feature is clearly covered by ToS</li> <li>Feature is implemented using a stable upstream API</li> <li>Feature is used by at least 2 customers</li> <li>Feature generates a manageable number of service tickets, whether questions or change orders</li> <li>Feature has well understood packaging and pricing</li> <li>Feature can be billed easily</li> </ul> </li> <li> <p>... without ruining admin's life</p> <ul> <li>At least 2 admins have required training</li> <li>Feature has good admin-facing documentation (2nd day ops, all processes in place and documented, etc.)</li> <li>Feature triggers a manageable number of P1 alerts</li> <li>Feature triggers a manageable number of P2 alerts</li> <li>Feature has good upstream support</li> <li>All information security risks related to feature have been identified</li> <li>(In case of a new supplier) Supplier collaborates directly with Elastisys admins</li> <li>Feature is covered by QA</li> <li>Feature is sufficiently redundant to be able to operate in degraded state upon faults</li> <li>Feature integrates well with Ops observability (alerting, logging, metrics)</li> </ul> </li> <li> <p>... without compromising our security posture</p> <ul> <li>Feature has good and well-understood access control towards customer</li> <li>Feature does not expose platform to additional risk (needs escalated privilegies that were not analyzed, etc.)</li> <li>Feature has good and well-understood security patching</li> <li>Feature has good and well-understood upgrades</li> <li>Feature has good and well-understood business continuity, i.e., high availability or self-healing</li> <li>Feature has good and well-understood disaster recovery</li> <li>Feature does not impair ability to upgrade underlying infrastructure and base OS</li> <li>(In case of a new supplier) Supplier provides sufficient security for our needs</li> <li>Feature has good and well understood way of measuring SLA fulfillment</li> </ul> </li> </ul> <p>These criteria should be taken as a direction, not a \"task list\". For some features, some of these criteria won't apply. For other features, we might accept that some of these criteria cannot be fully satisfied. It is the role of our QA manager to decide how to apply these criteria to each feature.</p>","tags":["ISO 27001 A.14.2.9 System Acceptance Testing"]},{"location":"operator-manual/qa/#how-to-perform-quality-assurance","title":"How to perform quality assurance?","text":"<p>When you have created your Compliant Kubernetes cluster it can be wise to run some checks to ensure that it works as expected. This document details some snippets that you can follow in order to ensure some functionality of the cluster.</p>","tags":["ISO 27001 A.14.2.9 System Acceptance Testing"]},{"location":"operator-manual/qa/#customer-api-and-harbor-access","title":"Customer API and Harbor access","text":"","tags":["ISO 27001 A.14.2.9 System Acceptance Testing"]},{"location":"operator-manual/qa/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>You've got Docker installed.</li> <li>You've exported <code>CK8S_CONFIG_PATH</code> in your shell.</li> <li>You've set <code>baseDomain</code> in your shell to what's used in your cluster.</li> <li>Your current working directory is the <code>compliantkubernets-apps</code> repository.</li> <li>You've installed the <code>kubectl</code> plugin <code>kubelogin</code>.     See instructions on how to install it.</li> </ul>","tags":["ISO 27001 A.14.2.9 System Acceptance Testing"]},{"location":"operator-manual/qa/#create-and-set-user-kubeconfig","title":"Create and set user kubeconfig","text":"<p><pre><code>./bin/ck8s user-kubeconfig\nsops -d -i --config ${CK8S_CONFIG_PATH}/.sops.yaml ${CK8S_CONFIG_PATH}/user/kubeconfig.yaml\nexport KUBECONFIG=${CK8S_CONFIG_PATH}/user/kubeconfig.yaml\n</code></pre> Authenticate by issuing any <code>kubectl</code> command, e.g. <code>kubectl get pods</code> Your browser will be opened and you'll be asked to login through Dex.</p>","tags":["ISO 27001 A.14.2.9 System Acceptance Testing"]},{"location":"operator-manual/qa/#login-to-harbor-gui-and-create-test-project","title":"Login to Harbor GUI and create 'test' project","text":"<ul> <li>Go to <code>https://harbor.${baseDomain}</code>, and login though OIDC.</li> <li>Create project 'test'.</li> <li>Click on your user in the top right corner and select User profile.</li> <li>Copy CLI secret.</li> </ul>","tags":["ISO 27001 A.14.2.9 System Acceptance Testing"]},{"location":"operator-manual/qa/#push-image-to-harbor-and-scan-it","title":"Push image to Harbor and scan it","text":"<ul> <li>Pull Nginx from dockerhub <code>docker pull nginx</code>.</li> <li>Login to the Harbor registry <code>docker login https://harbor.${baseDomain}</code>     Enter your Harbor username and the copied CLI secret.</li> <li>Prepare Nginx image for pushing to Harbor registry <code>docker tag nginx harbor.${baseDomain}/test/nginx</code></li> <li>Push image to Harbor <code>docker push harbor.${baseDomain}/test/nginx</code></li> <li>Enter 'test' project in the Harbor GUI, select the newly pushed image and scan it.</li> </ul>","tags":["ISO 27001 A.14.2.9 System Acceptance Testing"]},{"location":"operator-manual/qa/#create-secret-for-pulling-images-from-harbor","title":"Create secret for pulling images from harbor","text":"<pre><code>kubectl create secret generic regcred \\\n    --from-file=.dockerconfigjson=${HOME}/.docker/config.json \\\n    --type=kubernetes.io/dockerconfigjson\n\nkubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"regcred\"}]}'\n</code></pre>","tags":["ISO 27001 A.14.2.9 System Acceptance Testing"]},{"location":"operator-manual/qa/#test-pulling-from-harbor-and-start-privileged-and-unprivileged-pods","title":"Test pulling from Harbor and start privileged and unprivileged pods","text":"<pre><code>kubectl run --image nginxinc/nginx-unprivileged nginx-unprivileged\nkubectl run --image harbor.${baseDomain}/test/nginx nginx-privileged\n\n# You should see that both pods and that nginx-unprivileged eventually becomes running while nginx-privileged does not.\nkubectl get pods\n\n# Check events from the nginx-privileged.\nkubectl describe pod nginx-privileged\n# You should see 'Error: container has runAsNonRoot and image will run as root'.\n</code></pre>","tags":["ISO 27001 A.14.2.9 System Acceptance Testing"]},{"location":"operator-manual/qa/#cleanup-of-created-kubernetes-resources","title":"Cleanup of created Kubernetes resources","text":"<pre><code>kubectl delete pod --all\nkubectl delete secret regcred\n</code></pre>","tags":["ISO 27001 A.14.2.9 System Acceptance Testing"]},{"location":"operator-manual/safespring/","title":"Safespring","text":"<p>This page is out of date</p> <p>We are currently working on internal documentation to streamline Compliant Kubernetes onboarding for selected cloud providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date.</p> <p>Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly.</p>"},{"location":"operator-manual/safespring/#compliant-kubernetes-deployment-on-safespring","title":"Compliant Kubernetes Deployment on Safespring","text":"<p>This document contains instructions on how to set up a Compliant Kubernetes environment (consisting of a service cluster and one or more workload clusters) on Safespring.</p> <p>Note</p> <p>This guide is written for compliantkubernetes-apps v0.13.0</p> <p>TODO: The document is split into two parts:</p> <ul> <li> <p>Cluster setup (setting up infrastructure and the Kubernetes clusters).   We will be using the Terraform module for Openstack that can be found in the Kubespray repository.   Please refer to it if you need more details about this part of the setup.</p> </li> <li> <p>Apps setup (including information about limitations)</p> </li> </ul> <p>Before starting, make sure you have all necessary tools. In addition to these general tools, you will also need:</p> <ul> <li> <p>Openstack credentials (either using <code>openrc</code> or the <code>clouds.yaml</code> configuration file) for setting up the infrastructure.</p> </li> <li> <p>For Safespring, you can get these by logging into the Safespring Openstack dashboard and download either the <code>clouds.yaml</code> or <code>OpenRC</code> file from the API Access page.</p> </li> </ul> <p>Note</p> <p>Although recommended OpenStack authentication method is <code>clouds.yaml</code> it is more convenient to use the <code>openrc</code> method with compliant kubernetes as it works both with kubespray and terraform. If you are using the <code>clouds.yaml</code> method, at the moment, kubespray will still expect you to set a few environment variables.</p>"},{"location":"operator-manual/safespring/#initialize-configuration-folder","title":"Initialize configuration folder","text":"<p>Choose names for your service cluster and workload cluster(s):</p> <pre><code>SERVICE_CLUSTER=\"testsc\"\nWORKLOAD_CLUSTERS=( \"testwc0\" \"testwc1\" )\n</code></pre> <p>Start by initializing a Compliant Kubernetes environment using Compliant Kubernetes Kubespray. All of this is done from the root of the <code>compliantkubernetes-kubespray</code> repository.</p> <pre><code>export CK8S_CONFIG_PATH=~/.ck8s/&lt;environment-name&gt;\nexport SOPS_FP=&lt;PGP-fingerprint&gt;\n\nfor CLUSTER in $SERVICE_CLUSTER \"${WORKLOAD_CLUSTERS[@]}\"; do\n./bin/ck8s-kubespray init \"${CLUSTER}\" openstack \"${SOPS_FP}\"\ndone\n</code></pre>"},{"location":"operator-manual/safespring/#infrastructure-setup-using-terraform","title":"Infrastructure setup using Terraform","text":"<p>Configure Terraform by creating a <code>cluster.tfvars</code> file for each cluster. The available options can be seen in <code>kubespray/contrib/terraform/openstack/variables.tf</code>. There is a sample file that can be copied to get something to start from.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} ${WORKLOAD_CLUSTERS[@]}; do\ncp kubespray/contrib/terraform/openstack/sample-inventory/cluster.tfvars \"${CK8S_CONFIG_PATH}/${CLUSTER}-config/cluster.tfvars\"\ndone\n</code></pre> <p>Note</p> <p>You really must edit the values in these files. There is no way to set sane defaults for what flavor to use, what availability zones or networks are available across providers. In the section below some guidance and samples are provided but remember that they might be useless to you depending on your needs and setup.</p>"},{"location":"operator-manual/safespring/#infrastructure-guidance","title":"Infrastructure guidance","text":"<p>We recommend you to have at least three worker nodes with 4 cores and 8 GB memory each, and we recommend you to have at least 2 cores and 4 GB for your control plane nodes.</p> <p>Below is example <code>cluster.tfvars</code> for a few select openstack providers. The examples are copy-pastable, but you might want to change <code>cluster_name</code> and <code>network_name</code> (if neutron is used!).</p> Safespring sto1 with public IPs assigned to VMs (no floating IP)Safespring sto1 with private IPs and floating IPs assigned to VMs <pre><code># your Kubernetes cluster name here\ncluster_name = \"your-cluster-name\"\n\n# image to use for bastion, masters, standalone etcd instances, and nodes\nimage = \"ubuntu-20.04\"\n\n# 0|1 bastion nodes\nnumber_of_bastions = 0\n\nuse_neutron = 0\n\n# standalone etcds\nnumber_of_etcd = 0\n\n# masters\nnumber_of_k8s_masters = 0\nnumber_of_k8s_masters_no_etcd = 0\nnumber_of_k8s_masters_no_floating_ip = 1\nnumber_of_k8s_masters_no_floating_ip_no_etcd = 0\nflavor_k8s_master = \"8a707999-0bce-4f2f-8243-b4253ba7c473\"\n\n# nodes\nnumber_of_k8s_nodes = 0\nnumber_of_k8s_nodes_no_floating_ip = 3\nflavor_k8s_node = \"5b40af67-9d11-45ed-a44f-e876766160a5\"\n\n# networking\n# ssh access to nodes\nk8s_allowed_remote_ips = [\"0.0.0.0/0\"]\nworker_allowed_ports = [\n  { # Node ports\n    \"protocol\"         = \"tcp\"\n    \"port_range_min\"   = 30000\n    \"port_range_max\"   = 32767\n    \"remote_ip_prefix\" = \"0.0.0.0/0\"\n  },\n  { # HTTP\n    \"protocol\"         = \"tcp\"\n    \"port_range_min\"   = 80\n    \"port_range_max\"   = 80\n    \"remote_ip_prefix\" = \"0.0.0.0/0\"\n  },\n  { # HTTPS\n    \"protocol\"         = \"tcp\"\n    \"port_range_min\"   = 443\n    \"port_range_max\"   = 443\n    \"remote_ip_prefix\" = \"0.0.0.0/0\"\n  }\n]\nexternal_net = \"b19680b3-c00e-40f0-ad77-4448e81ae226\"\nuse_access_ip = 1\nnetwork_name = \"public\"\n\nuse_server_groups = true # Comment this out if you don't mind that the VMs are schedules on different physical machines\n                         # Also, comment this out if you run into problems on Safespring because of shortage of physical machines\n</code></pre> <p>For this to work, make sure that you can create networks and floating IPs on Safespring.</p> <pre><code>    ``` hcl\n    # your Kubernetes cluster name here\n    cluster_name = \"your-cluster-name\"\n\n    # image to use for bastion, masters, standalone etcd instances, and nodes\n    image = \"ubuntu-20.04\"\n\n    # 0|1 bastion nodes\n    number_of_bastions = 0\n\n    use_neutron = 0\n\n    # standalone etcds\n    number_of_etcd = 0\n\n    # masters\n    number_of_k8s_masters = 1\n    number_of_k8s_masters_no_etcd = 0\n    number_of_k8s_masters_no_floating_ip = 0\n    number_of_k8s_masters_no_floating_ip_no_etcd = 0\n    flavor_k8s_master = \"8a707999-0bce-4f2f-8243-b4253ba7c473\"\n\n    # nodes\n    number_of_k8s_nodes = 3\n    number_of_k8s_nodes_no_floating_ip = 0\n    flavor_k8s_node = \"5b40af67-9d11-45ed-a44f-e876766160a5\"\n\n    # networking\n    # ssh access to nodes\n    k8s_allowed_remote_ips = [\"0.0.0.0/0\"]\n    worker_allowed_ports = [\n      { # Node ports\n        \"protocol\"         = \"tcp\"\n        \"port_range_min\"   = 30000\n        \"port_range_max\"   = 32767\n        \"remote_ip_prefix\" = \"0.0.0.0/0\"\n      },\n      { # HTTP\n        \"protocol\"         = \"tcp\"\n        \"port_range_min\"   = 80\n        \"port_range_max\"   = 80\n        \"remote_ip_prefix\" = \"0.0.0.0/0\"\n      },\n      { # HTTPS\n        \"protocol\"         = \"tcp\"\n        \"port_range_min\"   = 443\n        \"port_range_max\"   = 443\n        \"remote_ip_prefix\" = \"0.0.0.0/0\"\n      }\n    ]\n    external_net = \"b19680b3-c00e-40f0-ad77-4448e81ae226\"\n    use_access_ip = 1\n    network_name = \"private\" # Change this if you would like a new network to be created\n\n    use_server_groups = true # Comment this out if you don't mind that the VMs are schedules on different physical machines\n                             # Also, comment this out if you run into problems on Safespring because of shortage of physical machines\n    ```\n</code></pre>"},{"location":"operator-manual/safespring/#expose-openstack-credentials-to-terraform","title":"Expose Openstack credentials to Terraform","text":"<p>Terraform will need access to Openstack credentials in order to create the infrastructure. More details can be found here.</p> <p>We will be using the declarative option with the <code>clouds.yaml</code> file. Since this file can contain credentials for multiple environments, we specify the name of the one we want to use in the environment variable <code>OS_CLOUD</code>:</p> <pre><code>export OS_CLOUD=&lt;name-of-openstack-cloud-environment&gt;\n</code></pre> <p>If you use the <code>clouds.yaml</code> file copy it in the <code>compliantkubernetes-kubespray/kubespray/contrib/terraform/openstack/</code> and edit the file to add your password <code>password: your-safespring-openstack-password</code> in the <code>auth:</code> section.</p> <p>Alternatively, using the <code>openrc</code> file:</p> <p><code>source /path/to/your/openrc/file</code></p>"},{"location":"operator-manual/safespring/#initialize-and-apply-terraform","title":"Initialize and apply Terraform","text":"<pre><code>MODULE_PATH=\"$(pwd)/kubespray/contrib/terraform/openstack\"\n\npushd \"${MODULE_PATH}\"\nfor CLUSTER in $SERVICE_CLUSTER \"${WORKLOAD_CLUSTERS[@]}\"; do\nterraform init\n  terraform apply -var-file=\"${CK8S_CONFIG_PATH}/${CLUSTER}-config/cluster.tfvars\" -state=\"${CK8S_CONFIG_PATH}/${CLUSTER}-config/terraform.tfstate\"\ndone\npopd\n</code></pre> <p>Warning</p> <p>The above will not work well if you are using a bastion host. This is due to some hard coded paths. To work around it, you may link the <code>kubespray/contrib</code> folder to the correct relative path, or make sure your <code>CK8S_CONFIG_PATH</code> is already at a proper place relative to the same.</p>"},{"location":"operator-manual/safespring/#install-kubernetes-using-kubespray","title":"Install Kubernetes using Kubespray","text":"<p>Before we can run Kubespray, we will need to go through the relevant variables. Additionally we will need to expose some credentials so that Kubespray can set up cloud provider integration.</p> <p>You will need to change at least one value: <code>kube_oidc_url</code> in <code>group_vars/k8s_cluster/ck8s-k8s_cluster.yaml</code>, normally this should be set to <code>https://dex.BASE_DOMAIN</code>.</p> <p>Note</p> <p>If you have <code>use_access_ip = 0</code> in <code>cluster.tfvars</code>, you should add the public ip address of the master nodes to the variable <code>supplementary_addresses_in_ssl_keys = [\"&lt;master-0-ip-address&gt;\",...]</code> somewhere under <code>group_vars/</code>.</p> <p>For cloud provider integration, you have a few options as described here. We will be going with the external cloud provider and simply source the Openstack credentials. See below for how to modify the variables that need to be modified.</p>"},{"location":"operator-manual/safespring/#setting-up-kubespray-variables","title":"Setting up Kubespray variables","text":"<p>In <code>${CK8S_CONFIG_PATH}/$CLUSTER-config/group_vars/k8s_cluster/ck8s-k8s-cluster-openstack.yaml</code>, the default variables should look like this:</p> <pre><code>etcd_kubeadm_enabled: true\n\ncloud_provider: external\nexternal_cloud_provider: openstack\ncalico_mtu: 1480\n\nexternal_openstack_cloud_controller_extra_args:\n# Must be different for every cluster in the same openstack project\ncluster-name: \"set-me\"\n\ncinder_csi_enabled: true\npersistent_volumes_enabled: true\nexpand_persistent_volumes: true\nopenstack_blockstorage_ignore_volume_az: true\n\nstorage_classes:\n- name: cinder-csi\nis_default: true\nparameters:\nallowVolumeExpansion: true\navailability: nova\n</code></pre> <p><code>cluster-name</code> should be set to a name that is unique in the Openstack project you're deploying your clusters in. If you don't have any other clusters in the project, just make sure that the service cluster and workload clusters have different names.</p> <p>Cinder CSI is enabled by default along with the configuration options to enable persistent volumes and the expansion of these volumes. It is also set to ignore the volume availability zone to allow volumes to attach to nodes in different or mismatching zones. The default works well with both CityCloud and SafeSpring.</p> <p>If you want to set up LBaaS in your cluster, you can add the following config:</p> <pre><code>external_openstack_lbaas_create_monitor: false\nexternal_openstack_lbaas_monitor_delay: \"1m\"\nexternal_openstack_lbaas_monitor_timeout: \"30s\"\nexternal_openstack_lbaas_monitor_max_retries: \"3\"\nexternal_openstack_lbaas_provider: octavia\nexternal_openstack_lbaas_use_octavia: true\n# external_openstack_lbaas_network_id: \"Neutron network ID to create LBaaS VIP\"\nexternal_openstack_lbaas_subnet_id: \"Neutron subnet ID to create LBaaS VIP\"\nexternal_openstack_lbaas_floating_network_id: \"Neutron network ID to get floating IP from\"\n# external_openstack_lbaas_floating_subnet_id: \"Neutron subnet ID to get floating IP from\"\nexternal_openstack_lbaas_method: \"ROUND_ROBIN\"\nexternal_openstack_lbaas_manage_security_groups: false\nexternal_openstack_lbaas_internal_lb: false\n</code></pre> <p>The <code>network_id</code> and <code>subnet_id</code> variables need to be set by you, depending on whether or not you used floating IP. <code>network_id</code> should match the <code>external_net</code> variable in your Terraform variables, whereas the <code>subnet_id</code> should match the subnet ID that Terraform outputs after it is applied.</p> <p>Additionally, when you later set up <code>compliantkubernetes-apps</code> in your cluster, you should set <code>ingressNginx.controller.service.enabled</code> to <code>true</code> and <code>ingressNginx.controller.service.type</code> to <code>LoadBalancer</code> in both your <code>sc-config.yaml</code> and <code>wc-config.yaml</code>. Use the IP of the <code>ingress-nginx-controller</code> service in your cluster when you set up your DNS.</p> <p>Note</p> <p>At this point if the cluster is running on Safespring and you are using <code>kubespray v2.17.0+</code> it is possible to create an application credential. Which will give the cluster its own set of credentials instead of using your own.</p> <p>To create a set of credentials use the following command: <code>openstack application credential create &lt;name&gt;</code></p> <p>And set the following environment variables</p> <pre><code>export OS_APPLICATION_CREDENTIAL_NAME: &lt;name&gt;\nexport OS_APPLICATION_CREDENTIAL_ID: &lt;project_id&gt;\nexport OS_APPLICATION_CREDENTIAL_SECRET: &lt;secret&gt;\n</code></pre>"},{"location":"operator-manual/safespring/#run-kubespray","title":"Run Kubespray","text":"<p>Copy the script for generating dynamic ansible inventories:</p> <pre><code>for CLUSTER in $SERVICE_CLUSTER \"${WORKLOAD_CLUSTERS[@]}\"; do\ncp kubespray/contrib/terraform/terraform.py \"${CK8S_CONFIG_PATH}/${CLUSTER}-config/inventory.ini\"\nchmod +x \"${CK8S_CONFIG_PATH}/${CLUSTER}-config/inventory.ini\"\ndone\n</code></pre> <p>Now it is time to run the Kubespray playbook!</p> <pre><code>for CLUSTER in $SERVICE_CLUSTER \"${WORKLOAD_CLUSTERS[@]}\"; do\n./bin/ck8s-kubespray apply \"${CLUSTER}\"\ndone\n</code></pre>"},{"location":"operator-manual/safespring/#test-access-to-the-kubernetes-api","title":"Test access to the Kubernetes API","text":"<p>You should now have an encrypted kubeconfig file for each cluster under <code>$CK8S_CONFIG_PATH/.state</code>. Check that they work like this:</p> <pre><code>for CLUSTER in $SERVICE_CLUSTER \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file \"${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml\" \"kubectl --kubeconfig {} cluster-info\"\ndone\n</code></pre> <p>The output should be similar to this.</p> <pre><code>Kubernetes control plane is running at https://&lt;public-ip&gt;:6443\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\nKubernetes control plane is running at https://&lt;public-ip&gt;:6443\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre>"},{"location":"operator-manual/safespring/#create-the-dns-records","title":"Create the DNS Records","text":"<p>You will need to setup a number of DNS entries for traffic to be routed correctly. Determine the public IP of one or more of the nodes in the service cluster. Then point these domains to the services.</p> <p>Since Safespring does not have a domain name service, use alternatives such as AWS Route53.</p>"},{"location":"operator-manual/safespring/#deploying-compliant-kubernetes-apps","title":"Deploying Compliant Kubernetes Apps","text":"<p>Now that the Kubernetes clusters are up and running, we are ready to install the Compliant Kubernetes apps.</p>"},{"location":"operator-manual/safespring/#clone-compliantkubernetes-apps-and-install-pre-requisites","title":"Clone <code>compliantkubernetes-apps</code> and Install Pre-requisites","text":"<p>If you haven't done so already, clone the <code>compliantkubernetes-apps</code> repo and install pre-requisites.</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes-apps.git\ncd compliantkubernetes-apps\nansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127.0.0.1, get-requirements.yaml\n</code></pre>"},{"location":"operator-manual/safespring/#initialize-the-apps-configuration","title":"Initialize the apps configuration","text":"<pre><code>export CK8S_ENVIRONMENT_NAME=my-environment-name\n#export CK8S_FLAVOR=[dev|prod] # defaults to dev\nexport CK8S_CONFIG_PATH=~/.ck8s/my-cluster-path\nexport CK8S_CLOUD_PROVIDER=# [exoscale|safespring|citycloud|aws|baremetal]\nexport CK8S_PGP_FP=&lt;your GPG key fingerprint&gt;  # retrieve with gpg --list-secret-keys\n\n./bin/ck8s init\n</code></pre> <p>This will initialise the configuration in the <code>${CK8S_CONFIG_PATH}</code> directory. Generating configuration files <code>sc-config.yaml</code> and <code>wc-config.yaml</code>, as well as secrets with randomly generated passwords in <code>secrets.yaml</code>. This will also generate read-only default configuration under the directory <code>defaults/</code> which can be used as a guide for available and suggested options.</p> <pre><code>ls -l $CK8S_CONFIG_PATH\n</code></pre>"},{"location":"operator-manual/safespring/#configure-the-apps","title":"Configure the apps","text":"<p>Edit the configuration files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code>, <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/secrets.yaml</code> and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted.</p> <pre><code>vim ${CK8S_CONFIG_PATH}/sc-config.yaml\nvim ${CK8S_CONFIG_PATH}/wc-config.yaml\nsops ${CK8S_CONFIG_PATH}/secrets.yaml\n</code></pre> <p>Tip</p> <p>The default configuration for the service cluster and workload cluster are available in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code> and can be used as a reference for available options.</p> <p>Warning</p> <p>Do not modify the read-only default configurations files found in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code>. Instead configure the cluster by modifying the regular files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> as they will override the default options.</p> <p>The following are the minimum change you should perform:</p> <pre><code># ${CK8S_CONFIG_PATH}/sc-config.yaml and ${CK8S_CONFIG_PATH}/wc-config.yaml\nglobal:\nbaseDomain: \"set-me\"  # set to $CK8S_ENVIRONMENT_NAME.$DOMAIN\nopsDomain: \"set-me\"  # set to ops.$CK8S_ENVIRONMENT_NAME.$DOMAIN\nissuer: letsencrypt-prod\n\nobjectStorage:\ntype: \"s3\"\ns3:\nregion: \"set-me\"  # Region for S3 buckets, e.g, west-1\nregionEndpoint: \"set-me\"  # e.g., https://s3.us-west-1.amazonaws.com\n\nstorageClasses:\ndefault:  cinder-csi\nnfs:\nenabled: false\ncinder:\nenabled: true\nlocal:\nenabled: false\nebs:\nenabled: false\n</code></pre> <pre><code># ${CK8S_CONFIG_PATH}/sc-config.yaml (in addition to the changes above)\ningressNginx:\ncontroller:\nservice:\ntype: \"this-is-not-used\"\nannotations: \"this-is-not-used\"\n\nharbor:\noidc:\ngroupClaimName: \"set-me\" # set to group claim name used by OIDC provider\n\nissuers:\nletsencrypt:\nprod:\nemail: \"set-me\"  # set this to an email to receive LetsEncrypt notifications\nstaging:\nemail: \"set-me\"  # set this to an email to receive LetsEncrypt notifications\n</code></pre> <p>Edit ${CK8S_CONFIG_PATH}/secrets.yaml with sops</p> <p><code>sops ${CK8S_CONFIG_PATH}/secrets.yaml</code></p> <pre><code># ${CK8S_CONFIG_PATH}/secrets.yaml\nobjectStorage:\ns3:\naccessKey: \"set-me\" # set to your s3 accesskey\nsecretKey: \"set-me\" # set to your s3 secretKey\n</code></pre>"},{"location":"operator-manual/safespring/#create-s3-buckets","title":"Create S3 buckets","text":"<p>Create object storage buckets for backups and container registry storage (if desired).</p> <p>For this you need to obtain access keys from Safespring to be able to create S3 buckets.</p> <pre><code>cd compliantkubernetes-apps\ngit checkout v0.17.0\nAWS_ACCESS_KEY=set-me\nAWS_ACCESS_SECRET_KEY=set-me\nscripts/S3/generate-s3cfg.sh safespring ${AWS_ACCESS_KEY} ${AWS_ACCESS_SECRET_KEY} s3.sto2.safedc.net sto2 &gt; ~/.s3cfg\nscripts/S3/entry.sh create\n</code></pre>"},{"location":"operator-manual/safespring/#test-s3","title":"Test S3","text":"<p>To ensure that you have configured S3 correctly, run the following snippet:</p> <pre><code>(\naccess_key=$(sops exec-file ${CK8S_CONFIG_PATH}/secrets.yaml 'yq r {} \"objectStorage.s3.accessKey\"')\nsecret_key=$(sops exec-file ${CK8S_CONFIG_PATH}/secrets.yaml 'yq r {} \"objectStorage.s3.secretKey\"')\nsc_config=$(yq m ${CK8S_CONFIG_PATH}/defaults/sc-config.yaml ${CK8S_CONFIG_PATH}/sc-config.yaml -a overwrite -x)\nregion=$(echo ${sc_config} | yq r - 'objectStorage.s3.region')\nhost=$(echo ${sc_config} | yq r -  'objectStorage.s3.regionEndpoint')\n\nfor bucket in $(echo ${sc_config} | yq r -  'objectStorage.buckets.*'); do\ns3cmd --access_key=${access_key} --secret_key=${secret_key} \\\n--region=${region} --host=${host} \\\nls s3://${bucket} &gt; /dev/null\n        [ ${?} = 0 ] &amp;&amp; echo \"Bucket ${bucket} exists!\"\ndone\n)\n</code></pre>"},{"location":"operator-manual/safespring/#prepare-for-compliant-kubernetes-apps","title":"Prepare for Compliant Kubernetes Apps","text":"<p>To make the kubeconfig files work with Compliant Kubernetes Apps, you will need to rename or copy them, since Compliant Kubernetes Apps currently only support clusters named <code>sc</code> and <code>wc</code>. If you have multiple workload clusters, you can make this work by setting <code>CK8S_CONFIG_PATH</code> to each <code>$CK8S_CONFIG_PATH/$CLUSTER-config</code> in turn. I.e. <code>CK8S_CONFIG_PATH</code> will be different for compliantkubernetes-kubespray and compliantkubernetes-apps.</p> <pre><code># In compliantkubernetes-kubespray\nCK8S_CONFIG_PATH=~/.ck8s/&lt;environment-name&gt;\n# In compliantkubernetes-apps (one config path per workload cluster)\nCK8S_CONFIG_PATH=~/.ck8s/&lt;environment-name&gt;/&lt;prefix&gt;-config\n</code></pre> <p>Copy the kubeconfig files to a path that Apps can find.</p> <p>Option 1 - A single workload cluster:</p> <pre><code>cp \"${CK8S_CONFIG_PATH}/.state/kube_config_${SERVICE_CLUSTER}.yaml\" \"${CK8S_CONFIG_PATH}/.state/kube_config_sc.yaml\"\ncp \"${CK8S_CONFIG_PATH}/.state/kube_config_${WORKLOAD_CLUSTERS[@]}.yaml\" \"${CK8S_CONFIG_PATH}/.state/kube_config_wc.yaml\"\n</code></pre> <p>You can now use the same <code>CK8S_CONFIG_PATH</code> for Apps as for compliantkubernetes-kubespray.</p> <p>Option 2 - A multiple workload cluster:</p> <pre><code>mkdir -p \"${CK8S_CONFIG_PATH}/${SERVICE_CLUSTER}-config/.state\"\ncp \"${CK8S_CONFIG_PATH}/.state/kube_config_${SERVICE_CLUSTER}.yaml\" \"${CK8S_CONFIG_PATH}/${SERVICE_CLUSTER}-config/.state/kube_config_sc.yaml\"\nfor CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nmkdir -p \"${CK8S_CONFIG_PATH}/${CLUSTER}-config/.state\"\ncp \"${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTERS}.yaml\" \"${CK8S_CONFIG_PATH}/${CLUSTER}-config/.state/kube_config_wc.yaml\"\ndone\n</code></pre> <p>You will then need to set <code>CK8S_CONFIG_PATH</code> to each <code>$CLUSTER-config</code> folder in turn, in order to install Apps on the service cluster and workload clusters.</p> <p>With this you should be ready to install Compliant Kubernetes Apps on top of the clusters.</p>"},{"location":"operator-manual/safespring/#edit-the-storage-class-manifests","title":"Edit the storage class manifests","text":"<pre><code>./bin/ck8s ops kubectl sc edit storageclass cinder-csi\n./bin/ck8s ops kubectl wc edit storageclass cinder-csi\n</code></pre> <p>Set storageclass.kubernetes.io/is-default-class: \"true\"</p>"},{"location":"operator-manual/safespring/#install-compliant-kubernetes-apps","title":"Install Compliant Kubernetes Apps!","text":"<p>Start with the service cluster:</p> <pre><code>ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml\n./bin/ck8s apply sc  # Respond \"n\" if you get a WARN\n</code></pre> <p>Then the workload clusters:</p> <pre><code>for CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\n    ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n    ./bin/ck8s apply wc  # Respond \"n\" if you get a WARN\ndone\n</code></pre>"},{"location":"operator-manual/safespring/#settling","title":"Settling","text":"<p>Important</p> <p>Leave sufficient time for the system to settle, e.g., request TLS certificates from LetsEncrypt, perhaps as much as 20 minutes.</p> <p>You can check if the system settled as follows:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces pods'\ndone\n</code></pre> <p>Check the output of the command above. All Pods needs to be Running or Completed.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces issuers,clusterissuers,certificates'\ndone\n</code></pre> <p>Check the output of the command above. All resources need to have the Ready column True.</p>"},{"location":"operator-manual/safespring/#testing","title":"Testing","text":"<p>After completing the installation step you can test if the apps are properly installed and ready using the commands below.</p> <p>Start with the service cluster:</p> <pre><code>ln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml\n./bin/ck8s test sc  # Respond \"n\" if you get a WARN\n</code></pre> <p>Then the workload clusters:</p> <pre><code>for CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n    ./bin/ck8s test wc  # Respond \"n\" if you get a WARN\ndone\n</code></pre> <p>Done. Navigate to the endpoints, for example <code>grafana.$BASE_DOMAIN</code>, <code>kibana.$BASE_DOMAIN</code>, <code>harbor.$BASE_DOMAIN</code>, etc. to discover Compliant Kubernetes's features.</p>"},{"location":"operator-manual/safespring/#removing-compliant-kubernetes-apps-from-your-cluster","title":"Removing Compliant Kubernetes Apps from your cluster","text":"<p>To remove the applications added by compliant kubernetes you can use the two scripts <code>clean-sc.sh</code> and <code>clean-wc.sh</code>, they are located here in the scripts folder.</p> <p>They perform the following actions:</p> <ol> <li>Delete the added helm charts</li> <li>Delete the added namespaces</li> <li>Delete any remaining PersistentVolumes</li> <li>Delete the added CustomResourceDefinitions</li> </ol> <p>Note: if user namespaces are managed by Compliant Kubernetes apps then they will also be deleted if you clean up the workload cluster.</p>"},{"location":"operator-manual/safespring/#remove-the-infrastructure","title":"Remove the infrastructure","text":"<p>Destroy the infrastructure using Terraform, the same way you created it:</p> <pre><code>cd compliantkubernetes-kubespray/\nMODULE_PATH=\"$(pwd)/kubespray/contrib/terraform/openstack\"\n\npushd \"${MODULE_PATH}\"\nfor CLUSTER in $SERVICE_CLUSTER \"${WORKLOAD_CLUSTERS[@]}\"; do\nterraform init\n  terraform destroy -var-file=\"${CK8S_CONFIG_PATH}/${CLUSTER}-config/cluster.tfvars\" -state=\"${CK8S_CONFIG_PATH}/${CLUSTER}-config/terraform.tfstate\"\ndone\npopd\n</code></pre> <p>Don't forget to remove any DNS records and object storage buckets that you may have created.</p>"},{"location":"operator-manual/troubleshooting/","title":"Troubleshooting Tools","text":"<p>Help! Something is wrong with my Compliant Kubernetes cluster. Fear no more, this guide will help you make sense.</p> <p>This guide assumes that:</p> <ul> <li>You have pre-requisites installed.</li> <li>Your environment variables, in particular <code>CK8S_CONFIG_PATH</code> is set.</li> <li>Your config folder (e.g. for OpenStack) is available.</li> <li><code>compliantkubernetes-apps</code> and <code>compliantkubernetes-kubespray</code> is available.</li> </ul>"},{"location":"operator-manual/troubleshooting/#i-have-no-clue-where-to-start","title":"I have no clue where to start","text":"<p>If you get lost, start checking from the \"physical layer\" and up.</p>"},{"location":"operator-manual/troubleshooting/#are-the-nodes-still-accessible-via-ssh","title":"Are the Nodes still accessible via SSH?","text":"<pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nansible -i ${CK8S_CONFIG_PATH}/${CLUSTER}-config/inventory.ini all -m ping\ndone\n</code></pre>"},{"location":"operator-manual/troubleshooting/#are-the-nodes-doing-fine","title":"Are the Nodes \"doing fine\"?","text":"<p>Dmesg should not display unexpected messages. OOM will show up here.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nansible -i ${CK8S_CONFIG_PATH}/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; hostname; dmesg | tail -n 10'\ndone\n</code></pre> <p>Uptime should show high uptime (e.g., days) and low load (e.g., less than 3):</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; hostname; uptime'\ndone\n</code></pre> <p>Any process that uses too much CPU?</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; hostname; ps -Ao user,uid,comm,pid,pcpu,tty --sort=-pcpu | head -n 6'\ndone\n</code></pre> <p>Is there enough disk space? All writeable file-systems should have at least 30% free.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; hostname; df -h'\ndone\n</code></pre> <p>Is there enough available memory? There should be at least a few GB of available memory.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; hostname; cat /proc/meminfo | grep Available'\ndone\n</code></pre> <p>Can Nodes access the Internet?</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; hostname; curl --silent  https://checkip.amazonaws.com'\ndone\n</code></pre> <p>Are the Nodes having the proper time? You should see <code>System clock synchronized: yes</code> and <code>NTP service: active</code>.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; timedatectl status'\ndone\n</code></pre>"},{"location":"operator-manual/troubleshooting/#is-the-base-os-doing-fine","title":"Is the base OS doing fine?","text":"<p>We generally run the latest Ubuntu LTS, at the time of this writing Ubuntu 20.04 LTS.</p> <p>You can confirm this by doing:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'cat /etc/lsb-release'\ndone\n</code></pre> <p>Are systemd units running fine? You should see <code>running</code> and not <code>degraded</code>.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'systemctl is-system-running'\ndone\n</code></pre>"},{"location":"operator-manual/troubleshooting/#are-the-kubernetes-clusters-doing-fine","title":"Are the Kubernetes clusters doing fine?","text":"<p>Are the Nodes reporting in on Kubernetes? All Kubernetes Nodes, both control-plane and workers, should be <code>Ready</code>:</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get nodes'\ndone\n</code></pre>"},{"location":"operator-manual/troubleshooting/#is-rook-doing-fine","title":"Is Rook doing fine?","text":"<p>If Rook is installed, is Rook doing fine? You should see <code>HEALTH_OK</code>.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} -n rook-ceph apply -f ./compliantkubernetes-kubespray/rook/toolbox-deploy.yaml'\ndone\n\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} -n rook-ceph exec deploy/rook-ceph-tools -- ceph status'\ndone\n</code></pre>"},{"location":"operator-manual/troubleshooting/#are-kubernetes-pods-doing-fine","title":"Are Kubernetes Pods doing fine?","text":"<p>Pods should be <code>Running</code> or <code>Completed</code>, and fully <code>Ready</code> (e.g., <code>1/1</code> or <code>6/6</code>)?</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces pods'\ndone\n</code></pre> <p>Are all Deployments fine? Deployments should show all Pods Ready, Up-to-date and Available (e.g., <code>2/2 2 2</code>).</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces deployments'\ndone\n</code></pre> <p>Are all DaemonSets fine? DaemonSets should show as many Pods Desired, Current, Ready and Up-to-date, as Desired.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nsops exec-file ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml \\\n'kubectl --kubeconfig {} get --all-namespaces ds'\ndone\n</code></pre>"},{"location":"operator-manual/troubleshooting/#are-helm-releases-fine","title":"Are Helm Releases fine?","text":"<p>All Releases should be <code>deployed</code>.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nexport KUBECONFIG=kube_config_$CLUSTER.yaml\n    sops -d ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml &gt; $KUBECONFIG\nhelm list --all --all-namespaces\n    shred $KUBECONFIG\ndone\n</code></pre>"},{"location":"operator-manual/troubleshooting/#is-cert-manager-doing-fine","title":"Is cert-manager doing fine?","text":"<p>Are (Cluster)Issuers fine? All Resources should be <code>READY=True</code> or <code>valid</code>.</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nexport KUBECONFIG=kube_config_$CLUSTER.yaml\n    sops -d ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml &gt; $KUBECONFIG\nkubectl get clusterissuers,issuers,certificates,orders,challenges --all-namespaces\n    shred $KUBECONFIG\ndone\n</code></pre>"},{"location":"operator-manual/troubleshooting/#where-do-i-find-the-nodes-public-and-private-ip","title":"Where do I find the Nodes public and private IP?","text":"<pre><code>find . -name inventory.ini\n</code></pre> <p>or</p> <pre><code>for CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\nansible-inventory -i ${CK8S_CONFIG_PATH}/${CLUSTER}-config/inventory.ini --list all\ndone\n</code></pre> <p><code>ansible_host</code> is usually the public IP, while <code>ip</code> is usually the private IP.</p>"},{"location":"operator-manual/troubleshooting/#node-cannot-be-access-via-ssh","title":"Node cannot be access via SSH","text":"<p>Important</p> <p>Make sure it is \"not you\". Are you well connected to the VPN? Is this the only Node which lost SSH access?</p> <p>Important</p> <p>If you are using Rook, it is usually set up with replication 2, which means it can tolerate one restarting Node. Make sure that, either Rook is healthy or that you are really sure you are restarting the right Node.</p> <p>Try connecting to the unhealthy Node via a different Node and internal IP:</p> <pre><code>UNHEALTHY_NODE=172.0.10.205  # You lost access to this one\nJUMP_NODE=89.145.xxx.yyy  # You have access to this one\n\nssh -J ubuntu@$JUMP_NODE ubuntu@$UNHEALTHY_NODE\n</code></pre> <p>Try rebooting the Node via cloud provider specific CLI:</p> <pre><code>UNHEALTHY_NODE=cksc-worker-2\n\n# Example for ExoScale\nexo vm reboot --force $UNHEALTHY_NODE\n</code></pre> <p>If using Rook make sure its health goes back to <code>HEALTH_OK</code>.</p>"},{"location":"operator-manual/troubleshooting/#a-node-has-incorrect-time","title":"A Node has incorrect time","text":"<p>Incorrect time on a Node can have sever consequences with replication and monitoring. In fact, if you follow ISO 27001, A.12.4.4 Clock Synchronisation requires you to ensure clocks are synchronized.</p> <p>These days, Linux distributions should come out-of-the-box with timesyncd for time synchronization via NTP.</p> <p>To figure out what is wrong, SSH into the target Node and try the following:</p> <pre><code>sudo systemctl status systemd-timesyncd\nsudo journalctl --unit systemd-timesyncd\nsudo timedatectl status\nsudo timedatectl timesync-status\nsudo timedatectl show-timesync\n</code></pre> <p>Possible causes include incorrect NTP server settings, or NTP being blocked by firewall. For reminder, NTP works over UDP port 123.</p>"},{"location":"operator-manual/troubleshooting/#node-seems-not-fine","title":"Node seems not fine","text":"<p>Important</p> <p>If you are using Rook, it is usually set up with replication 2, which means it can tolerate one restarting Node. Make sure that, either Rook is healthy or that you are really sure you are restarting the right Node.</p> <p>Try rebooting the Node:</p> <pre><code>UNHEALTHY_NODE=89.145.xxx.yyy\n\nssh ubuntu@$UNHEALTHY_NODE sudo reboot\n</code></pre> <p>If using Rook make sure its health goes back to <code>HEALTH_OK</code>.</p>"},{"location":"operator-manual/troubleshooting/#node-seems-really-not-fine-i-want-a-new-one","title":"Node seems really not fine. I want a new one.","text":"<p>Is it 2AM? Do not replace Nodes, instead simply add a new one. You might run out of capacity, you might lose redundancy, you might replace the wrong Node. Prefer to add a Node and see if that solves the problem.</p>"},{"location":"operator-manual/troubleshooting/#okay-i-want-to-add-a-new-node","title":"Okay, I want to add a new Node.","text":"<p>Prefer this option if you \"quickly\" need to add CPU, memory or storage (i.e., Rook) capacity.</p> <p>First, check for infrastructure drift, as shown here.</p> <p>Depending on your provider:</p> <ol> <li>Add a new Node by editing the <code>*.tfvars</code>.</li> <li>Re-apply Terraform.</li> <li>Re-create the <code>inventory.ini</code> (skip this step if the cluster is using a dynamic inventory).</li> <li>Re-apply Kubespray.</li> <li>Re-fix the Kubernetes API URL.</li> </ol> <p>Check that the new Node joined the cluster, as shown here.</p>"},{"location":"operator-manual/troubleshooting/#a-systemd-unit-failed","title":"A systemd unit failed","text":"<p>SSH into the Node. Check which systemd unit is failing:</p> <pre><code>systemctl --failed\n</code></pre> <p>Gather more information:</p> <pre><code>FAILED_UNIT=fwupd-refresh.service\n\nsystemctl status $FAILED_UNIT\njournalctl --unit $FAILED_UNIT\n</code></pre>"},{"location":"operator-manual/troubleshooting/#rook-seems-not-fine","title":"Rook seems not fine","text":"<p>Please check the following upstream documents:</p> <ul> <li>Rook Common Issues</li> <li>Ceph Common Issues</li> </ul>"},{"location":"operator-manual/troubleshooting/#pod-seems-not-fine","title":"Pod seems not fine","text":"<p>Before starting, set up a handy environment:</p> <pre><code>CLUSTER=cksc  # Cluster containing the unhealthy Pod\n\nexport KUBECONFIG=kube_config_$CLUSTER.yaml\nsops -d ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml &gt; $KUBECONFIG\n</code></pre> <p>Check that you are on the right cluster:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Find the name of the Pod which is not fine:</p> <pre><code>kubectl get pod -A\n\n# Copy-paste the Pod and Pod namespace below\nUNHEALTHY_POD=prometheus-kube-prometheus-stack-prometheus-0\nUNHEALTHY_POD_NAMESPACE=monitoring\n</code></pre> <p>Gather some \"evidence\" for later diagnostics, when the heat is over:</p> <pre><code>kubectl describe pod -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD\nkubectl logs -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD\n</code></pre> <p>Try to kill  and check if the underlying Deployment, StatefulSet or DaemonSet will restart it:</p> <pre><code>kubectl delete pod -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD\nkubectl get pod -A --watch\n</code></pre>"},{"location":"operator-manual/troubleshooting/#helm-release-is-failed","title":"Helm Release is <code>failed</code>","text":"<p>Before starting, set up a handy environment:</p> <pre><code>CLUSTER=cksc  # Cluster containing the failed Release\n\nexport KUBECONFIG=kube_config_$CLUSTER.yaml\nsops -d ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml &gt; $KUBECONFIG\n</code></pre> <p>Check that you are on the right cluster:</p> <pre><code>kubectl get nodes\n</code></pre> <p>Find the failed Release: <pre><code>helm ls --all-namespaces --all\n\nFAILED_RELEASE=user-rbac\nFAILED_RELEASE_NAMESPACE=kube-system\n</code></pre></p> <p>Just to make sure, do a drift check, as shown here.</p> <p>Remove the failed Release: <pre><code>helm uninstall -n $FAILED_RELEASE_NAMESPACE $FAILED_RELEASE\n</code></pre></p> <p>Re-apply <code>apps</code> according to documentation.</p>"},{"location":"operator-manual/troubleshooting/#cert-manager-is-not-fine","title":"cert-manager is not fine","text":"<p>Follow cert-manager's troubleshooting, specifically:</p> <ul> <li>Troubleshooting</li> <li>Troubleshooting Issuing ACME Certificates</li> </ul>"},{"location":"operator-manual/troubleshooting/#failed-to-perform-self-check-no-such-host","title":"Failed to perform self check: no such host","text":"<p>If with <code>kubectl describe challenges -A</code> you get an error similar to below:</p> <pre><code>Waiting for HTTP-01 challenge propagation: failed to perform self check\n    GET request ''http://url/.well-known/acme-challenge/xVfDZoLlqs4tad2qOiCT4sjChNRausd5iNpbWuGm5ls'':\n    Get \"http://url/.well-known/acme-challenge/xVfDZoLlqs4tad2qOiCT4sjChNRausd5iNpbWuGm5ls\":\n    dial tcp: lookup opensearch.domain on 10.177.0.3:53: no such host'\n</code></pre> <p>Then you might have a DNS issue inside your cluster. Make sure that <code>global.clusterDns</code> in <code>common-config.yaml</code> is set to the CoreDNS Service IP returned by <code>kubectl get svc -n kube-system coredns</code>.</p>"},{"location":"operator-manual/troubleshooting/#failed-to-perform-self-check-connection-timed-out","title":"Failed to perform self check: connection timed out","text":"<p>If with <code>kubectl describe challenges -A</code> you get an error similar to below:</p> <pre><code>Reason: Waiting for http-01 challenge propagation: failed to perform self check GET request 'http://abc.com/.well-known/acme-challenge/Oej8tloD2wuHNBWS6eVhSKmGkZNfjLRemPmpJoHOPkA': Get \"http://abc.com/.well-known/acme-challenge/Oej8tloD2wuHNBWS6eVhSKmGkZNfjLRemPmpJoHOPkA\": dial tcp 18.192.17.98:80: connect: connection timed out\n</code></pre> <p>Then your Kubernetes data plane Nodes cannot connect to themselves with the IP address of the load-balancer that fronts them. The easiest is to configure the load-balancer's IP address on the loopback interface of each Nodes. (See example here.)</p>"},{"location":"operator-manual/troubleshooting/#how-do-i-check-if-infrastructure-drifted-due-to-manual-intervention","title":"How do I check if infrastructure drifted due to manual intervention?","text":"<p>Go to the docs of the cloud provider and run Terraform <code>plan</code> instead of <code>apply</code>. For Exoscale, it looks as follows:</p> <pre><code>TF_SCRIPTS_DIR=$(readlink -f compliantkubernetes-kubespray/kubespray/contrib/terraform/exoscale)\nfor CLUSTER in ${SERVICE_CLUSTER} \"${WORKLOAD_CLUSTERS[@]}\"; do\npushd ${TF_SCRIPTS_DIR}\nexport TF_VAR_inventory_file=${CK8S_CONFIG_PATH}/${CLUSTER}-config/inventory.ini\n    terraform init\n    terraform plan \\\n-var-file=${CK8S_CONFIG_PATH}/${CLUSTER}-config/cluster.tfvars \\\n-state=${CK8S_CONFIG_PATH}/${CLUSTER}-config/terraform.tfstate\n    popd\ndone\n</code></pre>"},{"location":"operator-manual/troubleshooting/#how-do-i-check-if-the-kubespray-setup-drifted-due-to-manual-intervention","title":"How do I check if the Kubespray setup drifted due to manual intervention?","text":"<p>At the time of this writing, this cannot be done, but efforts are underway.</p>"},{"location":"operator-manual/troubleshooting/#how-do-i-check-if-apps-drifted-due-to-manual-intervention","title":"How do I check if <code>apps</code> drifted due to manual intervention?","text":"<pre><code># For service cluster\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${SERVICE_CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_sc.yaml\n./compliantkubernetes-apps/bin/ck8s ops helmfile sc diff  # Respond \"n\" if you get WARN\n</code></pre> <pre><code># For the workload clusters\nfor CLUSTER in \"${WORKLOAD_CLUSTERS[@]}\"; do\nln -sf $CK8S_CONFIG_PATH/.state/kube_config_${CLUSTER}.yaml $CK8S_CONFIG_PATH/.state/kube_config_wc.yaml\n    ./compliantkubernetes-apps/bin/ck8s ops helmfile wc diff  # Respond \"n\" if you get WARN\ndone\n</code></pre>"},{"location":"operator-manual/troubleshooting/#velero-backup-stuck-in-progress","title":"Velero backup stuck in progress","text":"<p>Velero is known to get stuck <code>InProgress</code> when doing backups</p> <pre><code>velero backup get\n\nNAME                                 STATUS             ERRORS   WARNINGS   CREATED                          EXPIRES   STORAGE LOCATION   SELECTOR\nvelero-daily-backup-20211005143248   InProgress         0        0          2021-10-05 14:32:48 +0200 CEST   29d       default            !nobackup\n</code></pre> <p>First try to delete the backup</p> <pre><code>./velero backup delete velero-daily-backup-20211005143248\n</code></pre> <p>Then kill all the pods under the velero namespace <pre><code>./compliantkubernetes-apps/bin/ck8s ops kubectl wc delete pods -n velero --all\n</code></pre></p> <p>Check that the backup is gone <pre><code>velero backup get\n\nNAME                                 STATUS             ERRORS   WARNINGS   CREATED                          EXPIRES   STORAGE LOCATION   SELECTOR\n</code></pre></p> <p>Recreate the backup from a schedule</p> <pre><code>velero backup create --from-schedule velero-daily-backup\n</code></pre>"},{"location":"operator-manual/user-alerts/","title":"Enabling user alerts","text":"<p>This is administrator-facing documentation associated with this user guide. Please read that one first.</p> <p>Important</p> <p>Alertmanager should no longer be access via an Ingress, since this circumvents access control and audit logs. Please find the updated access method in the user guide linked above.</p> <p>Perform the following configuration changes in <code>wc-config.yaml</code>:</p> <ol> <li>Set <code>user.alertmanager.enabled=true</code>.</li> <li>Ensure <code>user.alertmanager.ingress.enabled</code> is false.</li> </ol> <p>For v0.18 and below include the following changes:</p> <ol> <li>Update the <code>user.namespaces</code> list to include <code>alertmanager</code>.</li> <li>Set <code>user.alertmanager.namespace=alertmanager</code>.</li> </ol> <p>Then apply ck8s-apps.</p>"},{"location":"operator-manual/user-alerts/#example","title":"Example","text":"<p>Please find below an example taken from <code>wc-config.yaml</code>, which was tested with compliantkubernetes-apps v0.17.0 and also applies to version v0.18:</p> <pre><code>user:\n## This only controls if the namespaces should be created, user RBAC is always created.\ncreateNamespaces: true\n## List of user namespaces to create.\nnamespaces:\n- alertmanager\n- demo1\n- demo2\n- demo3\n## List of users to create RBAC rules for.\nadminUsers:\n- cristian.klein@elastisys.com\n- lars.larsson@elastisys.com\n- admin@example.com\n## User controlled alertmanager configuration.\nalertmanager:\nenabled: true\n## Namespace in which to install alertmanager\nnamespace: alertmanager\n## Create basic-auth protected ingress to alertmanager\ningress:\nenabled: false\n</code></pre> <p>Note</p> <p>For versions after v0.18 <code>alertmanager</code> may not be listed under <code>user.namespaces</code> and the option <code>user.alertmanager.namespace</code> is deprecated.</p>"},{"location":"release-notes/argocd/","title":"Release Notes","text":""},{"location":"release-notes/argocd/#compliant-kubernetes-argo-cd","title":"Compliant Kubernetes Argo CD","text":"<ul> <li>v2.4.20-ck8s1 - 2023-03-29</li> </ul>"},{"location":"release-notes/argocd/#v2420-ck8s1","title":"v2.4.20-ck8s1","text":"<p>Released 2023-03-29</p> <p>First release of Argo CD with version <code>2.4.20</code>!</p>"},{"location":"release-notes/ck8s/","title":"Compliant Kubernetes","text":""},{"location":"release-notes/ck8s/#release-notes","title":"Release Notes","text":""},{"location":"release-notes/ck8s/#compliant-kubernetes","title":"Compliant Kubernetes","text":"<ul> <li>v0.29.0 - 2023-03-16</li> <li>v0.28.1 - 2023-03-02</li> <li>v0.28.0 - 2023-01-30</li> <li>v0.27.0 - 2022-11-17</li> <li>v0.26.0 - 2022-09-19</li> <li>v0.25.0 - 2022-08-25</li> <li>v0.24.1 - 2022-08-01</li> <li>v0.24.0 - 2022-07-25</li> <li>v0.23.0 - 2022-07-06</li> <li>v0.22.0 - 2022-06-01</li> <li>v0.21.0 - 2022-05-04</li> <li>v0.20.0 - 2022-03-21</li> <li>v0.19.1 - 2022-03-01</li> <li>v0.19.0 - 2022-02-01</li> <li>v0.18.2 - 2021-12-16</li> <li>v0.17.2 - 2021-12-16</li> <li>v0.18.1 - 2021-12-08</li> <li>v0.17.1 - 2021-12-08</li> <li>v0.18.0 - 2021-11-04</li> <li>v0.17.0 - 2021-06-29</li> <li>v0.16.0 - 2021-05-27</li> </ul> <p>Note</p> <p>For a more detailed look check out the full changelog.</p>"},{"location":"release-notes/ck8s/#v0290","title":"v0.29.0","text":"<p>Released 2023-03-16</p>"},{"location":"release-notes/ck8s/#added","title":"Added","text":"<ul> <li>Static users can now be added in OpenSearch.</li> </ul>"},{"location":"release-notes/ck8s/#changed","title":"Changed","text":"<ul> <li>The Fluentd deplyoment has changed considerably and users must ensure that their custom filters continue to work as expected.</li> </ul>"},{"location":"release-notes/ck8s/#updated","title":"Updated","text":"<ul> <li>Cert-manager updated to <code>v1.11.0</code></li> <li>The containers in pods created by cert-manager have been renamed to better reflect what they do. This can be breaking for automation that relies on these names being static.</li> <li>The cert-manager Gateway API integration now uses the v1beta1 API version. ExperimentalGatewayAPISupport alpha feature users must upgrade to v1beta of Gateway API.</li> </ul>"},{"location":"release-notes/ck8s/#v0281","title":"v0.28.1","text":"<p>Released 2023-03-02</p>"},{"location":"release-notes/ck8s/#added_1","title":"Added","text":"<ul> <li>Added falco rules to ignore redis operator related alerts.</li> </ul>"},{"location":"release-notes/ck8s/#v0280","title":"v0.28.0","text":"<p>Released 2023-01-30</p>"},{"location":"release-notes/ck8s/#changed_1","title":"Changed","text":"<ul> <li>Updated Rook alerts to <code>v1.10.5</code>.</li> <li>Nginx ingress controller service can now have multiple annotations instead of just one.</li> <li>Synced all grafana dashboards to use the default organization timezone.</li> <li>Several default resource requests and limits have changed for the included services.</li> </ul>"},{"location":"release-notes/ck8s/#fixed","title":"Fixed","text":"<ul> <li>Use FQDN for services connecting from the workload cluster to the service cluster to prevent resolve timeouts.</li> <li>Fixed <code>KubeletDown</code> alert rule not alerting if a kubelet was missing.</li> <li>Added permissions to the <code>alerting_full_access</code> role in Opensearch to be able to view notification channels.</li> <li>Added <code>fluent-plugin-record-modifier</code> to the fluentd image to prevent mapping errors.</li> <li>Various fixes to network policies.</li> </ul>"},{"location":"release-notes/ck8s/#added_2","title":"Added","text":"<ul> <li>Improved security posture by adding network policies for some of the networking and storage components.</li> <li>Added alert for less kubelets than nodes in the cluster.</li> <li>Added alert for object limits in buckets.</li> </ul>"},{"location":"release-notes/ck8s/#v0270","title":"v0.27.0","text":"<p>Released 2022-11-17</p>"},{"location":"release-notes/ck8s/#updated_1","title":"Updated","text":"<ul> <li>Updated Dex helm chart to <code>v0.12.0</code>, which also upgrades Dex to <code>v2.35.1</code>.</li> <li>Updated Falco helm chart to <code>2.2.0</code>, which also upgrades Falco to <code>0.33.0</code> and Falco Sidekick to <code>2.26.0</code>.</li> <li>Updated Falco Exporter helm chart to <code>0.9.0</code>, which also upgrades Falco Exporter to <code>0.8.0</code>.</li> <li>Updated Velero helm chart to <code>v2.31.8</code>, which also upgrades Velero to <code>v1.9.2</code>.</li> <li>Updated Grafana helm chart to <code>v6.43.4</code>, which also upgrades Grafana to <code>v9.2.4</code>.</li> </ul>"},{"location":"release-notes/ck8s/#changed_2","title":"Changed","text":"<ul> <li>Improved Network security by adding Network policies to a lot of the included services.</li> <li>NetworkPolicies are now automatically propagated from a parent namespace to its subnamespaces in HNC.</li> <li>Several default resource requests and limits have changed for the included services.</li> <li>Lowered the default retention age for Kubernetes logs in the prod flavor down to 30 days.</li> <li>Made dex ID Token expiration time configurable.</li> <li>User alertmanager is now enabled by default.</li> </ul>"},{"location":"release-notes/ck8s/#fixed_1","title":"Fixed","text":"<ul> <li>Fixed an issue with the \"Kubernetes cluster status\" Grafana dashboard not loading data for some panels</li> <li>Rclone can now be configured to run every x minutes/hours/days/week/month/year.</li> </ul>"},{"location":"release-notes/ck8s/#added_3","title":"Added","text":"<ul> <li>Added RBAC for admin users to view Gatekeeper constraints.</li> <li>New section in the welcoming dashboards, displaying the most relevant features and changes for the user added in the last two releases.</li> <li>Added an option to configure alerts for growing indices in OpenSearch.</li> <li>The settings for this might need to be tweaked to better suit the environment.</li> <li>Added an alert for failed evicted pods (KubeFailedEvictedPods).</li> </ul>"},{"location":"release-notes/ck8s/#v0260","title":"v0.26.0","text":"<p>Released 2022-09-19</p>"},{"location":"release-notes/ck8s/#updated_2","title":"Updated","text":"<ul> <li>Harbor upgraded to <code>v2.6.0</code></li> <li>Upgraded Opensearch helm chart to <code>2.6.0</code>, this upgrades Opensearch to <code>2.3.0</code>. For more information about the upgrade, check out their 2.3 Launch Announcement.</li> </ul>"},{"location":"release-notes/ck8s/#fixed_2","title":"Fixed","text":"<ul> <li>Fixed the welcome dashboard template for OpenSearch Dashboards</li> </ul>"},{"location":"release-notes/ck8s/#added_4","title":"Added","text":"<ul> <li>Option to create custom solvers for letsencrypt issuers, including a simple way to add secrets</li> <li>Kube-bench runs on every node   Automated CIS tests are performed on each node using kube-bench   Added a CIS kube-bench Grafana dashboard</li> <li>Added option for kured to notify to slack when draning and rebooting nodes</li> <li>Allow users to proxy and port-forward to prometheus running in the workload cluster</li> </ul>"},{"location":"release-notes/ck8s/#v0250","title":"v0.25.0","text":"<p>Released 2022-08-25</p>"},{"location":"release-notes/ck8s/#added_5","title":"Added","text":"<ul> <li>Added Hierarchical Namespace Controller Allowing users to create and manage subnamespaces, namespaces within namespaces. You can read more about this in our FAQ.</li> <li>Added support for custom solvers in cluster issuers  Allowing DNS01 challenges for certificate requests.</li> <li>Added support for running Harbor in High Availability</li> </ul>"},{"location":"release-notes/ck8s/#updated_3","title":"Updated","text":"<ul> <li> <p>Updated cert-manager from v1.6.1 to v1.8.2  API versions <code>v1alpha2</code>, <code>v1alpha3</code>, and <code>v1beta1</code> have been removed from the custom resource definitions (CRDs), certificate rotation policy will now be validated. See their changelog for more details.</p> </li> <li> <p>Updated OpenSearch with new usability improvements and features  Checkout their launch announcement.</p> </li> </ul>"},{"location":"release-notes/ck8s/#changed_3","title":"Changed","text":"<ul> <li>New additions to the Kubernetes cluster status Grafana dashboard  It now shows information about resource requests and limits per node, and resource usage vs request per pod.</li> </ul>"},{"location":"release-notes/ck8s/#v0241","title":"v0.24.1","text":"<p>Released 2022-08-01</p> <ul> <li>Required patch to be able to use release <code>v0.24.0</code></li> </ul>"},{"location":"release-notes/ck8s/#fixed_3","title":"Fixed","text":"<ul> <li>Fixed a formatting issue with harbor s3 configuration.</li> </ul>"},{"location":"release-notes/ck8s/#v0240","title":"v0.24.0","text":"<p>Released 2022-07-25</p>"},{"location":"release-notes/ck8s/#updated_4","title":"Updated","text":"<ul> <li> <p>Upgraded Helm stack   Upgrades for Helm, Helmfile and Helm-secrets.</p> </li> <li> <p>Image upgrade to node-local-dns</p> </li> </ul>"},{"location":"release-notes/ck8s/#changed_4","title":"Changed","text":"<ul> <li>Improved stability to automatic node reboots</li> </ul>"},{"location":"release-notes/ck8s/#added_6","title":"Added","text":"<ul> <li>Further configurability to ingress-nginx</li> </ul>"},{"location":"release-notes/ck8s/#v0230","title":"v0.23.0","text":"<p>Released 2022-07-06</p>"},{"location":"release-notes/ck8s/#updated_5","title":"Updated","text":"<ul> <li>Updated the ingress controller <code>ingress-nginx</code> to image version v1.2.1</li> <li>You can find the changelog here.</li> </ul>"},{"location":"release-notes/ck8s/#changed_5","title":"Changed","text":"<ul> <li>Added support for accessing Alertmanager via port-forward</li> </ul>"},{"location":"release-notes/ck8s/#added_7","title":"Added","text":"<ul> <li>Backups can now be encrypted before they are replicated to an off-site S3 service.</li> <li>Improved metrics and alerting for OpenSearch.</li> </ul>"},{"location":"release-notes/ck8s/#fixed_4","title":"Fixed","text":"<ul> <li>The deployment of Dex is now properly configured to be HA, ensuring that the Dex instances are placed on different Kubernetes worker nodes.</li> </ul>"},{"location":"release-notes/ck8s/#v0220","title":"v0.22.0","text":"<p>Released 2022-06-01</p>"},{"location":"release-notes/ck8s/#added_8","title":"Added","text":"<ul> <li> <p>Added support for Elastx and UpCloud!</p> </li> <li> <p>New 'Welcoming' dashboard in OpenSearch and Grafana.   Users can now access public docs and different urls to the services provided by Compliant Kubernetes.</p> </li> <li> <p>Improved availability of metrics and alerting.   Alertmanager now runs with two replicas by default, Prometheus can now be run in HA mode.</p> </li> <li> <p>Added Falco rules to reduce alerts for services in Compliant Kubernetes.   Falco now alerts less on operations that are expected out of these services.</p> </li> </ul>"},{"location":"release-notes/ck8s/#fixed_5","title":"Fixed","text":"<ul> <li> <p>Fixed a bug where users couldn't silence alerts when portforwarding to alertmanager.</p> </li> <li> <p>Improved logging stack and fixed a number of issues to ensure reliability.</p> </li> </ul>"},{"location":"release-notes/ck8s/#v0210","title":"v0.21.0","text":"<p>Released 2022-05-04</p>"},{"location":"release-notes/ck8s/#changed_6","title":"Changed","text":"<ul> <li> <p>Users can now view ClusterIssuers.</p> </li> <li> <p>User admins can now add users to the ClusterRole user-view.   This is done by adding users to the ClusterRoleBinding <code>extra-user-view</code>.</p> </li> <li> <p>User can now get ClusterIssuers.</p> </li> <li> <p>Ensured all CISO dashboards are available to users.   All the grafana dashboards in our CISO docs are now available.</p> </li> <li> <p>Better stability for dex   Dex now runs with two replicas and has been updated.</p> </li> </ul>"},{"location":"release-notes/ck8s/#updated_6","title":"Updated","text":"<ul> <li>Image upgrades to reduce number of vulnerabilities   Upgrades for fluentd, grafana, and harbor chartmuseum.</li> </ul>"},{"location":"release-notes/ck8s/#v0200","title":"v0.20.0","text":"<p>Released 2022-03-21</p>"},{"location":"release-notes/ck8s/#added_9","title":"Added","text":"<ul> <li> <p>Added kured - Kubernetes Reboot Daemon.   This enables automatic node reboots and security patching of the underlying base Operating System image, container runtime and Kubernetes cluster components.</p> </li> <li> <p>Added fluentd grafana dashboard and alerts.</p> </li> <li> <p>Added RBAC for admin users.   Admin users can now list pods cluster wide and run the kubectl top command.</p> </li> <li> <p>Added containerd support for fluentd.</p> </li> </ul>"},{"location":"release-notes/ck8s/#changed_7","title":"Changed","text":"<ul> <li> <p>Added the new OPA policy.   To disallow the latest image tag.</p> </li> <li> <p>Persist Dex state in Kubernetes.   This ensure the JWT token received from an OpenID provider is valid even after security patching of Kubernetes cluster components.</p> </li> <li> <p>Add ingressClassName in ingresses where that configuration option is available.</p> </li> <li> <p>Thanos is now enabled by default.</p> </li> </ul>"},{"location":"release-notes/ck8s/#updated_7","title":"Updated","text":"<ul> <li> <p>Upgraded nginx-ingress helm chart to v4.0.17   This upgrades nginx-ingress to v1.1.1. When upgrading an ingressClass object called nginx will be installed, this class has been set as the default class in Kubernetes. Ingress-nginx has been configured to still handle existing ingress objects that do not specify any ingressClassName.</p> </li> <li> <p>Upgraded starboard-operator helm chart to v0.9.1   This is upgrading starboard-operator to v0.14.1</p> </li> </ul>"},{"location":"release-notes/ck8s/#removed","title":"Removed","text":"<ul> <li>Removed influxDB and dependent helm charts.</li> </ul>"},{"location":"release-notes/ck8s/#v0191","title":"v0.19.1","text":"<p>Released 2022-03-01</p>"},{"location":"release-notes/ck8s/#fixed_6","title":"Fixed","text":"<ul> <li>Fixed critical stability issue related to Prometheus rules being evaluated without metrics.</li> </ul>"},{"location":"release-notes/ck8s/#v0190","title":"v0.19.0","text":"<p>Released 2022-02-01</p>"},{"location":"release-notes/ck8s/#added_10","title":"Added","text":"<ul> <li> <p>Added Thanos as a new metrics backend.   Provides a much more efficient and reliable platform for long-term metrics, with the capabilities to keep metrics for much longer time periods than previously possible.   InfluxDB will still be supported in this release.</p> </li> <li> <p>Added a new feature to enable off-site replication of backups.   Synchronizes S3 buckets across regions or clouds to keep an off-site backup.</p> </li> <li> <p>Added a new feature to create and log into separate indices per namespace. Currently considered to be an alpha feature.</p> </li> </ul>"},{"location":"release-notes/ck8s/#changed_8","title":"Changed","text":"<ul> <li> <p>Replacing Open Distro for Elasticsearch with OpenSearch.   In this release, since the Open Distro project has reached end of life, Elasticsearch is replaced with OpenSearch and Kibana with OpenSearch Dashboards.   OpenSearch is a fully open source fork of Elasticsearch with a compatible API and familiar User Experience. Note that recent versions of official Elasticsearch clients and tools will not work with OpenSearch as they employ a product check, compatible versions can be found here.</p> </li> <li> <p>Enforcing OPA policies by default.   Provides strict safeguards by default.</p> </li> <li> <p>Allowing viewers to inspect and temporarily edit panels in Grafana.   Gives more insight to the metrics and data shown.</p> </li> <li> <p>Setting Fluentd to log the reason why when it can't push logs to OpenSearch.</p> </li> </ul>"},{"location":"release-notes/ck8s/#updated_8","title":"Updated","text":"<ul> <li>Large number of application and service updates, keeping up to date with new security fixes and changes.</li> </ul>"},{"location":"release-notes/ck8s/#v0182","title":"v0.18.2","text":"<p>Released 2021-12-16.</p> <p>Changes:</p> <ul> <li>Updated Open Distro for Elasticsearch to 1.13.3 to mitigate CVE-2021-44228 &amp; CVE-2021-45046</li> </ul>"},{"location":"release-notes/ck8s/#v0172","title":"v0.17.2","text":"<p>Released 2021-12-16.</p> <p>Changes:</p> <ul> <li>Updated Open Distro for Elasticsearch to 1.13.3 to mitigate CVE-2021-44228 &amp; CVE-2021-45046</li> </ul>"},{"location":"release-notes/ck8s/#v0181","title":"v0.18.1","text":"<p>Released 2021-12-08.</p> <p>Changes:</p> <ul> <li>updated Grafana to 8.0.7 in order to fix CVE-2021-43798</li> </ul>"},{"location":"release-notes/ck8s/#v0171","title":"v0.17.1","text":"<p>Released 2021-12-08.</p> <p>Changes:</p> <ul> <li>updated Grafana to 8.0.7 in order to fix CVE-2021-43798</li> </ul>"},{"location":"release-notes/ck8s/#v0180","title":"v0.18.0","text":"<p>Released 2021-11-04.</p> <p>Changes:</p> <ul> <li>Ingress-nginx-controller has been updated from v0.28.0 to v0.49.3, bringing various updates.<ul> <li>Additionally, the configuration option <code>allow-snippet-annotations</code> has been set to <code>false</code> to mitigate known security issue CVE-2021-25742</li> </ul> </li> <li>Fixes, minor version upgrades, improvements to resource requests and limits for applications, improvements to stability.</li> </ul>"},{"location":"release-notes/ck8s/#v0170","title":"v0.17.0","text":"<p>Released 2021-06-29.</p> <p>Changes:</p> <ul> <li>The dashboard tool Grafana has been updated to a new major version of 8.x.x. This introduces new features and fixes, as well as some possibly breaking changes. See their release notes for more information.</li> <li>The single-sign-on service Dex has been updated, bringing small changes and better consistency to the UI.</li> <li>Fixes, improvements to resource limits, resource usage, and stability.</li> </ul>"},{"location":"release-notes/ck8s/#v0160","title":"v0.16.0","text":"<p>Released 2021-05-27.</p> <p>Changes:</p> <ul> <li>The default retention values have been changed and streamlined for <code>authlog*</code> and <code>other*</code>. The former will be kept for a longer period of time while the latter for shorter, both have reduced sized according to their actual usage.</li> <li>Updates, fixes, and features to improve the security of the platform.</li> </ul>"},{"location":"release-notes/jaeger/","title":"Release Notes","text":""},{"location":"release-notes/jaeger/#compliant-kubernetes-jaeger","title":"Compliant Kubernetes Jaeger","text":"<ul> <li>v1.39.0-ck8s1 - 2023-03-07</li> </ul>"},{"location":"release-notes/jaeger/#v1390-ck8s1","title":"v1.39.0-ck8s1","text":"<p>Released 2023-03-07</p> <p>First release using Jaeger operator version 1.39.0!</p>"},{"location":"release-notes/kubespray/","title":"Release Notes","text":""},{"location":"release-notes/kubespray/#compliant-kubernetes-kubespray","title":"Compliant Kubernetes Kubespray","text":"<ul> <li>v2.21.0-ck8s1 - 2023-02-06</li> <li>v2.20.0-ck8s2 - 2022-10-24</li> <li>v2.20.0-ck8s1 - 2022-10-10</li> <li>v2.19.0-ck8s3 - 2022-09-23</li> <li>v2.19.0-ck8s2 - 2022-07-22</li> <li>v2.19.0-ck8s1 - 2022-06-27</li> <li>v2.18.1-ck8s1 - 2022-04-26</li> <li>v2.18.0-ck8s1 - 2022-02-18</li> <li>v2.17.1-ck8s1 - 2021-11-11</li> <li>v2.17.0-ck8s1 - 2021-10-21</li> <li>v2.16.0-ck8s1 - 2021-07-02</li> <li>v2.15.0-ck8s1 - 2021-05-27</li> </ul> <p>Note</p> <p>For a more detailed look check out the full changelog.</p>"},{"location":"release-notes/kubespray/#v2210-ck8s1","title":"v2.21.0-ck8s1","text":"<p>Released 2023-02-06</p>"},{"location":"release-notes/kubespray/#updated","title":"Updated:","text":"<ul> <li>Updated kubespray to <code>v1.21.0</code>      Kubernetes version upgraded to v1.25.6 in which Pod Security Policies (PSPs) are removed. You should not upgrade to this version if you are using PSPs. To deploy Compliant Kubernetes Apps on this version it needs to be on a compatible version which depends on this issue.      This version requires at least terraform version <code>0.14.0</code> in order to provision infrastructure using the kubespray provided terraform modules.</li> <li>Upgraded rook-ceph operator to <code>v1.10.5</code> and ceph to <code>v17.2.5</code>      If you are using the rook-ceph operator you can read the migration docs on how to upgrade these components.</li> </ul>"},{"location":"release-notes/kubespray/#changed","title":"Changed:","text":"<ul> <li>Improved setup for OpenStack with additional server groups      This allows anti-affinity to be set between arbitrary nodes, improving scheduling and stability.</li> <li>Switched from using upstream kubespray repository as submodule to the elastisys fork</li> </ul>"},{"location":"release-notes/kubespray/#added","title":"Added:","text":"<ul> <li>Added a get-requirements file to standardize which terraform version to use, <code>1.2.9</code></li> <li>Added ntp.se as standard ntp server</li> </ul>"},{"location":"release-notes/kubespray/#v2200-ck8s2","title":"v2.20.0-ck8s2","text":"<p>Released 2022-10-24</p>"},{"location":"release-notes/kubespray/#changed_1","title":"Changed:","text":"<ul> <li>Changed a Kubespray variable which is required for upgrading clusters on cloud providers that don't have external IPs on their control plane nodes</li> </ul>"},{"location":"release-notes/kubespray/#v2200-ck8s1","title":"v2.20.0-ck8s1","text":"<p>Released 2022-10-10</p>"},{"location":"release-notes/kubespray/#updated_1","title":"Updated:","text":"<ul> <li>Kubespray updated to <code>v2.20.0</code>      Kubernetes version upgraded to v1.24.6.</li> </ul>"},{"location":"release-notes/kubespray/#changed_2","title":"Changed:","text":"<ul> <li>Scripts are now using yq version 4, this requires <code>yq4</code> as an alias to yq v4</li> </ul>"},{"location":"release-notes/kubespray/#fixed","title":"Fixed:","text":"<ul> <li>Fixed multiple kube-bench fails (01.03.07, 01.04.01, 01.04.02)</li> </ul>"},{"location":"release-notes/kubespray/#v2190-ck8s3","title":"v2.19.0-ck8s3","text":"<p>Released 2022-09-23</p>"},{"location":"release-notes/kubespray/#updated_2","title":"Updated:","text":"<ul> <li>Bumped upcloud csi driver to <code>v0.3.3</code></li> </ul>"},{"location":"release-notes/kubespray/#v2190-ck8s2","title":"v2.19.0-ck8s2","text":"<p>Released 2022-07-22</p>"},{"location":"release-notes/kubespray/#added_1","title":"Added:","text":"<ul> <li>Added option to clusteradmin kubeconfigs to use OIDC for authentication</li> <li>Added New ansible playbooks to manage kubeconfigs and some RBAC</li> </ul>"},{"location":"release-notes/kubespray/#v2190-ck8s1","title":"v2.19.0-ck8s1","text":"<p>Released 2022-06-27.</p>"},{"location":"release-notes/kubespray/#updated_3","title":"Updated:","text":"<ul> <li>Kubespray updated to <code>v2.19.0</code>      Kubernetes version upgraded to 1.23.7.</li> </ul>"},{"location":"release-notes/kubespray/#v2181-ck8s1","title":"v2.18.1-ck8s1","text":"<p>Released 2022-04-26.</p>"},{"location":"release-notes/kubespray/#updated_4","title":"Updated:","text":"<ul> <li>Kubespray updated to <code>v2.18.1</code>      This introduces some fixes for cluster using containerd as container manager.</li> <li>Updated default etcd version to <code>3.5.3</code>     This fixes an issue where etcd data might get corrupted.</li> </ul>"},{"location":"release-notes/kubespray/#v2180-ck8s1","title":"v2.18.0-ck8s1","text":"<p>Released 2022-02-18.</p>"},{"location":"release-notes/kubespray/#updated_5","title":"Updated:","text":"<ul> <li>Kubespray updated to <code>v2.18.0</code>      Kubernetes upgraded to version 1.22.5.     This introduces new features and fixes, including security updates.     There's also a lot of deprecated API's that were removed in this version so take a good look at these notes before upgrading.</li> </ul>"},{"location":"release-notes/kubespray/#v2171-ck8s1","title":"v2.17.1-ck8s1","text":"<p>Released 2021-11-11.</p>"},{"location":"release-notes/kubespray/#updated_6","title":"Updated:","text":"<ul> <li>Kubespray updated to <code>v2.17.1</code>      Kubernetes version upgraded to 1.21.6, this patch is mostly minor fixes.</li> </ul>"},{"location":"release-notes/kubespray/#v2170-ck8s1","title":"v2.17.0-ck8s1","text":"<p>Released 2021-10-21.</p>"},{"location":"release-notes/kubespray/#updated_7","title":"Updated:","text":"<ul> <li>Kubespray updated to <code>v2.17.0</code>      Kubernetes version upgraded to 1.21.5, this introduces new features and fixes, including security updates and storage capacity tracking.</li> </ul>"},{"location":"release-notes/kubespray/#v2160-ck8s1","title":"v2.16.0-ck8s1","text":"<p>Released 2021-07-02.</p>"},{"location":"release-notes/kubespray/#updated_8","title":"Updated:","text":"<ul> <li>Kubespray updated to <code>v2.16.0</code>     Kubernetes version upgraded to 1.20.7, this introduces new features and fixes, including API and component updates.</li> </ul>"},{"location":"release-notes/kubespray/#v2150-ck8s1","title":"v2.15.0-ck8s1","text":"<p>Released 2021-05-27.</p> <p>First stable release!</p>"},{"location":"release-notes/postgres/","title":"Release Notes","text":""},{"location":"release-notes/postgres/#compliant-kubernetes-postgresql","title":"Compliant Kubernetes PostgreSQL","text":"<ul> <li>v1.8.2-ck8s1 - 2022-08-24</li> <li>v1.7.1-ck8s2 - 2022-04-26</li> <li>v1.7.1-ck8s1 - 2021-12-21</li> </ul> <p>Note</p> <p>These are only the user-facing changes.</p>"},{"location":"release-notes/postgres/#v182-ck8s1","title":"v1.8.2-ck8s1","text":"<p>Released 2022-08-24</p> <p>Changes:</p> <ul> <li>Upgraded postgres-operator to version <code>v1.8.2</code></li> <li>Added a service which allows users to port-forward to the service instead of directly to pods</li> </ul>"},{"location":"release-notes/postgres/#v171-ck8s2","title":"v1.7.1-ck8s2","text":"<p>Released 2022-04-26</p> <p>Changes:</p> <ul> <li>Fixed a vulnerability with logical backups</li> </ul>"},{"location":"release-notes/postgres/#v171-ck8s1","title":"v1.7.1-ck8s1","text":"<p>Released 2021-12-21</p> <p>First stable release!</p>"},{"location":"release-notes/rabbitmq/","title":"Release Notes","text":""},{"location":"release-notes/rabbitmq/#compliant-kubernetes-rabbitmq","title":"Compliant Kubernetes RabbitMQ","text":"<ul> <li>v3.10.7-ck8s1 - 2022-09-21</li> <li>v1.11.1-ck8s2 - 2022-06-08</li> <li>v1.11.1-ck8s1 - 2022-03-11</li> <li>v1.7.0-ck8s1 - 2021-12-23</li> </ul> <p>Note</p> <p>These are only the user-facing changes.</p>"},{"location":"release-notes/rabbitmq/#v3107-ck8s2","title":"v3.10.7-ck8s2","text":"<p>Released 2022-11-28</p>"},{"location":"release-notes/rabbitmq/#fixed","title":"Fixed:","text":"<ul> <li>Corrected the Queue Details Grafana dashboard and improved alerting</li> </ul>"},{"location":"release-notes/rabbitmq/#v3107-ck8s1","title":"v3.10.7-ck8s1","text":"<p>Released 2022-09-21</p> <p>Note</p> <p>From this release the version tracks the RabbitMQ server version rather than the RabbitMQ cluster operator version.</p>"},{"location":"release-notes/rabbitmq/#updated","title":"Updated:","text":"<ul> <li>Upgraded RabbitMQ server version to <code>3.10.7</code>      This is a two minor version jump that introduces new upstream features while remaining compatible with current clients.     The most exciting features includes the new Stream queue type tuned for bulk messaging, and much improved efficiency for Quorum and Classic queue types.     See the upstream changelog for more detailed information.</li> </ul>"},{"location":"release-notes/rabbitmq/#added","title":"Added:","text":"<ul> <li>Added support for external access      Using either a LoadBalancer or NodePort Service, additionally with a self-signed chain of trust to enable TLS and host verification.</li> </ul>"},{"location":"release-notes/rabbitmq/#changed","title":"Changed:","text":"<ul> <li>Improved observability      Improved the alerting and replaced the per queue metrics source and dashboard, removing the need for an external exporter.</li> </ul>"},{"location":"release-notes/rabbitmq/#v1111-ck8s2","title":"v1.11.1-ck8s2","text":"<p>Released 2022-06-08</p>"},{"location":"release-notes/rabbitmq/#changed_1","title":"Changed:","text":"<ul> <li>Reworked monitoring      Added additional metrics collection and a new dashboard to show metrics per queue, and fixed those added by the previous release.</li> <li>Tuned performance      Configured and tuned the performance according to RabbitMQ upstream production checklist.     Including better constraints to improve scheduling for redundancy.</li> </ul>"},{"location":"release-notes/rabbitmq/#v1111-ck8s1","title":"v1.11.1-ck8s1","text":"<p>Released 2022-03-11</p>"},{"location":"release-notes/rabbitmq/#updated_1","title":"Updated:","text":"<ul> <li>Upgraded RabbitMQ to version <code>3.8.21</code>      Using Cluster operator version <code>1.11.1</code> providing bugfixes.</li> </ul>"},{"location":"release-notes/rabbitmq/#added_1","title":"Added:","text":"<ul> <li>Added definitions-exporter      Taking daily backups of the RabbitMQ messaging topology and users for quick and easy reconfiguring in case of disaster.</li> </ul>"},{"location":"release-notes/rabbitmq/#changed_2","title":"Changed:","text":"<ul> <li>Reduced RabbitMQ privilege for security.</li> <li>Improved RabbitMQ observability through better monitoring.</li> </ul>"},{"location":"release-notes/rabbitmq/#v170-ck8s1","title":"v1.7.0-ck8s1","text":"<p>Released 2021-12-23</p> <p>First stable release using RabbitMQ version <code>3.8.16</code>!</p>"},{"location":"release-notes/redis/","title":"Release Notes","text":""},{"location":"release-notes/redis/#compliant-kubernetes-redis","title":"Compliant Kubernetes Redis","text":"<ul> <li>v1.1.1-ck8s4 - 2022-12-09</li> <li>v1.1.1-ck8s3 - 2022-10-04</li> <li>v1.1.1-ck8s2 - 2022-08-23</li> <li>v1.1.1-ck8s1 - 2022-03-07</li> <li>v1.0.0-ck8s1 - 2021-12-23</li> </ul> <p>Note</p> <p>These are only the user-facing changes.</p>"},{"location":"release-notes/redis/#v111-ck8s4","title":"v1.1.1-ck8s4","text":"<p>Released 2022-12-09</p> <p>Changes:</p> <ul> <li>Improved alerting and scheduling for better operational management and safety.</li> </ul>"},{"location":"release-notes/redis/#v111-ck8s3","title":"v1.1.1-ck8s3","text":"<p>Released 2022-10-04</p> <p>Changes:</p> <ul> <li>Fixed the safety of replication when master has persistence turned off</li> </ul>"},{"location":"release-notes/redis/#v111-ck8s2","title":"v1.1.1-ck8s2","text":"<p>Released 2022-08-23</p> <p>Changes:</p> <ul> <li>Improved support for running multiple Redis clusters in one Kubernetes environment.</li> </ul>"},{"location":"release-notes/redis/#v111-ck8s1","title":"v1.1.1-ck8s1","text":"<p>Released 2022-03-07</p> <p>Changes:</p> <ul> <li>Upgraded redis-operator to <code>v1.1.1</code></li> </ul>"},{"location":"release-notes/redis/#v100-ck8s1","title":"v1.0.0-ck8s1","text":"<p>Released 2021-12-23</p> <p>First stable release!</p>"},{"location":"user-guide/","title":"The Journey for Application Developers","text":"<p>Head over to Step 1: Prepare to get started on your journey!</p>"},{"location":"user-guide/alerts/","title":"Alerts","text":"<p>Compliant Kubernetes (CK8S) includes alerts via Alertmanager.</p> <p>Important</p> <p>By default, you will get some platform alerts. This may benefit you, by giving you improved \"situational awareness\". Please decide if these alerts are of interest to you or not. Feel free to silence them, as the Compliant Kubernetes administrator will take responsibility for them.</p> <p>Your focus should be on user alerts or application-level alerts, i.e., alerts under the control and responsibility of the Compliant Kubernetes user. We will focus on user alerts in this document.</p>","tags":["ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/alerts/#compliance-needs","title":"Compliance needs","text":"<p>Many regulations require you to have an incident management process. Alerts help you discover abnormal application behavior that need attention. This maps to ISO 27001 \u2013 Annex A.16: Information Security Incident Management.</p>","tags":["ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/alerts/#enabling-user-alerts","title":"Enabling user alerts","text":"<p>User alerts are handled by a project called AlertManager, which needs to be enabled by the administrator. Get in touch with the administrator and they will be happy to help.</p>","tags":["ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/alerts/#configuring-user-alerts","title":"Configuring user alerts","text":"<p>User alerts are configured via the Secret <code>alertmanager-alertmanager</code> located in the <code>alertmanager</code> namespace. This configuration file is specified here.</p> <pre><code># retrieve the old configuration\nkubectl get -n alertmanager secret alertmanager-alertmanager -o jsonpath='{.data.alertmanager\\.yaml}' | base64 -d &gt; alertmanager.yaml\n\n# edit alertmanager.yaml as needed\n\n# patch the new configuration\nkubectl patch -n alertmanager secret alertmanager-alertmanager -p \"'{\\\"data\\\":{\\\"alertmanager.yaml\\\":\\\"$(base64 -w 0 &lt; alertmanager.yaml)\\\"}}'\"\n</code></pre> <p>Make sure to configure and test a receiver for you alerts, e.g., Slack or OpsGenie.</p> <p>Note</p> <p>If you get an access denied error, check with your Compliant Kubernetes administrator.</p>","tags":["ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/alerts/#accessing-user-alertmanager","title":"Accessing user AlertManager","text":"<p>If you want to access AlertManager, for example to confirm that its configuration was picked up correctly, or to configure silences, proceed as follows:</p> <ol> <li>Type: <code>kubectl proxy</code>.</li> <li>Open this link in your browser.</li> </ol>","tags":["ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/alerts/#configuring-alerts","title":"Configuring alerts","text":"<p>Before setting up an alert, you must first collect metrics from your application by setting up either ServiceMonitors or PodMonitors. In general ServiceMonitors are recommended over PodMonitors, and it is the most common way to configure metrics collection.</p> <p>Then create a <code>PrometheusRule</code> following the examples below or the upstream documentation with an expression that evaluates to the condition to alert on. Prometheus will pick them up, evaluate them, and then send notifications to AlertManager.</p> <p>The API reference for Prometheus Operator describes how the Kubernetes resource is configured and the configuration reference for Prometheus describes the rules themselves.</p> <p>In Compliant Kubernetes the Prometheus Operator in the workload cluster is configured to pick up all PrometheusRules, regardless in which namespace they are or which labels they have.</p>","tags":["ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/alerts/#running-example","title":"Running Example","text":"<p>The user demo already includes a PrometheusRule, to configure an alert:</p> <pre><code>{{- if .Values.prometheusRule.enabled -}}\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\nname: {{ include \"ck8s-user-demo.fullname\" . }}\nlabels:\n{{- include \"ck8s-user-demo.labels\" . | nindent 4 }}\nspec:\ngroups:\n- name: ./example.rules\nrules:\n- alert: ApplicationIsActuallyUsed\nexpr: rate(http_request_duration_seconds_count[1m])&gt;1\n{{- end }}\n</code></pre> <p>The screenshot below gives an example of the application alert, as seen in AlertManager.</p> <p></p>","tags":["ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/alerts/#detailed-example","title":"Detailed example","text":"<p>PrometheusRules have two features, either the rules alerts based on expression, or the rules <code>records</code> based on a expression. The former is the way to create alerting rules and the latter is a way to precompute complex queries that will be stored as separate metrics:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\nlabels:\nprometheus: example\nrole: alert-rules\nname: prometheus-example-rules\nspec:\ngroups:\n- name: ./example.rules\n# interval: 30s # optional parameter to configure how often groups of rules are evaluated\nrules:\n- alert: ExampleAlert\nexpr: vector(1)\n# for: 1m # optional parameter to configure how long an alert must be triggered to be fired\nlabels:\nseverity: high\nannotations:\nsummary: \"Example Alert has been fired!\"\ndescription: \"The Example Alert has been fired! It shows the value {{ $value }}.\"\n- record: example_record_metric\nexpr: vector(1)\nlabels:\nrecord: example\n</code></pre> <p>For alert rules labels and annotations can be added or overridden that will become present in the resulting alert notifications, in addition the annotations support Go Templating allowing access to the evaluated value via the <code>$value</code> variable and all labels from the expression using the <code>$labels</code> variable.</p> <p>For recording rules labels can be added or overridden that will become present in the resulting metric.</p>","tags":["ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/backup/","title":"Backups","text":"<p>Compliant Kubernetes (CK8S) includes backup functionality through Velero, a backup tool for Kubernetes Resources and Persistent Volumes. For backup of container images, Harbor is used instead.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","GDPR Art. 17 Right to erasure (\"right to be forgotten\")"]},{"location":"user-guide/backup/#compliance-needs","title":"Compliance needs","text":"<p>The requirements to comply with ISO 27001 are stated in ISO 27001:2013. The annexes that are relevant to backups are:</p> <ul> <li>Annex 12, article A.12.3.1 \"Information Backup\".</li> </ul>","tags":["ISO 27001 A.12.3.1 Information Backup","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","GDPR Art. 17 Right to erasure (\"right to be forgotten\")"]},{"location":"user-guide/backup/#what-is-velero","title":"What is Velero?","text":"<p>Velero is an open source, cloud native tool for backing up and migrating Kubernetes Resources and Persistent Volumes. It has been developed by VMware since 2017. It allows for both manual and scheduled backups, and also allows for subsets of Resources in a cluster to be backed up rather than necessarily backing up everything.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","GDPR Art. 17 Right to erasure (\"right to be forgotten\")"]},{"location":"user-guide/backup/#usage","title":"Usage","text":"<p>The following are instructions for backing up and restoring resources.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","GDPR Art. 17 Right to erasure (\"right to be forgotten\")"]},{"location":"user-guide/backup/#backing-up","title":"Backing up","text":"<p>Compliant Kubernetes takes a daily backup of all Kubernetes Resources in all user namespaces. Persistent Volumes will be backed up if they are tied to a Pod. If backups are not wanted the label <code>compliantkubernetes.io/nobackup</code> can be added to opt-out of the daily backups.</p> <p>Application metrics (Grafana) and application log (Kibana) dashboards are also backup up by default.</p> <p>By default, backups are stored for 720 hours (30 days).</p>","tags":["ISO 27001 A.12.3.1 Information Backup","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","GDPR Art. 17 Right to erasure (\"right to be forgotten\")"]},{"location":"user-guide/backup/#restoring","title":"Restoring","text":"<p>Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first.</p> <p>To restore a backup on demand, contact your Compliant Kubernetes administrator.</p>","tags":["ISO 27001 A.12.3.1 Information Backup","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","GDPR Art. 17 Right to erasure (\"right to be forgotten\")"]},{"location":"user-guide/backup/#protection-of-backups","title":"Protection of Backups","text":"<p>The Compliant Kubernetes administrator will take the following measure to ensure backups are protected:</p> <ol> <li> <p>Backups are encrypted at rest, if the underlying infrastructure provider supports it.</p> <p>Why? This ensures backups remain confidential, even if, e.g., hard drives are not safely disposed.</p> </li> <li> <p>Backups are replicated to an off-site location, if requested. This process is performed from outside the cluster, hence the users -- or attackers gaining access to their application -- cannot access the off-site replicas.</p> <p>Why? This ensures backups are available even if the primary location is subject to a disaster, such as extreme weather. The backups also remain available -- though unlikely confidential -- in case an attacker manages to gain access to the cluster.</p> </li> </ol>","tags":["ISO 27001 A.12.3.1 Information Backup","BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","GDPR Art. 17 Right to erasure (\"right to be forgotten\")"]},{"location":"user-guide/ci-cd/","title":"CI/CD Integration","text":"<p>Compliant Kubernetes does not come with a CI/CD solution. Fortunately, it can be easily integrated with your existing CI/CD solution.</p> <p>Important</p> <p>Access control is an extremely important topic for passing an audit for compliance with data privacy and data security regulations. For example, Swedish patient data law requires all persons to be identified with individual credentials and that logs should capture who did what.</p> <p>Therefore, Compliant Kubernetes has put significant thought into how to do proper access control. As a consequence, CI/CD solutions that require cluster-wide permissions and/or introduce their own notion of access control are highly discouraged. Make sure you thoroughly evaluate your CI/CD solution with your CISO before investing in it.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)"]},{"location":"user-guide/ci-cd/#background","title":"Background","text":"<p>For the purpose of Compliant Kubernetes, one can distinguish between two \"styles\" of CI/CD: push-style and pull-style.</p> <p>Push-style CI/CD -- like GitLab CI or GitHub Actions -- means that a commit will trigger some commands on a CI/CD worker, which will push changes into the Compliant Kubernetes cluster. The CI/CD worker generally runs outside the Kubernetes cluster. Push-style CI/CD solutions should work out-of-the-box and require no special considerations for Compliant Kubernetes.</p> <p>Pull-styles CI/CD -- like ArgoCD or Flux -- means that a special controller is installed inside the cluster, which monitors a Git repository. When a change is detected the controller \"pulls\" changes into the cluster from the Git repository. The special controller often requires considerable permissions and introduces a new notion of access control, which is problematic from a compliance perspective. As shown below, some pull-style CI/CD solutions can be used with Compliant Kubernetes, others not.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)"]},{"location":"user-guide/ci-cd/#push-style-cicd","title":"Push-style CI/CD","text":"<p>Push-style CI/CD works pretty much as if you would access Compliant Kubernetes from your laptop, running <code>kubectl</code> or <code>helm</code> against the cluster, as required to deploy your application. However, for improved access control, the <code>KUBECONFIG</code> provided to your CI/CD pipeline should employ a ServiceAccount which is used only by your CI/CD pipeline. This ServiceAccount should be bound to a Role which gets the least permissions possible. For example, if your application only consists of a Deployment, Service and Ingress, those should be the only resources available to the Role.</p> <p>To create a <code>KUBECONFIG</code> for your CI/CD pipeline, proceed as shown below.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)"]},{"location":"user-guide/ci-cd/#pre-verification","title":"Pre-verification","text":"<p>First, make sure you are in the right namespace on the right cluster:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre> <p>You can only create a Role which is as powerful as you (see Privilege escalation prevention). Therefore, check what permissions you have and ensure they are sufficient for your CI/CD:</p> <pre><code>kubectl auth can-i --list\n</code></pre> <p>Note</p> <p>What permissions you need depends on your application. For example, the user demo creates Deployments, HorizontalPodAutoscalers, Ingresses, PrometheusRules, Services and ServiceMonitors. If unsure, simply continue. RBAC permissions errors are fairly actionable.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)"]},{"location":"user-guide/ci-cd/#create-a-role","title":"Create a Role","text":"<p>Next, create a Role for you CI/CD pipeline. If unsure, start from the example Role that the user demo's CI/CD pipeline needs.</p> <pre><code>kubectl apply -f ci-cd-role.yaml\n</code></pre> <p>Dealing with Forbidden or RBAC permissions errors</p> <p>Error from server (Forbidden): error when creating \"STDIN\": roles.rbac.authorization.k8s.io \"ci-cd\" is forbidden: user \"demo@example.com\" (groups=[\"system:authenticated\"]) is attempting to grant RBAC permissions not currently held:</p> <p>If you get an error like the one above, then it means you have insufficient permissions on the Compliant Kubernetes cluster. Contact your administrator.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)"]},{"location":"user-guide/ci-cd/#create-a-serviceaccount","title":"Create a ServiceAccount","text":"<p>User accounts are for humans, service accounts for robots. See User accounts versus service accounts. Hence, you should employ a ServiceAccount for your CI/CD pipeline.</p> <p>The following command creates a ServiceAccount for your CI/CD pipeline:</p> <pre><code>kubectl create serviceaccount ci-cd\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)"]},{"location":"user-guide/ci-cd/#create-a-rolebinding","title":"Create a RoleBinding","text":"<p>Now create a RoleBinding to bind the CI/CD ServiceAccount to the Role, so as to grant it associated permissions:</p> <pre><code>NAMESPACE=$(kubectl config view --minify --output 'jsonpath={..namespace}')\nkubectl create rolebinding ci-cd --role ci-cd --serviceaccount=$NAMESPACE:ci-cd\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)"]},{"location":"user-guide/ci-cd/#create-a-secret-with-a-token","title":"Create a Secret with a token","text":"<p>Now create a secret for the ServiceAccount that Kubernetes will populate with a token:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ci-cd\n  annotations:\n    kubernetes.io/service-account.name: ci-cd\ntype: kubernetes.io/service-account-token\nEOF\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)"]},{"location":"user-guide/ci-cd/#extract-the-kubeconfig","title":"Extract the KUBECONFIG","text":"<p>You can now extract the <code>KUBECONFIG</code> of the ServiceAccount:</p> <pre><code>SECRET_NAME=ci-cd\n\nserver=$(kubectl config view --minify --output 'jsonpath={..cluster.server}')\ncluster=$(kubectl config view --minify --output 'jsonpath={..context.cluster}')\n\nca=$(kubectl get secret $SECRET_NAME -o jsonpath='{.data.ca\\.crt}')\ntoken=$(kubectl get secret $SECRET_NAME -o jsonpath='{.data.token}' | base64 --decode)\nnamespace=$(kubectl get secret $SECRET_NAME -o jsonpath='{.data.namespace}' | base64 --decode)\n\necho \"\\\napiVersion: v1\nkind: Config\nclusters:\n- name: ${cluster}\n  cluster:\n    certificate-authority-data: ${ca}\n    server: ${server}\ncontexts:\n- name: default-context\n  context:\n    cluster: ${cluster}\n    namespace: ${namespace}\n    user: default-user\ncurrent-context: default-context\nusers:\n- name: default-user\n  user:\n    token: ${token}\n\" &gt; kubeconfig_ci_cd.yaml\n</code></pre> <p>The generated <code>kubeconfig_ci_cd.yaml</code> can then be used in your CI/CD pipeline. Note that, <code>KUBECONFIG</code>s -- especially the token -- must be treated as a secret and injected into the CI/CD pipeline via a proper secrets handing feature, such as GitLab CI's protected variable and GitHub Action's secrets.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)"]},{"location":"user-guide/ci-cd/#example-github-actions","title":"Example: GitHub Actions","text":"<p>Please find a concrete example for GitHub Actions here. Below is the produced output:</p> <p></p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)"]},{"location":"user-guide/ci-cd/#flux-v1","title":"Flux v1","text":"<p>Flux v1 is in maintenance mode and might become obsolete soon.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)"]},{"location":"user-guide/ci-cd/#flux-v2","title":"Flux v2","text":"<p>Flux v2 brings is own notion of access control and requires special considerations to ensure it obey Compliant Kubernetes access control. Installing it can only be done by the administrator of the Compliant Kubernetes cluster, after having made a thorough risk-reward analysis. At the time of this writing, due to these special considerations, we discourage Flux v2.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)"]},{"location":"user-guide/ci-cd/#argocd","title":"ArgoCD","text":"<p>First you need to ask your Kubernetes administrator to install the ArgoCD CRDs for you, and to give you enough permissions to apply the <code>namespace-install.yaml</code> manifest.</p> <p>By default, ArgoCD installs a ClusterRole with wide permissions, which can be used to bypass Compliant Kubernetes's access control. Using it as-is might be non-compliant with various regulations.</p> <p>Instead, use the namespace-install.yaml manifest to install ArgoCD with only namespace level privileges, you may also choose to install ArgoCD HA ha/namespace-install.yaml.</p> <p>As of ArgoCD <code>v2.4.11</code> there are no resource requests set on the containers in the installation manifests. To comply with Gatekeeper rules in the Kubernetes environment, you need to set resource requests manually by editing the <code>namespace-install.yaml</code> manifest. Here are some resource request and limit suggestions based on what previous customers' ArgoCD installations use.</p> <p>NOTE: These resource suggestions may not be optimal for your installation, you may need different tuning based on how you use ArgoCD.</p> applicationset-controllerdexnotifications-controllerredisrepo-serverserverapplication-controller <pre><code>        resources:\nrequests:\nmemory: \"32Mi\"\ncpu: \"5m\"\nlimits:\nmemory: \"64Mi\"\ncpu: \"10m\"\n</code></pre> <pre><code>        resources:\nrequests:\nmemory: \"32Mi\"\ncpu: \"5m\"\nlimits:\nmemory: \"64Mi\"\ncpu: \"10m\"\n</code></pre> <pre><code>        resources:\nrequests:\nmemory: \"32Mi\"\ncpu: \"5m\"\nlimits:\nmemory: \"64Mi\"\ncpu: \"10m\"\n</code></pre> <pre><code>        resources:\nrequests:\nmemory: \"24Mi\"\ncpu: \"30m\"\nlimits:\nmemory: \"48Mi\"\ncpu: \"60m\"\n</code></pre> <pre><code>        resources:\nrequests:\nmemory: \"100Mi\"\ncpu: \"10m\"\nlimits:\nmemory: \"200Mi\"\ncpu: \"60m\"\n</code></pre> <pre><code>        resources:\nrequests:\nmemory: \"50Mi\"\ncpu: \"5m\"\nlimits:\nmemory: \"100Mi\"\ncpu: \"10m\"\n</code></pre> <pre><code>        resources:\nrequests:\nmemory: \"64Mi\"\ncpu: \"25m\"\nlimits:\nmemory: \"128Mi\"\ncpu: \"50m\"\n</code></pre> <p>NOTE: In ArgoCD version <code>v2.4.11</code> there is a role missing in the <code>namespace-install.yaml</code> manifest (non-ha manifest) You need to manually add this role to the manifest:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\nlabels:\napp.kubernetes.io/component: redis\napp.kubernetes.io/name: argocd-redis\napp.kubernetes.io/part-of: argocd\nname: argocd-redis\nrules:\n- apiGroups:\n- security.openshift.io\nresourceNames:\n- nonroot\nresources:\n- securitycontextconstraints\nverbs:\n- use\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)"]},{"location":"user-guide/continous-development/","title":"Continuous Development","text":"<p>When developing on Kubernetes, it can be time-consuming to manually run commands to build new images, push images to a container registry, update Kubernetes manifests, and deploy new manifests to the cluster with each source-code change.</p> <p>Skaffold is a tool that can be used to ease this process. Skaffold will automate the process of building, pushing and deploying new images based on changes to the source-code.</p> <p>Skaffold requires minimal setup as the tool has no cluster-side components, and will automatically detect the configuration to use.</p>"},{"location":"user-guide/continous-development/#installing-skaffold","title":"Installing Skaffold","text":"<p>The Skaffold CLI can be installed by downloading and installing the latest release. Instructions can be found in the Skaffold documentation:</p> <pre><code>curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/v2.0.1/skaffold-linux-amd64 &amp;&amp; \\\nsudo install skaffold /usr/local/bin/\n</code></pre>"},{"location":"user-guide/continous-development/#getting-started-with-skaffold","title":"Getting started with Skaffold","text":"<p>Note</p> <p>Skaffold will use the active <code>KUBECONFIG</code> to authenticate to the Kubernetes cluster.</p> <p>If you haven't done so already, clone the user demo:</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes/\ncd compliantkubernetes/user-demo\n</code></pre>"},{"location":"user-guide/continous-development/#initialize-skaffold","title":"Initialize Skaffold","text":"<pre><code>skaffold init\n</code></pre> <p>This command will scan the current project for images to build and Kubernetes manifests to deploy. For each image that Skaffold finds you will be prompted to specify how to build them, or if they are not built by the current project.</p> <p>The first image Skaffold finds is busybox, however this image is not built from this project, so choose <code>None (image not built from these sources)</code></p> <p></p> <p>The second image Skaffold finds is the user-demo image and this image is built from the Dockerfile.</p> <p></p> <p>Skaffold then asks for which resources we want to create Kubernetes resources, but as the image already has a Helm Chart this can be skipped by pressing enter.</p> <p>Skaffold will then create the <code>skaffold.yaml</code> file containing our configuration. Skaffold will also automatically detect the Helm Chart that deploys the <code>user-demo</code> image.</p> <p>The <code>skaffold.yaml</code> must then be configured to use the correct domain and project for the image (More info). You need push access to this repository. (Optionally add a hostname to access the application).</p> <pre><code>build:\n artifacts:\n- - image: i-didnt-read-the-docs/ck8s-user-demo\n+ - image: &lt;DOMAIN&gt;/&lt;REGISTRY_PROJECT&gt;/ck8s-user-demo\n   docker:\n      dockerfile: Dockerfile\n...\n valuesFiles:\n  - deploy/ck8s-user-demo/values.yaml\n  version: 0.1.0\n+ setValues:\n+   image.repository: &lt;DOMAIN&gt;/&lt;REGISTRY_PROJECT&gt;/ck8s-user-demo\n+   ingress.hostname: demo.&lt;DOMAIN&gt;     # (Optional)\n</code></pre> <p>If the repository is private, a pull secret must be created to use it in Kubernetes, see Configure an Image Pull Secret</p>"},{"location":"user-guide/continous-development/#developing","title":"Developing","text":"<p>To start developing using Skaffold run:</p> <pre><code>skaffold dev\n</code></pre> <p>When you run <code>skaffold dev</code>, Skaffold will first build, and deploy all of the artifacts specified in <code>skaffold.yaml</code>. Skaffold will then begin monitoring all source file dependencies for all artifacts specified in the project and rebuild the associated artifacts and redeploy the new changes to your cluster as changes are made to these source files. So any changes made to the source-files will automatically be updated in the cluster.</p> <p>When starting <code>skaffold dev</code>, the logs of the deployed artifacts will automatically be directed to the console, which makes it easy to debug the application in the cluster.</p> <p>If <code>ingress.hostname</code> was configured previously the application can be accessed from there directly. Otherwise the flag <code>--port-forward</code> can be added to the command, and Skaffold will automatically forward the ports on the application to the local workstation:</p> <pre><code>skaffold dev --port-forward\n</code></pre>"},{"location":"user-guide/continous-development/#application-updates","title":"Application updates","text":"<p>When the application has been built and deployed to the cluster Skaffold shows which URL to access, shows the logs of the application, and starts listening for changes in the source-files.</p> <p></p> <p>When visiting the URL to the application or the portforwarded URL the following output can be seen:</p> <pre><code>{\"hostname\":\"ck8s-user-demo-dd9c58979-rm9rv\",\"version\":\"0.0.1\"}\n</code></pre> <p>If you inside the <code>routes/index.js</code> file add the following:</p> <pre><code>...\nres.send({\n hostname: os.hostname(),\n  version: process.env.npm_package_version,\n+ hello: \"world\"\n});\n...\n</code></pre> <p>And then save the file, Skaffold will automatically detect the change, build a new image, and deploy the new image to the cluster. After the deployment has stabilized, when visiting the same URL, the output is now:</p> <pre><code>{\"hostname\":\"ck8s-user-demo-54bbdcf6fc-gthsc\",\"version\":\"0.0.1\",\"hello\":\"world\"}\n</code></pre>"},{"location":"user-guide/continous-development/#configuration-updates","title":"Configuration updates","text":"<p>To see the amount of pods, run (inside another terminal):</p> <pre><code>$ kubectl get pods\nNAME                              READY   STATUS    RESTARTS   AGE\nck8s-user-demo-7645db4f5c-h4xks   1/1     Running   0          45s\nck8s-user-demo-7645db4f5c-svqfs   1/1     Running   0          35s\n</code></pre> <p>There are two pods running. To change this, edit the file <code>deploy/ck8s-user-demo/values.yaml</code>:</p> <pre><code>- replicaCount: 2\n+ replicaCount: 1\n</code></pre> <p>And save the file, this will also trigger Skaffold to update the deployment. Because the modification only impacts the Kubernetes configuration, the application image does not need to be rebuilt, and a new Helm revision may be deployed right away.</p> <p>Once the deployments have stabilized the amount of pods can be inspected again:</p> <pre><code>$ kubectl get pods\nNAME                              READY   STATUS    RESTARTS   AGE\nck8s-user-demo-7645db4f5c-svqfs   1/1     Running   0          4m45s\n</code></pre>"},{"location":"user-guide/continous-development/#clean-up","title":"Clean-up","text":"<p>To stop Skaffold <code>ctrl + c</code> can be used and will trigger Skaffold to stop listening for changes and clean-up the deployed artifacts from the cluster.</p> <p>The clean-up can also be triggered by running:</p> <pre><code>skaffold delete\n</code></pre>"},{"location":"user-guide/continous-development/#advanced","title":"Advanced","text":"<ul> <li> <p>Skaffold supports multiple different builder, such as Dockerfile, Bazel, Buildpacks or others   (More Info).</p> </li> <li> <p>Skaffold supports copying files to the running containers which will avoid rebuilding of   containers when not needed (More Info).</p> </li> </ul>"},{"location":"user-guide/continous-development/#further-reading","title":"Further Reading","text":"<ul> <li>Skaffold Documentation</li> </ul>"},{"location":"user-guide/delegation/","title":"How to Delegate?","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.9.4.1 Information Access Restriction</li> </ul> <p>Now that you are almost ready to go live, you will certainly want to delegate some permissions to other team members or IT systems in your organization. This page shows you how to do that.</p>","tags":["ISO 27001 A.9.4.1 Information Access Restriction","BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7"]},{"location":"user-guide/delegation/#authentication-vs-access-control","title":"Authentication vs. Access Control","text":"<p>Authentication is the act of proving your identity. Compliant Kubernetes is usually configured to use your organization's Identity Provider (IdP). Examples of supported IdPs includes Google, Active Directory, Okta or Jump Cloud. The email and group provided by your IdP are used for access control in various components.</p> <p>Next sections will explain how to handle access control in each user-facing Compliant Kubernetes component.</p>","tags":["ISO 27001 A.9.4.1 Information Access Restriction","BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7"]},{"location":"user-guide/delegation/#container-registry-harbor","title":"Container registry (Harbor)","text":"<p>Compliant Kubernetes uses Harbor as container registry. For access control, Harbor defines the concepts of:</p> <ul> <li>user and group -- for human access;</li> <li>robot account -- for IT system access.</li> </ul> <p>You don't need to create Harbor users or groups. Compliant Kubernetes configures Harbor in \"OIDC authentication mode\", which means that Harbor will automatically onboard users logging in via your IdP and will automatically get the group from your IdP. In contrast, you need to create robot accounts, as these only exist within Harbor.</p> <p>Your administrator will have configured one of your IdP groups as the \"Harbor system administrator\" group. Please read the upstream documentation linked below to learn how a Harbor admin can:</p> <ul> <li>manage user permissions by role and</li> <li>create robot accounts.</li> </ul> <p>Note</p> <p>You can either add users or groups to a project with various roles. To simplify access control, consider only using groups and assigning users to groups from your IdP.</p>","tags":["ISO 27001 A.9.4.1 Information Access Restriction","BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7"]},{"location":"user-guide/delegation/#kubernetes-api","title":"Kubernetes API","text":"<p>Kubernetes uses the following concepts for access control:</p> <ul> <li>users and groups -- these are provided by your IdP;</li> <li>ServiceAccounts -- these are configured within Kubernetes and are used by IT systems;</li> <li>Roles (and ClusterRoles) -- these define a set of permissions, i.e., allowed API operations;</li> <li>RoleBindings (and ClusterRoleBindings) -- these associate Roles, i.e., a set of permissions, with users, groups or ServiceAccounts.</li> </ul> <p>For delegating permissions to ServiceAccounts, follow the example on the CI/CD page.</p> <p>The next section present delegation to users and groups.</p>","tags":["ISO 27001 A.9.4.1 Information Access Restriction","BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7"]},{"location":"user-guide/delegation/#pre-verification","title":"Pre-verification","text":"<p>First, make sure you are in the right namespace on the right cluster:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre> <p>You can only delegate as much permission as you have (see Privilege escalation prevention). Therefore, check what permissions you have:</p> <pre><code>kubectl auth can-i --list\n</code></pre>","tags":["ISO 27001 A.9.4.1 Information Access Restriction","BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7"]},{"location":"user-guide/delegation/#create-a-role","title":"Create a Role","text":"<p>Next, create a Role capturing the set of permissions you want to delegate. If unsure, start from the example Role that the user demo's CI/CD pipeline needs.</p> <pre><code>kubectl apply -f ci-cd-role.yaml\n</code></pre>","tags":["ISO 27001 A.9.4.1 Information Access Restriction","BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7"]},{"location":"user-guide/delegation/#delegate-to-a-group","title":"Delegate to a Group","text":"<p>Prefer delegating to a group, so that access control is centralized in your IdP.</p> <pre><code>ROLE=my-role     # Role created above\nGROUP=my-group   # As set in your IdP\n\nkubectl create rolebinding $ROLE --role $ROLE --group=$GROUP --dry-run=client -o yaml &gt; my-role-binding.yaml\n# review my-role-binding.yaml\nkubectl apply -f my-role-binding.yaml\n</code></pre>","tags":["ISO 27001 A.9.4.1 Information Access Restriction","BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7"]},{"location":"user-guide/delegation/#add-a-user-admin","title":"Add a User admin","text":"<p>In Compliant Kubernetes v0.21.0 User admins can now add more <code>User admins</code> themselves.</p>","tags":["ISO 27001 A.9.4.1 Information Access Restriction","BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7"]},{"location":"user-guide/delegation/#steps","title":"Steps:","text":"<ol> <li>Edit the clusterrolebinding <code>extra-user-view</code> and add the desired users or groups under <code>subjects</code>. If unsure, look at an example subject from the official kubernetes documentation.</li> </ol> <pre><code>kubectl edit clusterrolebinding user-admin-cluster-wide-delegation\n</code></pre> <ol> <li>In each of your user namespaces that you want the users or groups to be admin in, edit the rolebinding <code>extra-workload-admins</code> and add the desired users or groups under <code>subjects</code>. If you have a root HNC namespace and you want the users or groups to be admin in all of your namespaces, you only need to edit the rolebinding in this root namespace and it will propagate.</li> </ol> <pre><code>kubectl edit rolebinding extra-workload-admins -n user-namespace\n</code></pre>","tags":["ISO 27001 A.9.4.1 Information Access Restriction","BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7"]},{"location":"user-guide/delegation/#application-metrics-grafana","title":"Application Metrics (Grafana)","text":"<p>Your administrator will have mapped your IdP groups to the Grafana viewer, editor and admin roles. Please read the upstream documentation to learn more.</p>","tags":["ISO 27001 A.9.4.1 Information Access Restriction","BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7"]},{"location":"user-guide/delegation/#application-logs-opensearch-dashboards","title":"Application Logs (OpenSearch Dashboards)","text":"<p>Note</p> <p>Compliant Kubernetes is currently migrating from OpenDistro for Elasticsearch to OpenSearch. As a result, Kibana will be replaced with OpenSearch Dashboards. You can track progress here. This section of the documentation will be updated once the migration is completed.</p> <p>TBD</p>","tags":["ISO 27001 A.9.4.1 Information Access Restriction","BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7"]},{"location":"user-guide/demarcation/","title":"Demarcation","text":"<p>TL;DR: You cannot install:</p> <ul> <li>ClusterRoles, ClusterRoleBindings, Roles and RoleBindings</li> <li>CustomResourceDefinitions (CRDs)</li> <li>Roles and RoleBindings</li> <li>PodSecurityPolicies</li> <li>ValidatingWebhookConfiguration, MutatingWebhookConfiguration</li> </ul> <p>This means that generally you cannot deploy Operators.</p> <p>TL;DR: You cannot:</p> <ul> <li>Run containers as root (<code>uid=0</code>)</li> <li>SSH into any Node</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/demarcation/#can-i","title":"Can I?","text":"<p>Compliant Kubernetes comes with a lot of safeguards to ensure you protect your business reputation and earn the trust of your customers. Furthermore, it is a good idea to keep regulators happy, since they bring public trust into digitalization. Public trust is necessary to shift customers away from pen-and-paper to drive usage of your amazing application.</p> <p>If you used Kubernetes before, especially if you acted as a Kubernetes administrator, then being a Compliant Kubernetes user might feel a bit limiting. For example, you might not be able to run containers with root (<code>uid=0</code>) as you were used to. Again, these are not limitations, rather safeguards.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/demarcation/#why","title":"Why?","text":"<p>As previously reported, Kubernetes is not secure by default, nor by itself. This is due to the fact that Kubernetes prefers to keep its \"wow, it just works\" experience. This might be fine for a company that does not process personal data. However, if you are in a regulated industry, for example, because you process personal data or health information, your regulators will be extremely unhappy to learn that your platform does not conform to security best practices.</p> <p>In case of Compliant Kubernetes this implies a clear separation of roles and responsibilities between Compliant Kubernetes users and administrators. The mission of administrators is to make you, the Compliant Kubernetes user, succeed. Besides allowing you to develop features as fast as possible, the administrator also needs to ensure that you build on top of a platform that lives up to regulatory requirements, specifically data privacy and data security regulations.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/demarcation/#general-principle","title":"General Principle","text":"<p>Compliant Kubernetes does not allow users to make any changes which may compromise the security of the platform. This includes compromising or working around access control, logging, monitoring, backups, alerting, etc. For example, accidental deletion of the CustomResourceDefinitions of Prometheus would prevent administrators from getting alerts and fixing cluster issues before your application is impacted. Similarly, accidentally deleting fluentd Pods would make it impossible to capture the Kubernetes audit log and investigate data breaches.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/demarcation/#specifics","title":"Specifics","text":"<p>To stick to the general principles above, Compliant Kubernetes puts the following technical safeguards. This list may be updated in the future to take into account the fast evolving risk and technological landscape.</p> <p>More technically, Compliant Kubernetes does not allow users to:</p> <ul> <li>change the Kubernetes API through CustomResourceDefinitions or Dynamic Webhooks;</li> <li>gain more container execution permissions by mutating PodSecurityPolicies; this implies that you cannot run container images as root or mount hostPaths;</li> <li>mutate ClusterRoles or Roles so as to escalate privileges;</li> <li>mutate Kubernetes resources in administrator-owned namespaces, such as <code>monitoring</code> or <code>kube-system</code>;</li> <li>re-configure system Pods, such as Prometheus or fluentd;</li> <li>access the hosts directly.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/demarcation/#but-what-if-i-really-need-to","title":"But what if I really need to?","text":"<p>Unfortunately, many application asks for more permissions than Compliant Kubernetes allows by default. When looking at the Kubernetes resources, the following are problematic:</p> <ul> <li>ClusterRoles, ClusterRoleBindings</li> <li>Too permissive Roles and RoleBindings</li> <li>PodSecurityPolicy and/or use of <code>privileged</code> PodSecurityPolicy</li> <li>CustomResourceDefinitions</li> <li>WebhookConfiguration</li> </ul> <p>In such a case, ask your administrator to make a risk-reward analysis. As long as they stick to the general principles, this should be fine. However, as much as they want to help, they might not be allowed to say \"yes\". Remember, administrators are there to help you focus on application development, but at the same time they are responsible to protect your application against security risks.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/deploy/","title":"Step 2: Deploy","text":"<p>Hello again, Application Developer! In this step, we will walk you through what is needed to deploy your application on Elastisys Compliant Kubernetes.</p>"},{"location":"user-guide/deploy/#demo-application-available","title":"Demo Application Available","text":"<p>In case you are just reading along, or do not already have a containerized application prepared, we have developed a demo application which allows you to quickly explore the benfits of Elastisys Compliant Kubernetes.</p> <p>The provided artifacts, including Dockerfile and Helm Chart, allow you to quickly get started on your journey to become an agile organization with zero compromise on compliance with data protection regulations.</p> <p>We have versions of it for Node JS and .NET available. You will note that once built and containerized, they deploy exactly the same.</p>"},{"location":"user-guide/deploy/#push-your-container-images","title":"Push Your Container Images","text":""},{"location":"user-guide/deploy/#configure-container-registry-credentials","title":"Configure container registry credentials","text":"<p>First, retrieve your Harbor CLI secret and configure your local Docker client.</p> <ol> <li>In your browser, type <code>harbor.$DOMAIN</code> where <code>$DOMAIN</code> is the information you retrieved from your administrator.</li> <li>Log into Harbor using Single Sign-On (SSO) via OpenID.</li> <li>In the right-top corner, click on your username, then \"User Profile\".</li> <li>Copy your CLI secret.</li> <li>Now log into the container registry: <code>docker login harbor.$DOMAIN</code>.</li> <li>You should see <code>Login Succeeded</code>.</li> </ol>"},{"location":"user-guide/deploy/#create-a-registry-project","title":"Create a registry project","text":"<p>Example</p> <p>Here is an example Dockerfile and .dockerignore to get you started. Don't forget to run as non-root.</p> <p>If you haven't already done so, create a project called <code>demo</code> via the Harbor UI, which you have accessed in the previous step.</p>"},{"location":"user-guide/deploy/#clone-the-user-demo","title":"Clone the user demo","text":"<p>If you haven't done so already, clone the user demo:</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes/\ncd compliantkubernetes/user-demo\n</code></pre>"},{"location":"user-guide/deploy/#build-and-push-the-image","title":"Build and push the image","text":"<pre><code>REGISTRY_PROJECT=demo  # Name of the project, created above\nTAG=v1                 # Container image tag\n\ndocker build -t harbor.$DOMAIN/$REGISTRY_PROJECT/ck8s-user-demo:$TAG .\ndocker push harbor.$DOMAIN/$REGISTRY_PROJECT/ck8s-user-demo:$TAG\n</code></pre> <p>You should see no error message. Note down the <code>sha256</code> of the image.</p>"},{"location":"user-guide/deploy/#verification","title":"Verification","text":"<ol> <li>Go to <code>harbor.$DOMAIN</code>.</li> <li>Choose the <code>demo</code> project.</li> <li>Check if the image was uploaded successfully, by comparing the tag's <code>sha256</code> with the one returned by the <code>docker push</code> command above.</li> <li>(Optional) While you're at it, why not run the vulnerability scanner on the image you just pushed.</li> </ol>"},{"location":"user-guide/deploy/#deploy-your-application","title":"Deploy Your Application","text":""},{"location":"user-guide/deploy/#pre-verification","title":"Pre-verification","text":"<p>Make sure you are in the right namespace on the right cluster:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre>"},{"location":"user-guide/deploy/#configure-an-image-pull-secret","title":"Configure an Image Pull Secret","text":"<p>To start, make sure you configure the Kubernetes cluster with an image pull secret. Ideally, you should create a container registry Robot Account, which only has pull permissions and use its token.</p> <p>Important</p> <p>Using your own registry credentials as an image pull secret, instead of creating a robot account, is against best practices and may violate data privacy regulations.</p> <p>Your registry credentials identify you and allow you to both push and pull images. A robot account should identify the Kubernetes cluster and be only allowed to pull images.</p> <pre><code>DOCKER_USER='robot$name'       # enter robot account name\nDOCKER_PASSWORD=               # enter robot secret\n</code></pre> <p>Now create a pull secret and (optionally) use it by default in the current namespace.</p> <pre><code># Create a pull secret\nkubectl create secret docker-registry pull-secret \\\n--docker-server=harbor.$DOMAIN \\\n--docker-username=$DOCKER_USER \\\n--docker-password=$DOCKER_PASSWORD\n\n# Set default pull secret in current namespace\nkubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"pull-secret\"}]}'\n</code></pre> <p>Note</p> <p>For each Kubernetes namespace, you will have to create an image pull secret and configure it to be default. Aim to have a one-to-one-to-one mapping between Kubernetes namespaces, container registry projects and robot accounts.</p>"},{"location":"user-guide/deploy/#deploy-user-demo","title":"Deploy user demo","text":"<p>Example</p> <p>Here is an example Helm Chart to get you started.</p> <p>If you haven't done so already, clone the user demo and ensure you are in the right folder:</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes/\ncd compliantkubernetes/user-demo\n</code></pre> <p>Ensure you use the right registry project and image tag, i.e., those that you pushed in the previous example</p> <pre><code>REGISTRY_PROJECT=demo\nTAG=v1\n</code></pre> <p>You are ready to deploy the application.</p> <pre><code>helm upgrade \\\n--install \\\nmyapp \\\ndeploy/ck8s-user-demo/ \\\n--set image.repository=harbor.$DOMAIN/$REGISTRY_PROJECT/ck8s-user-demo \\\n--set image.tag=$TAG \\\n--set ingress.hostname=demo.$DOMAIN\n</code></pre>"},{"location":"user-guide/deploy/#verification_1","title":"Verification","text":"<p>Verify that the application was deployed successfully:</p> <pre><code>kubectl get pods\n# Wait until the status of your Pod is Running.\n</code></pre> <p>Verify that the certificate was issued successfully:</p> <pre><code>kubectl get certificate\n# Wait until your certificate shows READY True.\n</code></pre> <p>Verify that your application is online. You may use your browser or <code>curl</code>:</p> <pre><code>curl --include https://demo.$DOMAIN\n# First line should be HTTP/2 200\n</code></pre> <p>Do not expose <code>$DOMAIN</code> to your users.</p> <p>Although your administrator will set <code>*.$DOMAIN</code> to point to your applications, prefer to buy a branded domain. For example, register the domain <code>myapp.com</code> and point it via a CNAME or ALIAS record to <code>myapp.$DOMAIN</code>.</p>"},{"location":"user-guide/deploy/#view-application-logs","title":"View Application Logs","text":"<p>The user demo application already includes structured logging: For each HTTP request, it logs the URL, the user agent, etc. Compliant Kubernetes further adds the Pod name, Helm Chart name, Helm Release name, etc. to each log entry.</p> <p>The screenshot below gives an example of log entries produced by the user demo application. It was obtained by using the index pattern <code>kubernetes*</code> and the filter <code>kubernetes.labels.app_kubernetes_io/instance:myapp</code>.</p> <p></p> <p>Note</p> <p>You may want to save frequently used searches as dashboards. Compliant Kubernetes saves and backs these up for you.</p>"},{"location":"user-guide/deploy/#next-step-operating","title":"Next step? Operating!","text":"<p>Now that you have deployed your containerized application and know how to look at its logs, what's next? Head over to the next step, where you learn how to operate and monitor it!</p>"},{"location":"user-guide/faq/","title":"Application Developer FAQ","text":"","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"]},{"location":"user-guide/faq/#why-cant-i-kubectl-run","title":"Why can't I <code>kubectl run</code>?","text":"<p>To increase security, Compliance Kubernetes does not allow by default to run containers as root. Additionally, the container image is not allowed to be pulled from a public docker hub registry and all Pods are required to be selected by some NetworkPolicy. This ensures that an active decision has been made for what network access the Pod should have and helps avoid running \"obscure things found on the internet\".</p> <p>Considering the above, you should start by pushing the container image you want to use to Harbor and make sure it doesn't run as root. See this document for how to use OIDC with docker. With that in place, you will need to create a NetworkPolicy for the Pod you want to run. Here is an example of how to create a NetworkPolicy that allows all TCP traffic (in and out) for Pods with the label <code>run: blah</code>.</p> <p>Note</p> <p>This is just an example, not a good idea! You should limit the policy to whatever your application really needs.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: blah\nspec:\n  podSelector:\n    matchLabels:\n      run: blah\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  # Allow all incoming traffic\n  - {}\n  egress:\n  # Allow all outgoing traffic\n  - {}\nEOF\n</code></pre> <p>Now you are ready to run a Pod! Make sure you match the name with the label you used for the NetworkPolicy. Kubectl will automatically set the label <code>run: &lt;name-of-pod&gt;</code> when you create a Pod with <code>kubectl run &lt;name-of-pod&gt;</code>. Here is an example command (please replace the <code>$MY_HARBOR_IMAGE</code>):</p> <pre><code>kubectl run blah --rm -ti --image=$MY_HARBOR_IMAGE\n</code></pre> <p>If your image runs as root by defaults, but can handle running as another user, you may override the user by adding a flag like this to the above command:</p> <pre><code>--overrides='{ \"spec\": { \"securityContext\": \"runAsUser\": 1000, \"runAsGroup\": 1000 } }'\n</code></pre>","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"]},{"location":"user-guide/faq/#how-do-i-give-access-to-a-new-application-developer-to-a-compliant-kubernetes-environment","title":"How do I give access to a new application developer to a Compliant Kubernetes environment?","text":"<p>Add the new user to the correct group via your Identity Provider (IdP), and Compliant Kubernetes will automatically pick it up.</p> <p>Feeling lost? To find out what users and groups currently have access to your Compliant Kubernetes environment, type:</p> <pre><code>kubectl get rolebindings.rbac.authorization.k8s.io workload-admin -o yaml\n# look at the 'subjects' field\n</code></pre> <p>If you are not using groups, contact your administrator.</p>","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"]},{"location":"user-guide/faq/#how-do-i-add-a-new-namespace","title":"How do I add a new namespace?","text":"<p>See Namespaces.</p>","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"]},{"location":"user-guide/faq/#why-cant-i-access-my-cluster-bad-request-unregistered-redirect_uri-httplocalhost18000","title":"Why can't I access my cluster? 'Bad Request Unregistered redirect_uri (\"http://localhost:18000\").'","text":"<p>Port 8000 is the only allowed port for OpenID callback URL and is needed by the <code>kubectl</code> OpenID plugin. If that port is used locally, then <code>kubectl</code> will try to bind to port 18000 which is not allowed due to security concerns. Make sure that nothing is running locally that is using port 8000.</p>","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"]},{"location":"user-guide/faq/#select-your-tenant","title":"Select your tenant?","text":"<p>If you are using Compliant Kubernetes <code>&lt;v0.26.0</code> then you will likely see this popup when logging in to Opensearch.</p> <p></p> <p>We have disabled multi tenancy in Opensearch and there is a bug in older versions of Opensearch which makes this popup appear when multi tenancy is disabled. It has been fixed in a newer version of Opensearch which is included in Compliant Kubernetes version <code>v0.26.0</code>.</p> <p>So if you are using Compliant Kubernetes <code>&lt;v0.26.0</code> then please just click <code>Cancel</code> or <code>X</code> and continue to use Opensearch as you would.</p>","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"]},{"location":"user-guide/faq/#what-is-encrypted-at-rest","title":"What is encrypted at rest?","text":"<p>Compliant Kubernetes encrypts everything at rest, including Kubernetes resources, PersistentVolumeClaims, logs, metrics and backups, if the underlying cloud provider supports it.</p> <p>Get in touch with your administrator to check the status. They are responsible for performing a provider audit.</p> <p>Why does Compliant Kubernetes not offer encryption-at-rest at the platform level?</p> <p>TL;DR: operational scalability and to avoid security theatre.</p> <p>We are frequently asked why we don't simply do full-disk encryption at the VM level, using something like cryptsetup. Let us explain our rationale.</p> <p>The reason why people want encryption-at-rest is to add another safeguard to data confidentiality. Encryption-at-rest is a must-have for laptops, as they can easily be stolen or get lost. However, it is a nice-to-have addition for servers, which are supposed to be in a physically protected data-center, with disks being safely disposed. This is verified during a provider audit.</p> <p>At any rate, if encryption-at-rest is deployed it must: (a) actually safeguard data confidentiality; (b) without prohibitive costs in terms of administration.</p> <p>A Compliant Kubernetes environment may comprise as many as 10 Nodes, i.e., VMs. These Nodes need to be frequently rebooted, to ensure Operating System (OS) security patches are applied. This is especially important for Linux kernel, container runtime (Docker) and Kubernetes security patches. Thanks to the power of Kubernetes, a carefully engineered and deployed application can tolerate such reboots with zero downtime. (See the go-live checklist.)</p> <p>The challenge is how to deliver the disk encryption key to the VM when they are booting. Let us explore a few options:</p> <ul> <li> <p>Non-option 1: Store the encryption key on the VM's <code>/boot</code> disk. This is obvious security theatre. For example, if server disks are stolen, the VM's data is in the hands of the thiefs.</p> </li> <li> <p>Non-option 2: Let admins type the encryption key on the VM's console. Asking admins to do this is time-consuming, error-prone, effectivly jeopardizing uptime. Instead, Compliant Kubernetes recommends automatic VM reboots during application \"quiet times\", such as at night, to ensure the OS is patched without sacrificing uptime.</p> </li> <li> <p>Non-option 3: Let the VM pull the encryption key via instance metadata or instance configuration. This would imply storing the encryption key on the cloud provider. If the cloud provider doesn't have encryption-at-rest, then the encryption key is also stored unencrypted, likely on the same server as the VM is running. Hence, this quickly ends up being security theatre.</p> </li> <li> <p>Non-option 4: Let the VM pull the encryption key from an external location which features encryption-at-rest. This would imply that the VM needs some kind of credentials to authenticate to the external location. Again these credentials are stored unencrypted on the cloud provider, so we are back to non-option 3.</p> </li> </ul> <p>Okay, so what is the real option, then?</p> <p>The only real option is to rely on support from the cloud provider. The latest generation (physical) servers feature a TPM to store the disk encryption key. This can be securely release to the Linux kernel thanks to pre-boot authentication. This process is performance-neutral and fully transparent to the VMs running on top of the servers.</p> <p>And that is why Compliant Kubernetes encrypts everything at rest, only if the underlying cloud provider supports it.</p>","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"]},{"location":"user-guide/go-live/","title":"Go-live Checklist","text":"<p>The administrator set up a shiny new Compliant Kubernetes environment. You containerized your application, deployed it, configured a working CI/CD pipeline, configured application alerts, etc. etc. All seems fine, but somehow you feel anxious about going into production 24/7.</p> <p>To move from production anxiety to production karma, here is a checklist to go through before going live 24/7. Make sure to perform this checklist in a shared session with the administrator.</p> <ul> <li> Load testing was performed.<ul> <li>Why? This ensures that enough capacity was allocated for the environment.</li> <li>How? Set up a synthetic workload generator or replay a relevant workload. Ask the administrator to monitor your environment's capacity usage, including that related to components necessary for application logs and application metrics.</li> <li>Desired outcome: Allocated capacity is sufficient.</li> <li>Possible resolution: Ensure the application has proper resource requests and limits (see our user demo as an example).</li> </ul> </li> <li> Load testing was performed while updating the application.<ul> <li>Why? This ensures that the application can be updated without downtime.</li> <li>How? Make a trivial change to your application, e.g., add \"Lorem ipsum\" in the output of some API, and redeploy.</li> <li>Desired outcome: Measured downtime is acceptable.</li> <li>Possible resolutions: Make sure you have the right deployment strategy. Prefer <code>RollingUpdate</code> over <code>Recreate</code>. Ensure other parameters of the deployment strategy are tuned as needed.</li> </ul> </li> <li> Load testing was performed while doing a rolling reboot of Nodes:<ul> <li>Why? Node failure may cause application downtime. Said downtime can be large if it happens at night, when administrators need to wake up before they can respond. Also, administrators need some extra capacity for performing critical security updates on the base operating system of the Nodes.</li> <li>How? As above, but now ask the administrator to perform a rolling reboot of Nodes.</li> <li>Desired outcome: The measured downtime (due to Pod migration) during Node failure or drain is acceptable. Capacity is sufficient to tolerate one Node failure or drain.</li> <li>Possible resolution:<ul> <li>Ensure the application has proper resource requests and limits (see our user demo for an example).</li> <li>Ensure the application has at least two replicas (see our user demo for an example).</li> <li>Ensure the application has <code>topologySpreadConstraints</code> to ensure Pods do not end up on the same Node (see our user demo for an example).</li> </ul> </li> </ul> </li> <li> [For multi-Zone environments] Load testing was performed while failing an entire Zone:<ul> <li>Why? If a multi-Zone environment was requested, then the additional resilience must be tested. Otherwise, Zone failure may cause application downtime.</li> <li>How? As above, but now ask the administrator to fail an entire Zone.</li> <li>Desired outcome: The measured downtime (due to Pod migration) during Zode failure is acceptable. Capacity is sufficient to tolerate one Zode failure.</li> <li>Possible resolution:<ul> <li>Ensure the application has proper resource requests and limits (see our user demo for an example).</li> <li>Ensure the application has at least two replicas (see our user demo for an example).</li> <li>Ensure the application has <code>topologySpreadConstraints</code> to ensure Pods do not end up on the same Zone (see our user demo for an example).</li> </ul> </li> </ul> </li> <li> Disaster recovery testing was performed:<ul> <li>Why? This ensures that the application and platform team agreed on who backs up what, instead of ending up thinking that \"backing up this thingy\" is the other team's problem.</li> <li>How? Ask the administrator to destroy the environment and restore from off-site backups. Check if your application is back up and its data is restored as expected.</li> <li>Desired outcome: Measured recovery point and recovery time is acceptable.</li> <li>Possible resolution: Ensure you store application either in PersistentVolumes -- these are backed up by default in Compliant Kubernetes -- or a managed database hosted inside Compliant Kubernetes.</li> </ul> </li> <li> Redeployment of the application from scratch works.<ul> <li>Why? This ensures that no tribal knowledge exists and your Git repository is truly the only source of truth.</li> <li>How? Ask your administrator to \"reset\" the environment, i.e., remove all container images, remove all cached container images, remove all Kubernetes resources, etc. Redeploy your application.</li> <li>Desired outcome: Measured setup time is acceptable.</li> <li>Possible resolutions: Make sure to add all code and Kubernetes manifests to your Git repository. Make sure that relevant documentation exists.</li> </ul> </li> </ul>","tags":["ISO 27001 A.17.1.3 Verify, Review & Evaluate Information Security Continuity","HIPAA S26 - Contingency Plan - Testing and Revision Procedure - \u00a7 164.308(a)(7)(ii)(D)"]},{"location":"user-guide/how-many-environments/","title":"How Many Environments?","text":"<p>Many regulations require strict separation between testing and production environments. In particular, production data should not be compromised, no matter what happens in testing environments. Therefore, Compliant Kubernetes recommends setting up at least two environments:</p> <ul> <li>staging;</li> <li>production.</li> </ul> <p>However, the exact number of environments will depend on your needs. Please use the two figures below to reason about environments, trading developer productivity and data security:</p> <p></p> <p></p>","tags":["ISO 27001 A.12.1.4 Separation of Development, Testing & Operational Environments","HIPAA S12 - Information Access Management - Isolating Healthcare Clearinghouse Functions - \u00a7 164.308(a)(4)(ii)(A)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","HSLF-FS 2016:40 3 kap. 10 \u00a7 Upphandling och utveckling"]},{"location":"user-guide/kubernetes-api/","title":"Kubernetes API","text":"<p>The Kubernetes API is the entrypoint to managing your Kubernetes resources. Your Compliant Kubernetes administrator will provide you with a kubeconfig file upon onboarding, which is required to access the API.</p> <p>The following sections describe how to access the cluster in order to manage your Kubernetes resources.</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/kubernetes-api/#authentication-and-access-control-in-compliant-kubernetes","title":"Authentication and Access Control in Compliant Kubernetes","text":"<p>In order to facilitate access control and audit logging, Compliant Kubernetes imposes a certain way to access the Kubernetes API. The kubeconfig file provides individual access to the Kubernetes API through dex. Normally, you should authenticate using your organizations identity provider connected to dex, but it is also possible for your administrator to configure static usernames and passwords.</p> <p>The authorization is done by the Kubernetes API based on Kubernetes role-based access controls. Your cluster administrator will grant you permissions as part of onboarding. You have administrator access to the user workload Kubernetes Namespaces by default. In order to follow the principle of least privilege, you as an user should only have sufficient access to manage resources required by your application. User access to the Kubernetes API may need to be restricted from case to case to follow the principle of least privilege.</p> <p>Note</p> <p>Regardless of your privilege, you will not be able to see components such as Harbor and Elasticsearch via the Kubernetes API. This is in order to comply with common logging policies, which requires logging to be sent to a tamper-proof environment. The tamper-proof environment needs to be separated from the production cluster.</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/kubernetes-api/#usage-guide","title":"Usage guide","text":"<p>This section focuses on using the kubeconfig.</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/kubernetes-api/#using-the-kubeconfig-file","title":"Using the kubeconfig file","text":"<p>The kubeconfig file can be used with <code>kubectl</code> by:</p> <ul> <li>Setting and exporting the <code>KUBECONFIG</code> environment variable:</li> </ul> <p></p> <ul> <li>Merging the configuration with your existing kubeconfig file, see Kubernetes documentation on merging kubeconfig files.</li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/kubernetes-api/#authenticating-to-the-kubernetes-api","title":"Authenticating to the Kubernetes API","text":"<p>To authenticate to the Kubernetes API, run a <code>kubectl</code> command. The <code>oidc-login</code> plugin will launch a browser where you log in to the cluster:</p> <p></p> <p>This page contains the authentication options provided by your administrator. Select your log in method and log in:</p> <p></p> <p>Once you have logged in through the browser, you are authenticated to the cluster:</p> <p></p> <p>Your credentials will then be used by the Kubernetes API to make sure you are authorized. You are now logged in and can use kubectl to manage your Kubernetes resources!</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/kubernetes-api/#running-example","title":"Running Example","text":"","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/kubernetes-api/#pre-verification","title":"Pre-verification","text":"<p>Make sure you are in the right namespace on the right cluster:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/kubernetes-api/#configure-an-image-pull-secret","title":"Configure an Image Pull Secret","text":"<p>To start, make sure you configure the Kubernetes cluster with an image pull secret. Ideally, you should create a container registry Robot Account, which only has pull permissions and use its token.</p> <p>Important</p> <p>Using your own registry credentials as an image pull secret, instead of creating a robot account, is against best practices and may violate data privacy regulations.</p> <p>Your registry credentials identify you and allow you to both push and pull images. A robot account should identify the Kubernetes cluster and be only allowed to pull images.</p> <pre><code>DOCKER_USER='robot$name'       # enter robot account name\nDOCKER_PASSWORD=               # enter robot secret\n</code></pre> <p>Now create a pull secret and (optionally) use it by default in the current namespace.</p> <pre><code># Create a pull secret\nkubectl create secret docker-registry pull-secret \\\n--docker-server=harbor.$DOMAIN \\\n--docker-username=$DOCKER_USER \\\n--docker-password=$DOCKER_PASSWORD\n\n# Set default pull secret in current namespace\nkubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"pull-secret\"}]}'\n</code></pre> <p>Note</p> <p>For each Kubernetes namespace, you will have to create an image pull secret and configure it to be default. Aim to have a one-to-one-to-one mapping between Kubernetes namespaces, container registry projects and robot accounts.</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/kubernetes-api/#deploy-user-demo","title":"Deploy user demo","text":"<p>Example</p> <p>Here is an example Helm Chart to get you started.</p> <p>If you haven't done so already, clone the user demo and ensure you are in the right folder:</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes/\ncd compliantkubernetes/user-demo\n</code></pre> <p>Ensure you use the right registry project and image tag, i.e., those that you pushed in the previous example</p> <pre><code>REGISTRY_PROJECT=demo\nTAG=v1\n</code></pre> <p>You are ready to deploy the application.</p> <pre><code>helm upgrade \\\n--install \\\nmyapp \\\ndeploy/ck8s-user-demo/ \\\n--set image.repository=harbor.$DOMAIN/$REGISTRY_PROJECT/ck8s-user-demo \\\n--set image.tag=$TAG \\\n--set ingress.hostname=demo.$DOMAIN\n</code></pre>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/kubernetes-api/#verification","title":"Verification","text":"<p>Verify that the application was deployed successfully:</p> <pre><code>kubectl get pods\n# Wait until the status of your Pod is Running.\n</code></pre> <p>Verify that the certificate was issued successfully:</p> <pre><code>kubectl get certificate\n# Wait until your certificate shows READY True.\n</code></pre> <p>Verify that your application is online. You may use your browser or <code>curl</code>:</p> <pre><code>curl --include https://demo.$DOMAIN\n# First line should be HTTP/2 200\n</code></pre> <p>Do not expose <code>$DOMAIN</code> to your users.</p> <p>Although your administrator will set <code>*.$DOMAIN</code> to point to your applications, prefer to buy a branded domain. For example, register the domain <code>myapp.com</code> and point it via a CNAME or ALIAS record to <code>myapp.$DOMAIN</code>.</p> <p>Use <code>topologySpreadConstraints</code> if you want cross-data-center resilience</p> <p>If you want your application to tolerate a whole zone (data-center) to go down, you need to add <code>topologySpreadConstraints</code> by uncommenting the relevant section in values.yaml.</p> <p>In order for this to work, your administrator must configure the Nodes with zone labels. You can verify if this was performed correctly typing <code>kubectl get nodes --show-labels</code> and checking if Nodes feature the <code>topology.kubernetes.io/zone</code> label.</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/kubernetes-api/#further-reading","title":"Further reading","text":"<ul> <li>dex on GitHub</li> <li>oidc-login/kubelogin on GitHub</li> <li>Organizing Cluster Access Using kubeconfig Files </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"user-guide/kubernetes-ui/","title":"Kubernetes UI (Lens)","text":"<p>Important</p> <p>As of September 2022, the Lens binary includes proprietary code on top of the open-source Lens source code. Unfortunately, some of that code includes non-features, like requiring a Lens ID for \"activating\" Lens and tracking your activity.</p> <p>We find such practices questionable, especially in highly-regulated industries. Therefore, we currently recommend using OpenLens, which is a \"clean\" binary of the Lens source code. OpenLens consists of all the open-source Lens code without the prioprietary code. Even better, OpenLens does not require \"activation\" and does not track your actions.</p> <p>The Compliant Kubernetes project monitors the situation and will decide on permanently switching to OpenLens or finding a suitable replacement, like the VSCode plugin for Kubernetes.</p> <p>Lens is a graphical user interface that you install locally on your machine. It provides an attractive and easy to use way of interacting with your Kubernetes cluster. Similar to <code>kubectl</code>, it can be used to manage multiple clusters. You can also install extensions that help you manage your clusters or applications deployed on them from a growing list of community-provided extensions.</p> <p>Because Lens is installed locally, and has no cluster-side component, it uses the exact same permissions as your Compliant Kubernetes user has. This makes it a perfectly safe and secure user interface to use and does not compromise your cluster's stability or security posture.</p>"},{"location":"user-guide/kubernetes-ui/#installing-lens","title":"Installing Lens","text":"<p>Head over to the official Lens website and download the appropriate installation package for your operating system. Follow the instructions, and you should have Lens installed.</p> <p>On Debian or Ubuntu-based systems, once you've downloaded the <code>.deb</code> file for your architecture, you simply run the following command (assuming you called the downloaded file <code>lens.deb</code>):</p> <pre><code>sudo dpkg -i lens.deb\n</code></pre>"},{"location":"user-guide/kubernetes-ui/#note-for-macos-and-linux-users","title":"Note for macOS and Linux users","text":"<p>If you followed the Install Prerequisites steps of this documentation, you have probably installed the <code>oidc-login</code> plugin to <code>kubectl</code> via <code>krew</code>. If so, Lens will not be able to find it. That makes Lens fail to authenticate via Dex, the OpenID Connect provider in Compliant Kubernetes.</p> <p>You have two options for making the <code>oidc-login</code> plugin findable by Lens:</p> <ol> <li> <p>Edit <code>~/.profile</code> and add:</p> <pre><code>if [ -d \"$HOME/.krew/bin\" ] ; then\n    PATH=\"$HOME/.krew/bin:$PATH\"\nfi\n</code></pre> </li> <li> <p>Run the following command:</p> </li> </ol> <pre><code>sudo ln -s ~/.krew/bin/kubectl-oidc_login /usr/local/bin\n</code></pre>"},{"location":"user-guide/kubernetes-ui/#getting-started-with-lens","title":"Getting started with Lens","text":""},{"location":"user-guide/kubernetes-ui/#connecting-to-a-cluster","title":"Connecting to a cluster","text":"<p>When Lens starts up, it will have located all the <code>kubectl</code> configuration files that are in your default directory (<code>$HOME/.kube</code>) and presents these in a list for you as clusters you can connect to.</p> <p></p> <p>If you choose a cluster on this screen, you can then connect to it by hitting the big image for it above the \"Click to open\" text:</p> <p></p> <p>Authentication with Dex does not happen within Lens itself, but it starts the correct component and will show the following screen:</p> <p></p> <p>Go to the address it says, i.e., localhost:8000 to authenticate via Dex:</p> <p></p> <p>Log in as usual with your identity provider and return to Lens. Hit the Reconnect button, and you're in!</p>"},{"location":"user-guide/kubernetes-ui/#cluster-settings","title":"Cluster settings","text":"<p>When you view your cluster's overview page, it comes up with an empty loading icon. That is because your user account does not have the permissions to view all Pods in all Namespaces. Lens assumes you do, but you do not.</p> <p>To remedy this, we can tell Lens which Namespaces to pull data from. Going back to the cluster selection screen, choose the Pen icon to configure the cluster's settings.</p> <p>You can then go to the setting called Namespaces and enter the names of the ones you have access to:</p> <p></p> <p>Going back to the Overview of the cluster, you will now instead see information about it:</p> <p></p> <p>Note that, for security reasons, monitoring metrics are not enabled, but you can read about that in the section below.</p>"},{"location":"user-guide/kubernetes-ui/#pod-information-and-interaction","title":"Pod information and interaction","text":"<p>In the Workload pane, you can see the Pods in your chosen Namespace:</p> <p></p> <p>If you choose a Pod, you will see detailed information about that Pod, such as its labels, and other metadata. Again, note that for security reasons, the monitoring metrics part is empty.</p> <p></p> <p>Among the things one can do with this overview, if the user has access to it, is to both manipulate the Pod as one would be able to do with <code>kubectl edit</code>, but also start an interactive shell session against it (like <code>kubectl exec</code> does):</p> <p></p>"},{"location":"user-guide/kubernetes-ui/#optional-permissions-and-steps-required-for-prometheus-integration","title":"Optional permissions and steps required for Prometheus integration","text":"<p>Lens can present monitoring information regarding your Pods if you have the appropriate permissions. To get the monitoring data for the graphs, it needs to query the Prometheus component in Compliant Kubernetes.</p> <p>However, your cluster administrator may have forbidden this access for security reasons.</p> <p>Why Prometheus integration can be a security risk</p> <p>With the Prometheus access Lens requires, one is able to exfiltrate the credentials that Prometheus itself uses for writing to long-term storage of metrics in Compliant Kubernetes. These could be used to send in junk data and possibly also overwrite the metrics that the real Prometheus process has written.</p> <p>For more information about this issue and how it develops, Compliant Kubernetes is tracking the implications of in this GitHub issue.</p> <p>That said, if you self-administer your cluster, are aware of the potential risks, and still would like to have Prometheus integration, you can do as follows.</p> <p>Download the lens-user-role.yaml file, the contents of which is shown below:</p> <p>Apply it, as-is, to your cluster by running <code>kubectl apply -f lens-user-role.yaml</code>.</p> <p>Assign this role to your user or the group your user belongs to by downloading and modifying it so that it lists your user name (typically that will be your email address) or the group you want to give these permissions to. Do that by downloading the lens-user-rolebinding-example.yaml file and editing it. The file is shown below:</p> <pre><code>\n</code></pre> <p>Apply it to your cluster after making your changes to a copy of the template via, for instance, <code>kubectl apply -f lens-user-rolebinding.yaml</code>.</p> <p>Next, we must configure Lens to connect to the Prometheus instance correctly:</p> <p></p> <p>The address to your Prometheus instance is: <code>monitoring/kube-prometheus-stack-prometheus:9090</code> and you should choose the <code>Prometheus Stack</code> option in the dropdown.</p> <p>With these settings in place, you will have metrics integration working, showing e.g. output like this for the overview and Pod information:</p> <p></p> <p></p>"},{"location":"user-guide/log-based-alerts/","title":"OpenSearch Alert","text":"<p>The alerting feature notifies you when data from one or more OpenSearch indices meets certain conditions. For example, you might want to notify a Slack channel if your application logs more than five HTTP 404 errors in one hour/minute, or you might want to page a developer if no new documents have been indexed in the past 20 minutes.</p> <p>Alerting features have been enabled by default in Elastisys Compliant Kubernetes as of version 0.19.X</p>","tags":["ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/log-based-alerts/#opensearch-alert-demo","title":"OpenSearch Alert Demo","text":"<p>When you log into OpenSearch Dashboards, you will start at the home page as shown below.</p> <p></p> <p>From here click \"Visualize &amp; analyze\" to continue and you will be greeted with the options to go forward to either Dashboard or Discover. Opening the sidebar in the top left will also provide navigation to OpenSearch Dashboards features, and here Alerting can be found in the page shown below.</p> <p></p> <p>Once you click  Alerting, it will navigate to the below page.</p> <p></p> <p>To use OpenSearch alerting feature, it involves two steps described below.</p> <ol> <li>Create Destination - A reusable location for the information that you want the monitor to send out after being triggered. Supported locations are Amazon Chime, Email, Slack, or custom webhook.</li> <li>Create Monitor - A job that runs on a defined schedule and queries OpenSearch indices. The results of these queries are then used as input for one or more triggers (Conditions that, if met, generate alerts).</li> </ol> <p>Step 1 - Create Destination</p> <p>Go to Destination &amp; Create Destinations as shown in the below.</p> <ul> <li>Name - Name of the destination, for example \u201cuser-demo-404-slack-notify\u201d</li> <li>Type - choose Slack or any other available types you want to use it.</li> <li>Webhook URL - If using Slack, paste the webhook URL. Please refer for more information slack-webhook</li> </ul> <p></p> <p>Step 2 - Create Monitors</p> <p>Go to Monitors &amp; Create Monitor as shown in the below.</p> <ul> <li>Monitor Name -  Name of the monitor, for example \u201cuser-demo-404-error\u201d</li> <li>Select Per query monitor or Per bucket monitor. - For more information Monitor-types</li> <li>Frequency -  How often to monitor, for instance, to check every 1 minute</li> <li> <p>Data source</p> <ul> <li> <p>Index  where your logs are stored, for instance, \u201ckubernetes\u201d (per default, Compliant Kubernetes will store all application logs indices that match the \u201ckubernetes*\u201d index pattern)</p> </li> <li> <p>Time field should be set to \u201c@timestamp\u201d</p> </li> </ul> </li> </ul> <p></p> <ul> <li>Query<ul> <li>Metrics - optional</li> <li>Time range for the last - Time frame of data the plugin should monitor . Ex-  1 minute(s)  Data filter - status-code is 404</li> </ul> </li> </ul> <p></p> <ul> <li>Triggers<ul> <li>Trigger name -  Name of the trigger. Ex- \u201c404-error occurred &gt;5 times in last 1 minute\u201d</li> <li>Severity level - Select the severity level range with 1(Highest) &amp; 5(Lowest)</li> <li>Trigger condition - Select the condition according to your applications . Ex- IS ABOVE = 5</li> <li>Actions - Create an action with name , destination and customized message notification accordingly.</li> </ul> </li> </ul> <p></p> <p></p> <ul> <li>Finally click - Create button to complete the creation of the monitor.</li> </ul> <p></p> <ul> <li>You can see the status of the monitor under Alerting&gt; Monitors&gt; user-demo-404-error as shown below.</li> </ul> <p></p>","tags":["ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/log-based-alerts/#test-alert-notification-to-slack","title":"Test alert notification to Slack.","text":"<ul> <li>Demo application deployed and users get 404 errors many times (5 is the condition set before) as shown below.</li> </ul> <ul> <li>We get the Slack notifications as shown below.</li> </ul> <ul> <li>Users can view the alert status under the Alerting tab as shown below and accordingly take the required action.</li> </ul> <ul> <li>Users can acknowledge the alerts under the Alerting tab as shown below.</li> </ul>","tags":["ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/log-based-alerts/#alert-state","title":"Alert state","text":"<ul> <li>Active - The alert is ongoing and unacknowledged. Alerts remain in this state until you acknowledge them, delete the trigger associated with the alert, or delete the monitor entirely.</li> <li>Acknowledged - Someone has acknowledged the alert, but not fixed the root cause.</li> <li>Completed - The alert is no longer ongoing. Alerts enter this state after the corresponding trigger evaluates to false.</li> <li>Error - An error occurred while executing the trigger\u2014usually the result of a bad trigger or destination.</li> <li>Deleted - Someone deleted the monitor or trigger associated with this alert while the alert was ongoing.</li> </ul> <p>You can find the more information about OpenSearch alerting by following the link.</p>","tags":["ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/","title":"Logging","text":"<p>Compliant Kubernetes (CK8s) provides the mechanism to manage your cluster as well as  the lifecycle of thousands of containerized applications deployed in  the cluster. The resources managed by CK8s are expected to be highly distributed with dynamic behaviors. An instance of CK8s cluster  environment involves several components with  nodes that host hundreds of containers that are constantly being spun up and destroyed based on workloads.</p> <p>When dealing with a large pool of containerized applications and workloads in CK8s, it is imperative to be proactive with continuous monitoring and debugging information in order to observe what is going on the cluster. These information can be seen at the container, node, or cluster level.  Logging as one of the three pillars of observability is a crucial element to manage and monitor services and infrastructure. It allows you to track debugging information at different levels of granularity.</p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#compliance-needs","title":"Compliance needs","text":"<p>The requirements to comply with ISO 27001 are stated in ISO 27001:2013. The annexes that mostly concerns logging are:</p> <ul> <li>Annex 12, article A.12.4.1 \"Event Logging\" and A.12.4.3 \"Administrator and Operator Logs\".</li> <li>Annex 16 which deals with incident management.</li> </ul> <p>In Compliant Kubernetes, OpenSearch is separate from the production workload, hence it complies with A.12.4.2 \"Protection of Log Information\". The cloud provider should ensure that the clock of Kubernetes nodes is synchronized, hence complying with A.12.4.4 \"Clock Synchronisation\".</p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#opensearch","title":"OpenSearch","text":"<p>Raw logs in CK8s are normalized, filtered, and processed by fluentd and shipped to OpenSearch for storage and analysis. OpenSearch is derived from the fully open source version of Elasticsearch called  Open Distro for Elasticsearch.</p> <p>OpenSearch provides a powerful, easy-to-use event monitoring and alerting system, enabling you to monitor, search, visualize your data among other things. OpenSearch Dashboards is used as visualization and analysis interface for OpenSearch for all your logs.</p> <p>Note</p> <p>Compliant Kubernetes v0.18 and earlier used Open Distro for Elasticsearch, providing fully open source versions of Elasticsearch and Kibana. This project has now reached end of life and continues through OpenSearch, replacing Elasticsearch with OpenSearch and Kibana with OpenSearch Dashboards. Although a big change for the project, it still remains highly compatible and with minor differences in features and user experience.</p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#visualization-using-opensearch-dashboards","title":"Visualization using OpenSearch Dashboards","text":"<p>OpenSearch Dashboards is used as a data visualization and exploration tool for log time-series  and aggregate analytics. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support.</p> <p>When you log into OpenSearch Dashboards, you will start at the home page as shown below.</p> <p></p> <p>From here click \"Visualize &amp; analyze\" to continue and you will be greeted with the options to go forward to either Dashboard or Discover. Opening the sidebar in the top left will also provide navigation to OpenSearch Dashboards features, and here Visualize can be found in addition to the two former two outlined in the page shown below.</p> <p></p> <p>Since we are concerned with searching logs and their visualization, we will focus on these three features indicated by the red rectangle in the figure above. If you are interested to know more about the rest please visit the official OpenSearch Dashboards documentation.</p> <p>Before we dive in further, let us discuss the type of logs ingested into OpenSearch. Logs in CK8s cluster are filtered and indexed by fluentd into four categories.</p> <p>Application level logs</p> <ul> <li> <p>Kubeaudit logs related to Kubernetes audits to provide security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\".</p> </li> <li> <p>Kubernetes logs that provide insight into CK8s resources such as Nodes, Pods, Containers, Deployments and ReplicaSets. This allows you to observe the interactions between those resources and see the effects that one action has on another. Generally, logs in the CK8s ecosystem can be divided into the cluster level (logs outputted by components such as the kubelet, the API server, the scheduler) and the application level (logs generated by pods and containers). This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\".</p> </li> </ul> <p>Platform level logs</p> <ul> <li> <p>Authlog includes information about system authorization, along with user logins and the authentication mechanism that were used. Such as SSH access to the Nodes. This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\".</p> </li> <li> <p>Others logs other than the above two are indexed and shipped to OpenSearch as others. These logs are collected from the Node's <code>journald</code> logging system.</p> </li> </ul> <p>Note</p> <p>Users can only view the logs of kubernetes and kubeaudit. authlog and others are for Compliant Kubernetes administrators.</p> <p>Let us dive into it then.</p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#data-visualization-and-exploration","title":"Data Visualization and Exploration","text":"<p>As you can see in the figure above, data visualization and exploration in OpenSearch Dashboards has three components: Discover, Visualize and Dashboard. The following section describes each components using examples.</p> <p>Note</p> <p>These following examples were created for Open Distro for Elasticsearch and Kibana, however the user experience is the same when using OpenSearch Dashboards.</p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#discover","title":"Discover","text":"<p>The Discover component in OpenSearch Dashboards is used for exploring, searching and filtering logs.</p> <p>Navigate to Discover as shown previously to access the features provided by it. The figure below shows partial view of the page that you will get under Discover.</p> <p></p> <p>As you can see in the above figure, the kubeaudit index logs are loaded by default. If you want to explore logs from either of the other two log indices please select the right index under the dropdown menu marked log index category.</p> <p>To appreciate the searching and filtering capability, let us get data for the following question:</p> <p>Get all logs that were collected for the past 20 hours in host 172.16.0.3 where the responseStatus reason is notfound</p> <p>We can use different ways to find the answer for the question. Below is one possible solution.</p> <ol> <li> <p>Write sourceIPs: 172.16.0.3  in the search textbox.</p> </li> <li> <p>Click Add Filter and select responseStatus.reason and is under field and Operator dropdown menus respectively. Finally, enter notfound under Value input box and click Save. The following figure shows the details.</p> <p></p> </li> <li> <p>To enter the 20 hours, click part that is labelled Time in the Discover figure above, then enter 20 under the input box and select hours in the dropdown menu. Make sure that you are under Relative tab. Finally, click update. The following figure shows how to set the hours. Note that the data will be automatically updated as time passes to reflect the past 20 hours data from the current time.</p> <p></p> </li> </ol> <p>Once you are done, you will see a result similar to the following figure.</p> <p></p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#visualize","title":"Visualize","text":"<p>The Visualize component in OpenSearch Dashboards is to create different visualizations. Let us create a couple of visualizations.</p> <p>To create visualizations:</p> <ol> <li>Open the sidebar and click Visualize under OpenSearch Dashboards.</li> <li>Click Create visualization link located on the top right side of the page.</li> <li>Select a visualization type, we will use Pie here.</li> <li>Choose the index name or saved query name, if any,  under New Pie / Choose a source. We will use the Kubernetes index here.</li> </ol> <p>By default a pie chart with the total number of logs will be provided by OpenSearch Dashboards. Let us divide the pie chart based on the number of logs contributed by each namespace. To do that perform the following steps:</p> <ol> <li> <p>Under Buckets click add then Split Slices. See the figure below.</p> <p></p> </li> <li> <p>Under aggregation select Significant Terms terms. see the figure below.</p> <p></p> </li> <li> <p>Select Kubernetes.namespace_name.keyword under field. See the figure below.</p> <p></p> </li> </ol> <p>The final result will look like the following figure.</p> <p></p> <p>Please save the pie chart as we will use it later.</p> <p>Let us create a similar pie chart using host instead of namespace. The chart will look like the following figure.</p> <p></p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#dashboard","title":"Dashboard","text":"<p>The Dashboard component in OpenSearch Dashboards is used for organizing related visualizations together.</p> <p>Let us bring the two visualizations that we created above together in a single dashboard.</p> <p>To do that:</p> <ol> <li>Open the sidebar and click Dashboard under OpenSearch Dashboards.</li> <li>Click Create Dashboard link located on the top right side of the page.</li> <li>Click Add existing link located on the left side.</li> <li>Select the name of the two charts/visualizations that you created above.</li> </ol> <p>The figure below shows the dashboard generated from the above steps showing the two pie charts in a single page.</p> <p></p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#accessing-falco-and-opa-logs","title":"Accessing Falco and OPA Logs","text":"<p>To access Falco or OPA logs, go to the  Discover panel and write Falco or OPA on the search textbox.  Make sure that  the Kubernetes log index category is selected.</p> <p>The figure below shows  the search result for Falco logs. </p> <p>The figure below shows the search result for OPA logs. </p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#handling-mapping-conflicts","title":"Handling Mapping Conflicts","text":"<p>If you get the following error:</p> <pre><code>Mapping conflict! A field is defined as several types (string,\ninteger, etc) across the indices that match this pattern. You may still\nbe able to use these conflict fields in parts of Kibana, but they will\nbe unavailable for functions that require Kibana to know their type.\nCorrecting this issue will require re-indexing your data.\n</code></pre> <p>This means that your application has changed the type of a field in your structured logs. For example, say version A of your application logs the HTTP request path in <code>request</code>. Later, version B logs the HTTP request path in <code>request.path</code> and the HTTP verb in <code>request.verb</code>. Essentially, <code>request</code> has changed from string to dictionary.</p> <p>As a first step, review your application change management policy to reduce the chance of a log field changing type.</p> <p>Second, ask your administrator to re-index the affected indices.</p> <p>Note</p> <p>Re-indexing requires a lot of permissions, including creating and deleting indices, and changing Index templates. This may interfere with audit logs and compromise platform security. Therefore, to ensure platform security, re-indexing can only be performed by Compliant Kubernetes administrators.</p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#running-example","title":"Running Example","text":"<p>The user demo application already includes structured logging: For each HTTP request, it logs the URL, the user agent, etc. Compliant Kubernetes further adds the Pod name, Helm Chart name, Helm Release name, etc. to each log entry.</p> <p>The screenshot below gives an example of log entries produced by the user demo application. It was obtained by using the index pattern <code>kubernetes*</code> and the filter <code>kubernetes.labels.app_kubernetes_io/instance:myapp</code>.</p> <p></p> <p>Note</p> <p>You may want to save frequently used searches as dashboards. Compliant Kubernetes saves and backs these up for you.</p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#exporting-logs","title":"Exporting logs","text":"<p>At the moment the reporting feature in OpenSearch doesn't work so instead we recommend you to use elasticsearch-dump.</p> <p>Example of exporting the <code>kubernetes-*</code> index pattern to a folder <code>opensearch-dump</code>:</p> <pre><code>docker pull elasticdump/elasticsearch-dump\nmkdir opensearch-dump\n\n# OpenSearch username and password\n# This will be handed out from your Compliant Kubernetes administrator\nOPENSEARCH_USERNAME=\"your-username\"\nOPENSEARCH_PASSWORD=\"your-password\"\n\n# Your domain that is used for your cluster.\n# This is the same as the one you are using for your other services (grafana, harbor, etc.)\nDOMAIN=\"your-domain\"\n\ndocker run --rm -ti -v $(pwd)/opensearch-dump:/tmp elasticdump/elasticsearch-dump \\\n--input=\"http://${OPENSEARCH_USERNAME}:${OPENSEARCH_PASSWORD}@opensearch.ops.${DOMAIN}/kubernetes-*\" \\\n--type=data \\\n--output=/tmp/opensearch-dump.json \\\n--searchBody='{\"query\":{......}}'\n</code></pre> <p>For more examples and how to use the tool, read the documentation in the repo.</p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#log-review-dashboard","title":"Log review dashboard","text":"<p>This dashboard can be viewed to get a quick overview of the cluster's state.</p> <p></p>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#kubeaudit","title":"kubeaudit","text":"<ul> <li>All api-requests = Successful API requests</li> <li>Forbid error = Forbidden API requests</li> <li>Client Error = Client error logs</li> <li>Server error = Server error logs</li> </ul>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#kubernetes","title":"kubernetes","text":"<ul> <li>error OR denied = Error &amp; denied logs</li> </ul>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#other","title":"other","text":"<ul> <li>error OR critical OR alert OR warning = System logs of priority 1-4</li> </ul>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#authlog","title":"authlog","text":"<ul> <li>number of authlog seassions = Authlog seassions</li> </ul>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/logs/#further-reading","title":"Further Reading","text":"<ul> <li>OpenSearch</li> <li>OpenSearch Dashboards</li> <li>Open Distro for Elasticsearch</li> <li>Kibana</li> <li>Open Distro for Elasticsearch \u2013 How Different Is It?</li> <li>Fluentd</li> </ul>","tags":["ISO 27001 A.12.4.1 Event Logging","ISO 27001 A.12.4.3 Administrator & Operator Logs","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/long-term-log-retention/","title":"Long-term log retention","text":"<p>Compliant Kubernetes by default sets an retention of 30 days for logs, with an additional 30 days though backups of this logs, within OpenSearch. Many regulators, including Swedish Healthcare, require a minimum of 5 year log retention.</p> <p>This is not provided at the platform level by Compliant Kubernetes as it runs the risk of GDPR non-complicance. Logs may include sensitive information like personal data, which requires that the the retention scheme is designed together with application-specific knowledge to ensure compliance. Specifically, this includes that the retention scheme ensures that erased personal data can not be accidentally restored, as per Art. 17 GDPR Right to erasure (\u2018right to be forgotten\u2019).</p> <p>Using application-specific knowledge would also make it possible to reduce the amount of logs stored, by filtering out so only the required logs are kept. Minimising the kept data, storage costs and storage management.</p>","tags":["HIPAA S48 - Audit Controls - \u00a7 164.312(b)","GDPR Art. 17 Right to erasure (\"right to be forgotten\")"]},{"location":"user-guide/long-term-log-retention/#exporting-logs-for-long-term-storage","title":"Exporting logs for long-term storage","text":"<p>To enable long-term log retention we instead recommend using Elasticdump. This tool can export logs from OpenSearch within Compliant Kubernetes on a per document basis in either CSV or JSON format, allowing other tools to process the logs and ship them somewhere else. It can also perform transformations, compress using Gzip, and write them into a file or send them to S3 object storage.</p> <p>Using this tool, along with the REST API of OpenSearch, then it is possible to create scripts to export logs for long-term storage using a Kubernetes CronJob. Down below are some examples how to discover indices to export from OpenSearch, some commands to use with Elasticdump, and an example Dockerfile and some Kubernetes manifests.</p>","tags":["HIPAA S48 - Audit Controls - \u00a7 164.312(b)","GDPR Art. 17 Right to erasure (\"right to be forgotten\")"]},{"location":"user-guide/long-term-log-retention/#accessing-opensearch","title":"Accessing OpenSearch","text":"<p>Info</p> <p>To access OpenSearch contact your Compliant Kubernetes administrator and ask them to create a user with suitable permissions listed here below.</p> <p>This is the permissions required for the OpenSearch API snippets and the permissions required by Elasticdump:</p> <pre><code>cluster_permissions:\n- cluster_monitor\n- indices:data/read/scroll\n- indices:data/read/scroll/clear\nindex_permissions:\n- index_patterns:\n- '*'\nallowed_actions:\n- indices:admin/aliases/get # Can be omitted when aliases is not used\n- indices:monitor/*\n- index_patterns:\n- kubernetes*\nallowed_actions:\n- indices:admin/get\n- indices:admin/mappings/get\n- indices_monitor\n- read\n- search\n</code></pre> <p>With <code>${DOMAIN}</code> set to the domain of your environment, the variables then needed to connect the becomes:</p> <pre><code>export OS_PROTOCOL=\"https\"\nexport OS_ENDPOINT=\"opensearch.ops.${DOMAIN}\"\nexport OS_USERNAME=\"&lt;provided-by-admin&gt;\"\nexport OS_PASSWORD=\"&lt;provided-by-admin&gt;\"\n\n# The index pattern we want to export, normally \"kubernetes*\"\nexport OS_PATTERN=\"kubernetes*\"\n</code></pre> <p>Important</p> <p>These variables will be used later on in the example snippets.</p>","tags":["HIPAA S48 - Audit Controls - \u00a7 164.312(b)","GDPR Art. 17 Right to erasure (\"right to be forgotten\")"]},{"location":"user-guide/long-term-log-retention/#discovering-aliases-and-indices","title":"Discovering aliases and indices","text":"<p>In OpenSearch logs are stored into indices. These indices are managed in a way that will limit them both in time and size, to make them more manageable. Each index typically represents a days worth of logs, but if the size of the index exceeds a set threshold a new one will be created to limit their maximum size.</p> <p>The indices are all grouped within index aliases, a sort of virtual index that behind the scenes link to other indices. This allows one to read from all indices and write to one designated write index, all using the same name.</p> <p>Since only the write index can change one method to select indices for exporting into log-term storage is to only export the read indices. This way there is no need to check and update indices in case they've changed since the previous export run, simplifying the export logic.</p> <p>Example: List all indices using a pattern</p> <pre><code># call as: get_indices &lt;pattern&gt;\nget_indices() {\npattern=\"$1\"\n\nres=\"$(curl -u \"${OS_USERNAME}:${OS_PASSWORD}\" -XGET \"${OS_PROTOCOL}://${OS_ENDPOINT}/_cat/indices?h=index\")\"\n\nif echo \"${res}\" | grep \"error\" &gt;&amp;2; then\nexit 1\nelif echo \"${res}\" | grep \"fail\" &gt;&amp;2; then\nexit 1\nelse\necho \"${res}\" | sed -n \"/${pattern}/p\" | sort\n    fi\n}\n</code></pre> <p>This will generate a list of all indices within OpenSearch for the specified pattern.</p> <p>The pattern accept regex used by <code>sed</code> and should in most instances be <code>kubernetes*</code> to only include the application logs indices.</p> <p>Example: List all write indices using a pattern</p> <pre><code># call as: get_write_index &lt;pattern&gt;\nget_write_index() {\npattern=\"$1\"\n\nres=\"$(curl -u \"${OS_USERNAME}:${OS_PASSWORD}\" -XGET \"${OS_PROTOCOL}://${OS_ENDPOINT}/_cat/aliases?h=alias,index,is_write_index\")\"\n\nif echo \"${res}\" | grep \"error\" &gt;&amp;2; then\nexit 1\nelif echo \"${res}\" | grep \"fail\" &gt;&amp;2; then\nexit 1\nelse\necho \"${res}\" | grep \"true\" | sed -n \"/${pattern}/p\" | awk '{print $2}' || true\nfi\n}\n</code></pre> <p>This will generate a list of all write indices within OpenSearch for the specified pattern.</p> <p>The pattern accept regex used by <code>sed</code> and should in most instances be <code>kubernetes*</code> to only include the application logs alias.</p> <p>Since the pattern only should only match a single alias, and since each alias can only have a single write index, the output should be validated that it at most only contains one index.</p> <p>Using these two example functions we can now fetch the indices for a pattern and find the write index for any matching alias, meaning we can iterate over them, filter out the write index, and perform our backup:</p> <p>Example: Iterating over indices, filtering out the write index, and perform export action</p> <pre><code>indices=$(get_indices \"${OS_PATTERN}\")\nwrite_index=$(get_write_index \"${OS_PATTERN}\")\n\nfor index in ${indices}; do\nif [ \"${index}\" = \"${write_index}\" ]; then\ncontinue # skipping as it is the write index\nfi\n\nperform_export \"${index}\"\ndone\n</code></pre> <p>For more complex tasks checkout the OpenSearch REST API reference.</p>","tags":["HIPAA S48 - Audit Controls - \u00a7 164.312(b)","GDPR Art. 17 Right to erasure (\"right to be forgotten\")"]},{"location":"user-guide/long-term-log-retention/#exporting-indices","title":"Exporting indices","text":"<p>With Elasticdump it is possible to export logs out to the console, to file, or to S3. In these example snippets we well export them to S3. To be able to do some management functions we will also use s3cmd, most importantly to be able to check for existing exports.</p> <p>Using S3 both for Elasticdump and s3cmd will require the following variables:</p> <pre><code>export S3_BUCKET=\"&lt;bucket&gt;\"\nexport S3_REGION=\"&lt;region&gt;\"\nexport S3_ENDPOINT=\"&lt;region-endpoint&gt;\"\nexport S3_FORCE_PATH_STYLE=\"&lt;false|true&gt;\" # Generally \"false\" for AWS and Exoscale, else \"true\"\nexport AWS_ACCESS_KEY_ID=\"&lt;access-key&gt;\"\nexport AWS_SECRET_ACCESS_KEY=\"&lt;secret-key&gt;\"\n\nif [ \"$S3_FORCE_PATH_STYLE\" = \"true\" ]; then\nexport S3_BUCKET_ENDPOINT=\"${S3_ENDPOINT}\"\nelse\nexport S3_BUCKET_ENDPOINT=\"%(bucket)s.${S3_ENDPOINT}\"\nfi\n</code></pre> <p>Important</p> <p>These variables will be used later on in the example snippets.</p> <p>Example: Export entire index to S3</p> <pre><code># With ${index} set to the index to export.\nelasticdump \\\n--input \"${OS_PROTOCOL}://${OS_USERNAME}:${OS_PASSWORD}@${OS_ENDPOINT}/${index}\" \\\n--output \"s3://$S3_BUCKET/${index}.json.gz\" \\\n--s3Region ${S3_REGION} \\\n--s3Endpoint ${S3_ENDPOINT} \\\n--s3ForcePathStyle ${S3_FORCE_PATH_STYLE} \\\n--s3Compress \\\n--concurrency 40 \\\n--concurrencyInterval 1000 \\\n--intervalCap 20 \\\n--limit 1000\n</code></pre> <p>This process will take a while depending on the size of the index. By default it will not try to delete, replace or update any resources, so this must be enabled using the appropriate flags or it should be managed by other means like s3cmd.</p> <p>Caution</p> <p>If this process is aborted it will leave multipart uploads after itself that should be cleared, else they will still use storage on the S3 service!</p> <p>These can be listed and then removed with s3cmd: <pre><code>s3cmd \\\n--host=${S3_ENDPOINT} \\\n--host-bucket=${S3_BUCKET_ENDPOINT} \\\nmultipart \"s3://${S3_BUCKET}\"\n\ns3cmd \\\n--host=${S3_ENDPOINT} \\\n--host-bucket=${S3_BUCKET_ENDPOINT} \\\nabortmp \"s3://${S3_BUCKET}/&lt;multipart-upload-path&gt;\" \"&lt;multipart-upload-id&gt;\"\n</code></pre></p> <p>In the example above only certain logs can be exported by adding a query using OpenSearch Query DSL with the <code>--searchBody '&lt;query&gt;'</code> flag. This way it is possible to filter on certain labels to only export logs for a particular namespace, deployment, or even using identifier within structured logs. An example for a specific namespace would be: <pre><code>{\n\"query\": {\n\"term\": {\n\"kubernetes.namespace_name\": \"production\"\n}\n}\n}\n</code></pre> Since the Query DSL is in JSON format it must be properly quoted or escaped to keep its format, preferably the whitespace should be stripped before sending it as an argument to Elasticdump.</p> <p>Example: Putting it all together</p> <pre><code># call as: perform_export &lt;index&gt;\nperform_export() {\nindex=\"$1\"\n\ncheck=\"$(s3cmd \"--host=${S3_ENDPOINT}\" \"--host-bucket=${S3_BUCKET_ENDPOINT}\" ls \"s3://${S3_BUCKET}/${index}.json.gz\"\nif [ -n \"${check}\" ]; then\nreturn # skipping as it is already exported\nfi\n\n# Just as an example\nquery='{\"query\": {\"term\": {\"kubernetes.namespace_name\": \"production\"}}}'\n\nelasticdump \\\n--input \"${OS_PROTOCOL}://${OS_USERNAME}:${OS_PASSWORD}@${OS_ENDPOINT}/${index}\" \\\n--output \"s3://$S3_BUCKET/${index}.json.gz\" \\\n--s3Region ${S3_REGION} \\\n--s3Endpoint ${S3_ENDPOINT} \\\n--s3ForcePathStyle ${S3_FORCE_PATH_STYLE} \\\n--s3Compress \\\n--concurrency 40 \\\n--concurrencyInterval 1000 \\\n--intervalCap 20 \\\n--limit 1000 \\\n--searchBody \"${query}\"\n}\n</code></pre>","tags":["HIPAA S48 - Audit Controls - \u00a7 164.312(b)","GDPR Art. 17 Right to erasure (\"right to be forgotten\")"]},{"location":"user-guide/long-term-log-retention/#deploying-cronjobs","title":"Deploying CronJobs","text":"<p>The simplest way to prepare this for deployment is to build a container image including Bash, Elasticdump and s3cmd, and set up a CronJob to run this on a preferred schedule.</p> <p>Here are some examples of how to build and deploy them:</p> <p>Example: Containerfile / Dockerfile</p> <pre><code>FROM docker.io/library/ubuntu:jammy\n\nARG DEBIAN_FRONTEND=noninteractive\nARG TZ=Etc/UTC\n\nRUN apt update &amp;&amp; \\\napt install -y --no-install-recommends ca-certificates curl npm s3cmd &amp;&amp; \\\napt clean -y &amp;&amp; \\\nrm -rf /var/lib/apt\n\nRUN npm install elasticdump@v6.88.0 -g\n\nCMD [\"elasticdump\"]\n</code></pre> <p>Example: Kubernetes resources</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\nname: export-script\ndata:\nscript.sh: |-\n&lt;bash-script&gt;\n---\napiVersion: v1\nkind: Secret\nmetadata:\nname: export-secret\ntype: Opaque\nstringData:\n# OpenSearch\nOS_PROTOCOL: \"${OS_PROTOCOL}\"\nOS_ENDPOINT: \"${OS_ENDPOINT}\"\nOS_USERNAME: \"${OS_USERNAME}\"\nOS_PASSWORD: \"${OS_PASSWORD}\"\nOS_PATTERN: \"${OS_PATTERN}\"\n# S3\nS3_BUCKET: \"${S3_BUCKET}\"\nS3_REGION: \"${S3_REGION}\"\nS3_ENDPOINT: \"${S3_ENDPOINT}\"\nS3_BUCKET_ENDPOINT: \"${S3_BUCKET_ENDPOINT}\"\nS3_FORCE_PATH_STYLE: \"${S3_FORCE_PATH_STYLE}\"\nAWS_ACCESS_KEY_ID: \"${AWS_ACCESS_KEY_ID}\"\nAWS_SECRET_ACCESS_KEY: \"${AWS_SECRET_ACCESS_KEY}\"\n---\napiVersion: batch/v1\nkind: CronJob\nmetadata:\nname: export\nspec:\nschedule: &lt;schedule&gt; # example \"@daily\"\nconcurrencyPolicy: Forbid\njobTemplate:\nspec:\ntemplate:\nmetadata:\nlabels:\napp: log-export\nspec:\nautomountServiceAccountToken: false\nrestartPolicy: Never\ncontainers:\n- name: export\nimage: &lt;image&gt;\ncommand:\n- /scripts/script.sh\nenvFrom:\n- secretRef:\nname: export-secret\nresources:\nrequests:\ncpu: 500m\nmemory: 500Mi\nlimits:\ncpu: 1000m\nmemory: 750Mi\nsecurityContext:\nrunAsNonRoot: true\ncapabilities:\ndrop:\n- ALL\nvolumeMounts:\n- name: script\nmountPath: /scripts\nreadOnly: true\nsecurityContext:\nrunAsNonRoot: true\nrunAsGroup: 65534\nrunAsUser: 65534\nfsGroup: 65534\nvolumes:\n- name: script\nconfigMap:\nname: export-script\ndefaultMode: 0777\n</code></pre> <p>For more information about CronJobs checkout the Kubernetes documentation and reference about the subject.</p>","tags":["HIPAA S48 - Audit Controls - \u00a7 164.312(b)","GDPR Art. 17 Right to erasure (\"right to be forgotten\")"]},{"location":"user-guide/maintenance/","title":"What to expect from maintenance","text":""},{"location":"user-guide/maintenance/#different-kinds-of-maintenance","title":"Different kinds of maintenance","text":"<ul> <li>Patching the underlying OS on the nodes</li> <li>Upgrading the Compliant Kubernetes application stack</li> <li>Upgrading Kubernetes</li> </ul>"},{"location":"user-guide/maintenance/#what-impact-could-these-kinds-of-maintenance-have-on-your-application","title":"What impact could these kinds of maintenance have on your application?","text":"<p>Let's go through them one by one.</p>"},{"location":"user-guide/maintenance/#patching-the-os-on-the-nodes","title":"Patching the OS on the nodes","text":"<p>Some service disruption is expected here, the nodes need to reboot in order to install the OS upgrades/security patches. This should be done automatically by Kured in almost all cases going forward, luckily Kured can be scheduled to perform these upgrades during night-time or whenever application traffic is expected to be low. Thanks to Kured these upgrades are not usually a problem.</p>"},{"location":"user-guide/maintenance/#upgrading-the-compliant-kubernetes-application-stack","title":"Upgrading the Compliant Kubernetes application stack","text":"<p>There is barely any downtime expected from upgrading the base Compliant Kubernetes application stack. This is because most of the components being upgraded are not intertwined with your application, the only exception being NGINX Ingress Controller, which is not commonly upgraded.</p> <p>If you have any other managed services from us such as PostgreSQL, Redis, RabbitMQ or TimescaleDB, these services might be upgraded during the application maintenance windows. Upgrading these services can cause some short service disruptions and making them temporarily unreachable for your application.</p>"},{"location":"user-guide/maintenance/#upgrading-kubernetes","title":"Upgrading Kubernetes","text":"<p>The most impactful type of maintenance are the Kubernetes upgrades, which means that the nodes need to be rebooted. Currently there is no automatic process of doing this upgrade, so it has to be done manually. We do these upgrades on scheduled maintenance windows during office hours. The way we handle the upgrades is that we drain and reboot all nodes, one at the time.</p> <p>If parts of your application is running on just one node, then service disruptions are to be expected and parts of the application may become unreachable for short periods during the maintenance window.</p> <p>The worst case would be if the nodes were near full on resources, then the pods may not be able to be scheduled on another node while getting rebooted. This would mean that the pods running on that node would need to wait for its node to be ready again before it can be scheduled, which could be minutes of downtime.</p> <p>Note that this is not just a problem for Compliant Kubernetes, the same process would need to be followed when upgrading a \"vanilla\" Kubernetes cluster.</p> <p>To minimize the impact on the application you should use two replicas for your application and also set up topologySpreadConstraints to make sure that the replicas do not get scheduled on the same node.</p>"},{"location":"user-guide/metrics/","title":"Metrics","text":"<p>This guide gives an introduction to Prometheus and Grafana and where they fit in Compliant Kubernetes, in terms of reducing the compliance burden.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#why-prometheus-and-grafana","title":"Why Prometheus and Grafana?","text":"<p>Prometheus is an open-source solution for monitoring and alerting. It works by collecting and processing metrics from the various services in the cluster. It is widely used, stable, and a CNCF member. It is relatively easy to write ServiceMonitors for any custom services to get monitoring data from them into Prometheus.</p> <p>Grafana is the most widely used technology for visualization of metrics and analytics. It supports a multitude of data sources and it is easy to create custom dashboards. Grafana is created by Grafana Labs, a CNCF Silver Member.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#compliance-needs","title":"Compliance needs","text":"<p>The requirements to comply with ISO 27001 are stated in ISO 27001:2013 The annexes that mostly concerns monitoring and alerting are Annex 12, article A.12.1.3 \"capacity management\", and Annex 16 which deals with incident management.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#capacity-management","title":"Capacity management","text":"<p>Article A.12.1.3 states that \"The use of resources must be monitored, tuned and projections made of future capacity requirements to ensure the required system performance to meet the business objectives.\"</p> <ul> <li>Promethus and Grafana helps with this as the resource usage, such as storage capacity, CPU, and network usage can be monitored. Using visualization in Grafana, projections can be made as to future capacity requirements.</li> </ul> <p>The article goes on to say that \"Capacity management also needs to be: Pro-active \u2013 for example, using capacity considerations as part of change management; Re-active \u2013 e.g. triggers and alerts for when capacity usage is reaching a critical point so that timely increases, temporary or permanent can be made.\"</p> <ul> <li>Prometheus has a rich alerting functionality, allowing you to set up alerts to warn if, for example, thresholds are exceeded or performance is degraded.</li> </ul>","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#incident-management","title":"Incident management","text":"<p>Annex A.16.1 is about management of information security incidents, events and weaknesses. The objective in this Annex A area is to ensure a consistent and effective approach to the lifecycle of incidents, events and weaknesses. Incidents needs to be tracked, reported, and lessons learned from them to improve processes and reduce the possibility of similar incidents occurring in the future.</p> <p>Prometheus and Grafana can help with this by making it easier to:</p> <ul> <li>collect evidence as soon as possible after the occurrence.</li> <li>conduct an information security forensics analysis</li> <li>communicate the existence of the information security incident or any relevant details to the leadership.</li> </ul>","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#prometheus-and-grafana-in-compliant-kubernetes","title":"Prometheus and Grafana in Compliant Kubernetes","text":"","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#prometheus","title":"Prometheus","text":"<p>Compliant Kubernetes installs the prometheus-operator by default. The Prometheus Operator for Kubernetes provides easy monitoring definitions for Kubernetes services and deployment and management of Prometheus instances as it can create/configure/manage Prometheus clusters atop Kubernetes. The following CRDs are installed by default.</p> crd apigroup kind can be used by users alertmanagers monitoring.coreos.com Alertmanager NO podmonitors monitoring.coreos.com PodMonitor YES prometheuses monitoring.coreos.com Prometheus NO prometheusrules monitoring.coreos.com PrometheusRule YES servicemonitors monitoring.coreos.com ServiceMonitor YES thanosrulers monitoring.coreos.com ThanosRuler NO","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#accessing-prometheus","title":"Accessing Prometheus","text":"<p>If you want to access the web interface of Prometheus, proceed as follows:     1. Type: <code>kubectl proxy</code>     2. Open this link in your browser</p> <p>The Prometheus UI is only available by default starting in Compliant Kubernetes version 0.26.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#grafana","title":"Grafana","text":"<p>Grafana can be accessed at the endpoint provided by the Compliant Kubernetes install scripts. If you have configured dex you can login with a connected account.</p> <p>Compliant Kubernetes deploys Grafana with a selection of dashboards by default. Dashboards are accessed by clicking the Dashboard icon (four squares) at the lefthand side of the Grafana window and selecting Browse. Some examples of useful dashboards are listed below.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#node-health","title":"Node health","text":"<p>The Nodes dashboard (Node Exporter / Nodes) gives a quick overview of the status (health) of a node in the cluster. By selecting an instance in the \"instance\" dropdown metrics for CPU, Load, Memory, Disk and Network I/O is showed for that node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs.</p> <p></p>","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#pod-health","title":"Pod health","text":"<p>The Pods dashboard (Kubernetes/Compute resources/Pods) gives a quick overview of the status (health) of a pod in the cluster. By selecting a pod in the \"pod\" dropdown metrics for CPU, Memory, and Network I/O is showed for that node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs.</p> <p></p>","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#collecting-metrics","title":"Collecting metrics","text":"<p>Configuring Prometheus to collect metrics from an application requires either a ServiceMonitor or a PodMonitor, targeting a Kubernetes Service or Pod respectively. They are both described upstream in the API reference for Prometheus Operator. In general ServiceMonitors are recommended over PodMonitors, and it is the most common way to configure metrics collection.</p> <p>In Compliant Kubernetes the Prometheus Operator in the workload cluster is configured to pick up all ServiceMonitors and PodMonitors, regardless in which namespace they are or which labels they have.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#running-example","title":"Running Example","text":"<p>The user demo already includes a ServiceMonitor, as required for Compliant Kubernetes to collect metrics from its <code>/metrics</code> endpoint:</p> <pre><code>{{- if .Values.serviceMonitor.enabled -}}\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\nname: {{ include \"ck8s-user-demo.fullname\" . }}\nlabels:\n{{- include \"ck8s-user-demo.labels\" . | nindent 4 }}\nspec:\nselector:\nmatchLabels:\n{{- include \"ck8s-user-demo.selectorLabels\" . | nindent 6 }}\nendpoints:\n- port: http\n{{- end }}\n</code></pre> <p>The screenshot below shows Grafana in \"Explore\" mode (the compass icon to the left) featuring the query <code>rate(http_request_duration_seconds_count[1m])</code>. It shows the request rate for the user demo application for each path and status code. As can be seen in the graph, the <code>/users</code> endpoint is getting more traffic than the other endpoints.</p> <p></p> <p>The \"Explore\" mode is great for developing queries and exploring the data set. If you want to save a query so you can refer back to it, you can create a Dashboard instead. Dashboards consist of multiple Panels, each of which, can display the results of running queries. Learn more about Grafana panels.</p> <p>Note</p> <p>You may want to save frequently used Dashboards. Compliant Kubernetes saves and backs these up for you.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#troubleshooting-metrics-collection","title":"Troubleshooting metrics collection","text":"<p>It is possible to see if monitors are picked up by accessing Prometheus' web interface. Navigating to \"Status\" &gt; \"Service Discovery\" will show all monitors picked by Prometheus, and the \"(x/y active targets)\" will show how many targets of those monitors are active. Active targets are actively scraped by Prometheus and inactive targets are those that fail to match the selectors of the monitor.</p> <p>Monitors can be expanded further down to list and inspect its targets, within each one the \"Discovered Labels\" column will list information about the object in Kubernetes and in the \"Target Labels\" it will show the labels recorded from the target.</p> <p>If the \"Target Labels\" is \"Dropped\" for a target then it means that it has been excluded from scraping since it doesn't match the monitor. There are three key things to check to make sure a target is properly picked up by a monitor:</p> <ol> <li> <p>Make sure either that the monitor is in the same namespace as the target, or that the monitor has the correct namespace selector for the target:   <pre><code> apiVersion: monitoring.coreos.com/v1\n kind: ServiceMonitor\n metadata:\n   ...\n spec:\n   ...\n+  namespaceSelector:\n+    matchNames:\n+      - &lt;namespace&gt;\n  ...\n</code></pre></p> </li> <li> <p>Make sure that the selector of the monitor matches the target:   <pre><code> apiVersion: v1\n kind: Service\n metadata:\n   ...\n+  labels:\n+    app: target\n  ...\n spec:\n   ...\n ---\n apiVersion: monitoring.coreos.com/v1\n kind: ServiceMonitor\n metadata:\n   ...\n spec:\n   ...\n+  selector:\n+    matchLabels:\n+      app: target\n  ...\n</code></pre></p> </li> <li> <p>Make sure that the port of the monitor matches the target:   <pre><code> apiVersion: v1\n kind: Service\n metadata:\n   ...\n spec:\n   ...\n+  ports:\n+    - name: target\n+      port: 9000\n  ...\n ---\n apiVersion: monitoring.coreos.com/v1\n kind: ServiceMonitor\n metadata:\n   ...\n spec:\n   ...\n+  endpoints:\n    # either\n+    - port: target\n    # or\n+    - port: 9000\n  ...\n</code></pre></p> </li> </ol> <p>The same concept applies to PodMonitors and Pods.</p> <p>Then when the targets are active it is possible to see scrape information by navigating to \"Status\" &gt; \"Targets\". Here Prometheus gives information about the time, status, and duration for scrapes.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/metrics/#further-reading","title":"Further reading","text":"<p>For more information please refer to the official Prometheus and Grafana documentation.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management","ISO 27001 A.16 Information Security Incident Management"]},{"location":"user-guide/namespaces/","title":"Namespaces","text":"","tags":["ISO 27001 A.12.1.4 Separation of Development, Testing & Operational Environments","ISO 27001 A.14.2.5 Secure System Engineering Principles"]},{"location":"user-guide/namespaces/#hnc","title":"HNC","text":"<p>Hierarchical Namespace Controller (HNC) is included in Compliant Kubernetes. It allows the super application developer to manage namespaces as subnamespaces and delegates access automatically. From the perspective of Kubernetes these are regular namespaces, but these can be modified via a namespaced resource by the user. Building a good namespace structure will enable you to apply namespace-scoped RBAC resources to multiple namespaces at once.</p>","tags":["ISO 27001 A.12.1.4 Separation of Development, Testing & Operational Environments","ISO 27001 A.14.2.5 Secure System Engineering Principles"]},{"location":"user-guide/namespaces/#namespace-management","title":"Namespace Management","text":"<p>Creating a subnamespace: <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: hnc.x-k8s.io/v1alpha2\nkind: SubnamespaceAnchor\nmetadata:\n  name: &lt;descendant-namespace&gt;\n  namespace: &lt;parent-namespace&gt;\nEOF\n</code></pre></p> <p>Verify that it gets created: <pre><code>kubectl get ns &lt;descendant-namespace&gt;\n</code></pre></p> <p>Verify that it gets configured: <pre><code>$ kubectl get subns -n &lt;parent-namespace&gt; &lt;descendant-namespace&gt; -o yaml\napiVersion: hnc.x-k8s.io/v1alpha2\nkind: SubnamespaceAnchor\nmetadata:\n    ...\n  name: &lt;descendant-namespace&gt;\n  namespace: &lt;parent-namespace&gt;\n...\nstatus:\n  status: Ok\n</code></pre></p> <p>If the status is <code>Ok</code> then the subnamespace is ready to go.</p> <p>Tip</p> <p>HNC also comes with the HNS <code>kubectl</code> plugin.</p> <p>Using this plugin creating subnamespaces is as easy as: <pre><code>kubectl hns create -n &lt;parent-namespace&gt; &lt;descendant-namespace&gt;\n</code></pre></p> <p>And provides more detailed information using: <pre><code>kubectl hns describe &lt;namespace&gt;\n\nkubectl hns tree &lt;namespace&gt;\n</code></pre></p>","tags":["ISO 27001 A.12.1.4 Separation of Development, Testing & Operational Environments","ISO 27001 A.14.2.5 Secure System Engineering Principles"]},{"location":"user-guide/namespaces/#resource-propagation","title":"Resource Propagation","text":"<p>When a subnamespace is created all <code>Roles</code> and <code>RoleBindings</code> will propagate from the parent namespace to the descendant namespace to ensure that correct access is set. This is what lets you apply namespace-scoped RBAC resources to multiple namespaces at once. Propagated copies cannot be modified, these types of resources cannot be created in a parent namespace if it conflicts with a resource in a descendant namespace. To put an exception annotate the <code>Role</code> or <code>RoleBinding</code> with <code>propagate.hnc.x-k8s.io/none: \"true\"</code> to prevent if from being propagated at all. Another option is to only propagate to selected descendant namespaces use <code>propagate.hnc.x-k8s.io/treeSelect: ...</code>, include descendant namespaces with <code>&lt;descendant-namespace&gt;</code> or exclude namespaces with <code>!&lt;descendant-namespace&gt;</code>.</p> <p>Note</p> <p>In Compliant Kubernetes <code>v0.27.0</code> and later <code>NetworkPolicies</code> will also propagate from the parent namespace to the descendant namespace just like <code>Roles</code> and <code>Rolebindings</code>.</p>","tags":["ISO 27001 A.12.1.4 Separation of Development, Testing & Operational Environments","ISO 27001 A.14.2.5 Secure System Engineering Principles"]},{"location":"user-guide/namespaces/#further-reading","title":"Further Reading","text":"<ul> <li>HNC User Documentation</li> <li>Introducing HNC</li> </ul>","tags":["ISO 27001 A.12.1.4 Separation of Development, Testing & Operational Environments","ISO 27001 A.14.2.5 Secure System Engineering Principles"]},{"location":"user-guide/network-model/","title":"Network Model","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.10.1.2 Key Management</li> <li>A.13.1.1 Network Controls</li> <li>A.13.1.2 Security of Network Services</li> <li>A.13.1.3 Segregation in Networks</li> </ul> <p></p> <p>The diagram above present a useful model when reasoning about networking in Compliant Kubernetes.</p> <p>Note</p> <p>This is just a model and not an architectural diagram. Under the hood, things are a lot more complicated.</p>","tags":["ISO 27001 A.10.1.2 Key Management","ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"user-guide/network-model/#private-network","title":"Private Network","text":"<p>Your application Pods, as well as Pods of additional services, can communicate on a secure private network, via RFC1918 private IP addresses. It is analogous to a VPC in VM-based workloads.</p> <p>In Compliant Kubernetes, it is the responsibility of the administrator to ensure the in-cluster private network is secure and trusted, either by performing an infrastructure audit or deploying Pod-to-Pod encryption.</p> <p>You should use NetworkPolicies to segregate your Pods. This improves your security posture by reducing the blast radius in case parts of your application are under attack.</p> <p>Example</p> <p>Feel free to take inspiration from the user demo.</p> <p>More example recipes for Kubernetes Network Policies that you can just copy paste can be found here.</p>","tags":["ISO 27001 A.10.1.2 Key Management","ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"user-guide/network-model/#private-dns","title":"Private DNS","text":"<p>The private network also features a private DNS. A Service <code>my-svc</code> in the namespace <code>my-namespace</code> can be accessed from within the Kubernetes cluster as <code>my-svc.my-namespace</code>.</p> <p>IP addresses of Pods are not stable. For example, the rollout of a new container image creates new Pods, which will have new IP addresses. Therefore, you should always use private DNS names of Services to connect your application Pods, as well as to connect your application to additional services.</p>","tags":["ISO 27001 A.10.1.2 Key Management","ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"user-guide/network-model/#ingress","title":"Ingress","text":"<p>Your application users should never ever access the private network directly. Instead external access is enabled by creating Ingress objects. Compliant Kubernetes already comes with cert-manager and is already configured with a ClusterIssuer. A secure ACME protocol is used to issue and rotate certificates using the LetsEncrypt public service.</p> <p>Assuming you configured a Service and a Deployment for you application, making application users access your application involves two steps:</p> <ol> <li>Create the right DNS CNAME record.</li> <li>Create the right Ingress resource.</li> </ol>","tags":["ISO 27001 A.10.1.2 Key Management","ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"user-guide/network-model/#running-example","title":"Running Example","text":"<p>Let us assume you want to host your application behind the nicely branded domain <code>demo.example.com</code>. Proceed as follows:</p> <p>For step 1, create a DNS CNAME as follows:</p> <pre><code>demo.example.com. 900 CNAME app.$DOMAIN.\n</code></pre> <p>where <code>$DOMAIN</code> is the environment-specific variable you received from the administrator. The line above is presented in DNS Zone file format and is widely accepted by DNS providers.</p> <p>After configuration, make sure the DNS record is properly configured and propagaged, by typing:</p> <pre><code>host -a demo.example.com.\n</code></pre> <p>Important</p> <p>In the above examples, the domain name is fully qualified, i.e., it ends with a dot. Make sure your DNS provider does not mis-interpret it as a relative domain name. Otherwise, you risk creating a DNS record like <code>demo.example.com.example.com</code> which is rarely what you want.</p> <p>Important</p> <p>Be cautious when using CNAMEs and apex domains (e.g., <code>example.com</code>). See here for a long discussion of potential problems and current workarounds.</p> <p>For step 2, create an Ingress object with the right <code>metadata.annotations</code> and <code>spec.tls</code>, as exemplified below:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: myapp-ck8s-user-demo\nannotations:\n# To list your current ClusterIssuers, simply use 'kubectl get ClusterIssuers'.\ncert-manager.io/cluster-issuer: letsencrypt-prod\nkubernetes.io/ingress.class: nginx\n## Uncomment the line below to implement source IP allowlisting.\n## Blocklisted IPs will get HTTP 403.\n# nginx.ingress.kubernetes.io/whitelist-source-range: 98.128.193.2/32\n## Uncomment the lines below to get OAuth authentication\n## You will also need to configure and install oauth2-proxy.\n## For an example and more details, see https://github.com/elastisys/compliantkubernetes/blob/main/user-demo/deploy/oauth2-proxy.yaml\n# nginx.ingress.kubernetes.io/auth-url: \"https://$host/oauth2/auth\"\n# nginx.ingress.kubernetes.io/auth-signin: \"https://$host/oauth2/start?rd=$escaped_request_uri\"\n# nginx.ingress.kubernetes.io/auth-response-headers: \"authorization\"\nspec:\nrules:\n- host: \"demo.example.com\"\nhttp:\npaths:\n- path: /\npathType: Prefix\nbackend:\nservice:\nname: myapp-ck8s-user-demo\nport:\nnumber: 3000\ntls:\n- hosts:\n- \"demo.example.com\"\nsecretName: demo.example.com-tls\n</code></pre> <p>Example</p> <p>Feel free to take inspiration from the user demo.</p> <p>If you want to protect your Ingress with OAuth2-based authentication, check out oauth2-proxy.</p> <p>Important</p> <p>The DNS name in <code>spec.rules[0].host</code> and <code>spec.tls[0].hosts[0]</code> must be the same as the DNS entry used by your application users, in the example above <code>demo.example.com</code>. Otherwise, the application users will get a \"Your connection is not private\" error.</p> <p>Important</p> <p>Some load-balancers fronting Compliant Kubernetes do not preserve source IP. This makes source IP allowlisting unusable.</p> <p>To check if source IP is preserved, check the HTTP request headers received by your application, specifically <code>x-forwarded-for</code> and <code>x-real-ip</code>. The user demo logs all HTTP request headers, as shown in the screenshot below.</p> <p></p>","tags":["ISO 27001 A.10.1.2 Key Management","ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"user-guide/network-model/#letsencrypt","title":"LetsEncrypt","text":"<p>Let\u2019s Encrypt is a certificate authority that provides free SSL/TLS certificates via an automated process. Their certificates are accepted by most of today\u2019s browsers.</p> <p>On Compliant Kubernetes, we provide a cert-manager setup which you can use to create, sign, install and renew certificates for your domains/apps running in CK8S.</p>","tags":["ISO 27001 A.10.1.2 Key Management","ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"user-guide/network-model/#issuing-a-certificate","title":"Issuing a Certificate","text":"<p>You can use cert-manager setup for general purpose certificates not directly linked to an Ingress object.</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\nname: custom-cert\nspec:\ndnsNames: # (1)\n- 'domain.example.com'\nissuerRef: # (2)\nkind: ClusterIssuer\nname: letsencrypt-prod\nsecretName: custom-cert # (3)\n</code></pre> <ol> <li>For which domains the certificate will be valid for.</li> <li>Reference to the issuer to use.</li> <li>The created certificate is stored in this secret.</li> </ol> <p>And you can directly link a certificate to an Ingress object :</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nannotations:\ncert-manager.io/cluster-issuer: letsencrypt-prod # (1)\nname: webapp-ingress\nnamespace: default # (2)\nspec:\nrules:\n- host: example.com\nhttp:\npaths:\n- pathType: Prefix\npath: /\nbackend:\nservice:\nname: myservice\nport:\nnumber: 80\ntls: # (3)\n- hosts:\n- example.com\nsecretName: webapp-certificate # (4)\n</code></pre> <ol> <li>Annotation indicating the issuer to use.</li> <li>Target namespace where the object will be created.</li> <li>Placing a host in the TLS config will determine what ends up in the cert\u2019s subjectAltNames.</li> <li>The created certificate is stored in this secret.</li> </ol>","tags":["ISO 27001 A.10.1.2 Key Management","ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"user-guide/network-model/#lets-encrypt-environments","title":"Let's Encrypt Environments","text":"<p>LetsEncrypt provides two environments as part of their ACME V2 standardization: Staging &amp; Production.</p> <p>The ACME URL for LetsEncrypt ACME v2 staging environment is: https://acme-staging-v02.api.letsencrypt.org/directory</p> <p>The ACME URL for LetsEncrypt ACME v2 production environment is: https://acme-v02.api.letsencrypt.org/directory</p> <p>Both environments serve to issue valid certificates, the difference is the CA, on staging, the CA is not trusted by any application, web browser ..</p> <p>We highly recommend testing against the Let\u2019s Encrypt staging environment and use it for any non-production workloads. This will allow you to get things right before issuing trusted certificates and reduce the chance of you running up against rate limits. It should be to test that your client is working fine and can generate the challenges, certificates\u2026</p> <p>Important</p> <p>Certificates issued by the LetsEncrypt staging environment are signed by untrusted authorities, similar to self-signed certificates. They are typically not used in production environments.</p>","tags":["ISO 27001 A.10.1.2 Key Management","ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"user-guide/network-model/#rate-limits","title":"Rate Limits","text":"<p>LetsEncrypt provides rate-limits on generated certificates to ensure fair usage across all clients. The production environment limits can be exceeded more frequently in environments certificates are installed or reinstalled frequently. This can result in failed installations due to rate limit exceptions on certificate generation.</p> <p>In such environments, it is better to use the LetsEncrypt staging environment, which has much higher limits than the production environment.</p> <p>The default rate limits for the production environment are listed in the following page by letencrypt : Production Rate Limits.</p> <p>The staging environment uses the same rate limits as described for the production environment with some exceptions that are listed here : Staging Rate Limits.</p>","tags":["ISO 27001 A.10.1.2 Key Management","ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"user-guide/network-model/#demarcation-of-responsibilities","title":"Demarcation of Responsibilities","text":"<p>You are responsible for:</p> <ul> <li>creating Pods (via Deployments), Service and Ingress;</li> <li>segregating the private network via NetworkPolicies;</li> <li>configuring Ingresses as required to enable HTTPS encryption.</li> </ul> <p>The user demo already showcases the above.</p> <p>The Compliant Kubernetes administrator is responsible for:</p> <ul> <li>ensuring cert-manager works and is configured correctly;</li> <li>ensuring ClusterIssuers exist and are configured correctly;</li> <li>ensure the private network is secure or trusted.</li> </ul>","tags":["ISO 27001 A.10.1.2 Key Management","ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"user-guide/network-model/#further-reading","title":"Further Reading","text":"<ul> <li>DNS for Services and Pods</li> <li>Ingress</li> <li>NetworkPolicies</li> </ul>","tags":["ISO 27001 A.10.1.2 Key Management","ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t"]},{"location":"user-guide/operate/","title":"Step 3: Operate","text":"<p>Welcome to the third and final step, Application Developer!</p> <p>In this step, you will learn how to operate your application on Elastisys Compliant Kubernetes.</p>"},{"location":"user-guide/operate/#configure-dashboards-and-alerts","title":"Configure Dashboards and Alerts","text":""},{"location":"user-guide/operate/#monitor-your-application","title":"Monitor your Application","text":"<p>To monitor your application, you will log in to your Grafana. Recall how to log in to your web portals from Step 1: Prepare.</p> <p>Grafana visually displays the monitoring data that Prometheus has collected on your behalf. A significant amount of metrics are already collected for you, out of the box, on Elastisys Compliant Kubernetes. This means you can visualize data about the cluster immediately.</p> <p>But Prometheus can also be instructed to collect specific metrics from your own application. Perhaps this is more useful to you than monitoring metrics that relate to cluster health (in particular if somebody else managed Elastisys Compliant Kubernetes for you).</p> <p>To instruct Promethus on how to do this, you create a ServiceMonitor. This is a Kubernetes resource that configures Prometheus and specifies how to collect metrics from a particular application.</p> <p>The user demo already includes a ServiceMonitor, as required for Compliant Kubernetes to collect metrics from its <code>/metrics</code> endpoint:</p> <pre><code>{{- if .Values.serviceMonitor.enabled -}}\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\nname: {{ include \"ck8s-user-demo.fullname\" . }}\nlabels:\n{{- include \"ck8s-user-demo.labels\" . | nindent 4 }}\nspec:\nselector:\nmatchLabels:\n{{- include \"ck8s-user-demo.selectorLabels\" . | nindent 6 }}\nendpoints:\n- port: http\n{{- end }}\n</code></pre> <p>The screenshot below shows Grafana in \"Explore\" mode (the compass icon to the left) featuring the query <code>rate(http_request_duration_seconds_count[1m])</code>. It shows the request rate for the user demo application for each path and status code. As can be seen in the graph, the <code>/users</code> endpoint is getting more traffic than the other endpoints.</p> <p></p> <p>The \"Explore\" mode is great for developing queries and exploring the data set. If you want to save a query so you can refer back to it, you can create a Dashboard instead. Dashboards consist of multiple Panels, each of which, can display the results of running queries. Learn more about Grafana panels.</p> <p>Note</p> <p>You may want to save frequently used Dashboards. Compliant Kubernetes saves and backs these up for you.</p> <p>Go deeper into metrics.</p>"},{"location":"user-guide/operate/#alert-on-application-metrics","title":"Alert on Application Metrics","text":"<p>Visualizing monitoring metrics is one thing. Sometimes, you may need to act on what they show, immediately. For that reason, the Prometheus monitoring system includes AlertManager.</p> <ul> <li>Prometheus is responsible for maintaining a set of Rules, which express trigger conditions via expressions. Once a rule has triggered, it has entered an alerting state.</li> <li>AlertManager is responsible for forwarding information about any rules in the alerting state to your chosen destination, which could be your company's Slack or similar. A number of integrations are available.</li> </ul> <p>If you wish to create rules based on application-specific monitoring metrics, you must first create appropriate ServiceMonitors as described above.</p> <p>The user demo already includes a PrometheusRule, to configure an alert:</p> <pre><code>{{- if .Values.prometheusRule.enabled -}}\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\nname: {{ include \"ck8s-user-demo.fullname\" . }}\nlabels:\n{{- include \"ck8s-user-demo.labels\" . | nindent 4 }}\nspec:\ngroups:\n- name: ./example.rules\nrules:\n- alert: ApplicationIsActuallyUsed\nexpr: rate(http_request_duration_seconds_count[1m])&gt;1\n{{- end }}\n</code></pre> <p>The screenshot below gives an example of the application alert, as seen in AlertManager.</p> <p></p> <p>Go deeper into metric alerts.</p>"},{"location":"user-guide/operate/#alert-on-log-contents","title":"Alert on Log Contents","text":"<p>Similar to alerting based on monitoring metrics, you may need to alert based on application log contents. For instance, it might make sense to send any log line of the <code>FATAL</code> log level to your Slack channel for immediate attention.</p> <p>The process of setting up log-based alerts is highly graphical, and supported by your OpenSearch Dashboards that is part of Elastisys Compliant Kubernetes. Recall how to log in to your web portals from Step 1: Prepare.</p> <p>Go deeper into log-based alerts.</p>"},{"location":"user-guide/operate/#test-backups-and-capacity-management","title":"Test Backups and Capacity Management","text":"<p>Disaster recovery is about so much more than backing up and restoring data. Backing up data is a necessary, but not sufficient, part of that.</p> <p>Not having sufficient capacity is also a kind of disaster, albeit, one that is easy to mitigate.</p>"},{"location":"user-guide/operate/#back-up-application-data","title":"Back up Application Data","text":"<p>Compliant Kubernetes takes a daily backup of all Kubernetes Resources in all user namespaces. Persistent Volumes will be backed up if they are tied to a Pod. If backups are not wanted the label <code>compliantkubernetes.io/nobackup</code> can be added to opt-out of the daily backups.</p> <p>Application metrics (Grafana) and application log (Kibana) dashboards are also backup up by default.</p> <p>By default, backups are stored for 720 hours (30 days).</p> <p>Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first.</p> <p>To restore a backup on demand, contact your Compliant Kubernetes administrator.</p> <p>Go deeper into backups.</p>"},{"location":"user-guide/operate/#capacity-management","title":"Capacity Management","text":"<p>Capacity management is about having sufficient capacity for your needs, be they in terms of storage or computational power.</p> <p>Your Elastisys Compliant Kubernetes administrator should perform capacity management of the platform, to ensure that there is a sufficient amount of spare capacity on a cluster level.</p> <p>As an application developer, you should perform capacity management on a Pod level. This primarily means setting resource requests correctly for containers inside Pods, making use of multiple instances in your Deployments and Stateful Sets (possibly via horizontal Pod autoscaling). The use of resource requests and limits is enforced via an Elastisys Compliant Kubernetes safeguard.</p>"},{"location":"user-guide/operate/#automate-with-cicd","title":"Automate with CI/CD","text":"<p>Elastisys Compliant Kubernetes currently does not dictate or recommend any particular CI/CD solution over any other. It is, however, easy to integrate with various CI/CD solutions, such as GitHub Actions.</p> <p>The basic steps for a generic push-style CI/CD solution (such as GitHub Actions) are to:</p> <ol> <li>Create a limited <code>Role</code>, that has the least possible privileges required to deploy your application.</li> <li>Create a <code>ServiceAccount</code> and binding to the role created earlier via a <code>RoleBinding</code>, granting it the permissions needed for deploying the application.</li> <li>Getting the token for the ServiceAccount, so you can craft a <code>KUBECONFIG</code> to use with <code>kubectl</code> or <code>helm</code> in your CI/CD solution.</li> </ol> <p>Adding an in-cluster CI/CD solution is a work in progress, pending security reviews of alternatives in the ecosystem.</p> <p>Go deeper into CI/CD.</p>"},{"location":"user-guide/operate/#next-step-going-deeper","title":"Next step? Going deeper!","text":"<p>By now, you're fully up and running! You have an application, updating it is a breeze, and you can monitor it and look at its logs. The next step is to open the \"Go deeper\" section of this documentation and read up on more topics that interest you.</p> <p>Thank you for starting your journey beyond the clouds with Elastisys Compliant Kubernetes!</p>"},{"location":"user-guide/prepare-application/","title":"Prepare Your Application","text":"<p>To make the most out of Compliant Kubernetes, prepare your application so it features:</p> <ul> <li>some REST endpoints: NodeJS, .NET;</li> <li>structured logging: NodeJS, .NET;</li> <li>metrics endpoint: NodeJS, .NET;</li> <li>Dockerfile, which showcases:<ul> <li>How to run as non-root: NodeJS, .NET;</li> </ul> </li> <li>Helm Chart, which showcases:<ul> <li>HTTPS Ingresses;</li> <li>ServiceMonitor for metrics collection;</li> <li>PrometheusRule for alerting;</li> <li>topologySpreadConstraints for tolerating single Node or single Zone failure;</li> <li>resources for capacity management;</li> <li>NetworkPolicies for network segmentation;</li> </ul> </li> <li>Grafana dashboards for metrics visualization;</li> <li>script for local development and testing;</li> </ul> <p>Bonus:</p> <ul> <li>ability to make it crash (<code>/crash</code>).</li> </ul> <p>Feel free to clone our user demo for inspiration:</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes/\ncd compliantkubernetes/user-demo\n</code></pre>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","BSI IT-Grundschutz APP.4.4.A21"]},{"location":"user-guide/prepare-application/#make-sure-your-application-tolerates-nodes-replacement","title":"Make Sure Your Application Tolerates Nodes Replacement","text":"<p>Important</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.12.6.1 Management of Technical Vulnerabilities</li> </ul> <p>Compliant Kubernetes recommends against PodDisruptionBudgets (PDBs). PDBs can easily be misconfigured to block draining Nodes, which interferes with automatic OS patching and compromises the security posture of the environment. Instead, prefer engineering your application to deal with disruptions. The user demo already showcases how to achieve this with replication and topologySpreadConstraints. Make sure to move state, even soft state, to specialized services.</p> <p>Further reading:</p> <ul> <li>Dealing with Disruptions</li> </ul>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","BSI IT-Grundschutz APP.4.4.A21"]},{"location":"user-guide/prepare/","title":"Step 1: Prepare","text":"<p>Hi there, Application Developer! Happy to have you on board with Elastisys Compliant Kubernetes!</p> <p>In this part, you will learn about the things you should do to prepare to get started with the platform.</p> <p>We assume somebody else, your administrator, has already set up the platform for you. You will therefore have received:</p> <ul> <li>URLs for the Elastisys Compliant Kubernetes UI components: OpenSearch Dashboards, Grafana, and Harbor;</li> <li>a kubeconfig file for configuring <code>kubectl</code> or Lens access to the underlying Kubernetes cluster; and</li> <li>(optionally and rarely) a static username and password. Note that normally, you should log in via a username and a password of your organization's Identity Provider, such as LDAP, Active Directory, or Google Workspaces account.</li> </ul> <p>Do you not already have an Elastisys Compliant Kubernetes platform up and running? Request one from a managed service provider and get started!</p>"},{"location":"user-guide/prepare/#install-prerequisite-software","title":"Install Prerequisite Software","text":"<p>Required software:</p> <ul> <li>oidc-login, which helps you log into your Kubernetes cluster via OpenID Connect integration with your Identity Provider of choice</li> </ul> <p>Your cluster management software of choice, of which you can choose either or both:</p> <ul> <li>kubectl, a command-line tool to help manage your Kubernetes resources</li> <li>OpenLens, a graphical user interface to help manage your Kubernetes resources (see also our dedicated page on Lens integration)</li> </ul> <p>Optional, but very useful, tools for developers and DevOps engineers:</p> <ul> <li>docker, if you want to build (Docker) container images locally</li> <li>helm, if you want to manage your application with the Helm package manager</li> </ul> <p>Once installed, you can verify that configuration is correct by issuing a few simple commands.</p>"},{"location":"user-guide/prepare/#access-your-web-portals","title":"Access Your Web Portals","text":"<p>Those URLs that your Elastisys Compliant Kubernetes administrator gave you all have a <code>$DOMAIN</code>, which will typically include your company name and perhaps the environment name.</p> <p>Your web portals are available at:</p> <ul> <li><code>harbor.$DOMAIN</code> -- the Harbor container image registry, which will be the home to all your container images</li> <li><code>opensearch.$DOMAIN</code> -- the OpenSearch Dashboards portal, where you will view your application and audit logs</li> <li><code>grafana.$DOMAIN</code> -- the Grafana portal, where you will view your monitoring metrics for both the platform, as such, and your application-specific metrics</li> </ul>"},{"location":"user-guide/prepare/#containerize-your-application","title":"Containerize Your Application","text":"<p>Elastisys Compliant Kubernetes runs containerized applications in a Kubernetes platform. It is a Certified Kubernetes distribution, which means that if an application is possible to deploy on a standard Kubernetes environment, it can be deployed on Elastisys Compliant Kubernetes.</p> <p>However, there are some restrictions in place for security reasons. In particular, containers cannot be run as root. Following this best practice is a simple way to ensure additional security for your containerized applications deployed in Kubernetes.</p> <p>There are additional safeguards in place that reflect the security posture of Elastisys Compliant Kubernetes that impact your application. These prevent users from doing potentially unsafe things. In particular, users are not allowed to:</p> <ul> <li>change the Kubernetes API through CustomResourceDefinitions or Dynamic Webhooks;</li> <li>gain more container execution permissions by mutating PodSecurityPolicies; this implies that you cannot run container images as root or mount hostPaths;</li> <li>mutate ClusterRoles or Roles so as to escalate privileges;</li> <li>mutate Kubernetes resources in administrator-owned namespaces, such as <code>monitoring</code> or <code>kube-system</code>;</li> <li>re-configure system Pods, such as Prometheus or fluentd;</li> <li>access the hosts directly.</li> </ul>"},{"location":"user-guide/prepare/#next-step-deploying","title":"Next step? Deploying!","text":"<p>Ready with a containerized application? Head over to the next step, where you learn how to deploy it!</p>"},{"location":"user-guide/registry/","title":"Harbor - private container registry","text":"<p>This guide gives an introduction to Harbor and where it fits in Compliant Kubernetes, in terms of reducing the compliance burden.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"user-guide/registry/#what-is-a-container-registry-and-why-it-is-needed","title":"What is a container registry and why it is needed?","text":"<p>A container registry is a system where you can store your container images in order to later use the images when you deploy your application (e.g. as a Pod in a Kubernetes cluster). The images need a permanent storage since they are used many times by different instances, especially in Kubernetes where Pods (which are using the images) are considered ephemeral, so it is not enough to just store images directly on nodes/virtual machines. There are many popular container registries available as services, e.g. Docker Hub and Google Container Registry.</p> <p>A common workflow with container registries is to build your images in a CI/CD pipeline, push the images to your registry, let the pipeline change your deployments that uses the images, and let the deployments pull down the new images from the repository.</p> <p></p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"user-guide/registry/#what-is-harbor","title":"What is Harbor?","text":"<p>Harbor is an open source container registry tool that allows you to host a registry privately. It also comes with some extra features such as vulnerability scanning and role based access control, this increases security and eases compliance with certain regulations. Harbor is also a CNCF Graduated project, proving that it is widely used and is well supported.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"user-guide/registry/#why-is-harbor-used-in-compliant-kubernetes","title":"Why is Harbor used in Compliant Kubernetes?","text":"<p>Harbor is used in Compliant Kubernetes to provide a secure container registry and a way to manage container image vulnerabilities. Harbor comes packaged with a container image vulnerability scanner that can check if there are any known vulnerabilities in the images you upload to Harbor. The default scanner is Trivy, which provides a comprehensive vulnerability detection both at the OS package and language-specific package levels.</p> <p>Below you can see both an image that has not been scanned and the same image after it has been scanned. After the image is scanned you can see the description, vulnerable package, and severity of each vulnerability as well as if it has been fixed in a later version. You can either scan the images manually or enable automatic scanning whenever a new image is pushed to Harbor, we recommend automatic scanning.</p> <p> </p> <p>In Harbor you can then also restrict so that you can't pull down images that have vulnerabilities of a certain severity or higher. This ensures that you don't accidentally start to use vulnerable images.</p> <p>If you try to deploy a Pod that uses a vulnerable image it will fail to pull the image. When you then inspect the Pod with <code>kubectl describe</code> you will find an error message similar to this:</p> <pre><code>Failed to pull image \"harbor.test.compliantkubernetes.io/test/ubuntu\": rpc error: code = Unknown desc = Error response from daemon: unknown: current image with 77 vulnerabilities cannot be pulled due to configured policy in 'Prevent images with vulnerability severity of \"Medium\" or higher from running.' To continue with pull, please contact your project administrator to exempt matched vulnerabilities through configuring the CVE whitelist.\n</code></pre> <p>By default we also prevent you from running images from anywhere else than your Harbor instance. This is to ensure that you use all of these security features and don't accidentally pull down vulnerable images from other container registries. We are using Open Policy Agent and Gatekeeper to manage this prevention. If you try to deploy a Pod with an image from another registry you will get an error message similar to this:</p> <pre><code>for: \"unsafe-image.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-harbor-repo] container \"unsafe-container\" has an invalid image repo \"unsafe.registry.io/ubuntu\", allowed repos are [\"harbor.test.compliantkubernetes.io\"]\n</code></pre>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"user-guide/registry/#running-example","title":"Running Example","text":"","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"user-guide/registry/#configure-container-registry-credentials","title":"Configure container registry credentials","text":"<p>First, retrieve your Harbor CLI secret and configure your local Docker client.</p> <ol> <li>In your browser, type <code>harbor.$DOMAIN</code> where <code>$DOMAIN</code> is the information you retrieved from your administrator.</li> <li>Log into Harbor using Single Sign-On (SSO) via OpenID.</li> <li>In the right-top corner, click on your username, then \"User Profile\".</li> <li>Copy your CLI secret.</li> <li>Now log into the container registry: <code>docker login harbor.$DOMAIN</code>.</li> <li>You should see <code>Login Succeeded</code>.</li> </ol>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"user-guide/registry/#create-a-registry-project","title":"Create a registry project","text":"<p>Example</p> <p>Here is an example Dockerfile and .dockerignore to get you started. Don't forget to run as non-root.</p> <p>If you haven't already done so, create a project called <code>demo</code> via the Harbor UI, which you have accessed in the previous step.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"user-guide/registry/#clone-the-user-demo","title":"Clone the user demo","text":"<p>If you haven't done so already, clone the user demo:</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes/\ncd compliantkubernetes/user-demo\n</code></pre>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"user-guide/registry/#build-and-push-the-image","title":"Build and push the image","text":"<pre><code>REGISTRY_PROJECT=demo  # Name of the project, created above\nTAG=v1                 # Container image tag\n\ndocker build -t harbor.$DOMAIN/$REGISTRY_PROJECT/ck8s-user-demo:$TAG .\ndocker push harbor.$DOMAIN/$REGISTRY_PROJECT/ck8s-user-demo:$TAG\n</code></pre> <p>You should see no error message. Note down the <code>sha256</code> of the image.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"user-guide/registry/#verification","title":"Verification","text":"<ol> <li>Go to <code>harbor.$DOMAIN</code>.</li> <li>Choose the <code>demo</code> project.</li> <li>Check if the image was uploaded successfully, by comparing the tag's <code>sha256</code> with the one returned by the <code>docker push</code> command above.</li> <li>(Optional) While you're at it, why not run the vulnerability scanner on the image you just pushed.</li> </ol>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"user-guide/registry/#user-access","title":"User access","text":"<p>If OIDC was enabled (e.g. DeX) your Harbor user will be created when you first login to the web interface. That user will not have admin privileges, if you need admin rights please contact the administrator by opening a support ticket.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"user-guide/registry/#further-reading","title":"Further reading","text":"<p>For more information please refer to the official Harbor, Trivy, Open Policy Agent and Gatekeeper documentation.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7"]},{"location":"user-guide/setup/","title":"Install Prerequisites","text":"<p>As a user, you will need the following before you get started with Compliant Kubernetes:</p> <p>Required software:</p> <ul> <li>oidc-login, which helps you log into your Kubernetes cluster via OpenID Connect integration with your Identity Provider of choice</li> </ul> <p>Your cluster management software of choice, of which you can choose either or both:</p> <ul> <li>kubectl, a command-line tool to help manage your Kubernetes resources</li> <li>OpenLens, a graphical user interface to help manage your Kubernetes resources (see also our dedicated page on Lens integration)</li> </ul> <p>Optional, but very useful, tools for developers and DevOps engineers:</p> <ul> <li>docker, if you want to build (Docker) container images locally</li> <li>helm, if you want to manage your application with the Helm package manager</li> </ul>"},{"location":"user-guide/setup/#verify-your-prerequisite-software-and-its-configuration","title":"Verify Your Prerequisite Software and its Configuration","text":"<p>The easiest way to get started is to request a working installation from a managed Compliant Kubernetes provider. This means you will receive:</p> <ul> <li>URLs for the Elastisys Compliant Kubernetes UI components: OpenSearch Dashboards, Grafana, and Harbor;</li> <li>a kubeconfig file for configuring <code>kubectl</code> or Lens access to the underlying Kubernetes cluster; and</li> <li>(optionally and rarely) a static username and password. Note that normally, you should log in via a username and a password of your organization's Identity Provider, such as LDAP, Active Directory, or Google Workspaces account.</li> </ul> <p>Make sure you have configured your tools properly:</p> <pre><code>export KUBECONFIG=path/of/kubeconfig.yaml  # leave empty if you use the default of ~/.kube/config\nexport DOMAIN=  # the domain you received from the administrator\n</code></pre> <p>To verify if the required tools are installed and work as expected, type:</p> <pre><code>docker version\nkubectl version  --client\nhelm version\n# You should see the version number of installed tools and no errors.\n</code></pre> <p>To verify the received KUBECONFIG, type:</p> <pre><code># Notice that you will be asked to complete browser-based single sign-on\nkubectl get nodes\n# You should see the Nodes of your Kubernetes cluster\n</code></pre> <p>To verify the received URLs, type:</p> <pre><code>curl --head https://dex.$DOMAIN/healthz\ncurl --head https://harbor.$DOMAIN/healthz\ncurl --head https://grafana.$DOMAIN/healthz\ncurl --head https://opensearch.$DOMAIN/api/status\ncurl --insecure --head https://app.$DOMAIN/healthz  # Ingress Controller\n# All commands above should return 'HTTP/2 200'\n</code></pre>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting","text":"<p>Going through these basic troubleshooting steps should help you as an application developer identify where a problem may lie. If any of these steps do not give the expected \"fine\" output, use <code>kubectl describe</code> to investigate.</p> <p>If you are using Lens instead of the <code>kubectl</code> command-line interface, clicking through your Deployments and Pods will reveal the same information as the commands given below.</p>"},{"location":"user-guide/troubleshooting/#is-the-kubernetes-cluster-fine","title":"Is the Kubernetes cluster fine?","text":"<p>All Nodes need to have status <code>Ready</code>.</p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"user-guide/troubleshooting/#are-my-application-pods-fine","title":"Are my application Pods fine?","text":"<p>Pods should be <code>Running</code> or <code>Completed</code>, and fully <code>Ready</code> (e.g., <code>1/1</code> or <code>6/6</code>)?</p> <pre><code>kubectl get pods\n</code></pre> <p>Check your Pods for excessive resource usage:</p> <pre><code>kubectl top pod\n</code></pre> <p>Inspect application logs and metrics.</p>"},{"location":"user-guide/troubleshooting/#are-my-deployments-fine","title":"Are my Deployments fine?","text":"<p>Are all Deployments fine? Deployments should show all Pods Ready, Up-to-date and Available (e.g., <code>2/2 2 2</code>).</p> <pre><code>kubectl get deployments\n</code></pre>"},{"location":"user-guide/troubleshooting/#are-helm-releases-fine","title":"Are Helm Releases fine?","text":"<p>All Releases should be <code>deployed</code>.</p> <pre><code>helm list --all\n</code></pre>"},{"location":"user-guide/troubleshooting/#are-my-certificates-fine","title":"Are my Certificates fine?","text":"<p>All Certificates needs to be Ready.</p> <pre><code>kubectl get certificates\n</code></pre>"},{"location":"user-guide/troubleshooting/#is-the-api-server-healthy","title":"Is the API server healthy?","text":"<p>The command below should return <code>HTTP/2 200</code>. <pre><code>curl --fail --verbose -k https://$loadbalancer_ip_address:6443/healthz\n</code></pre></p>"},{"location":"user-guide/troubleshooting/#are-compliantkubernetes-apps-services-healthy","title":"Are Compliantkubernetes apps services healthy?","text":"<p>All commands below should return <code>HTTP/2 200</code>.</p> <pre><code>curl --fail --verbose https://dex.$DOMAIN/healthz\ncurl --fail --verbose https://harbor.$DOMAIN/healthz\ncurl --fail --verbose https://grafana.$DOMAIN/healthz\ncurl --fail --verbose https://opensearch.$DOMAIN/\ncurl --fail --verbose -k https://app.$DOMAIN/healthz  # WC Ingress Controller\n</code></pre>"},{"location":"user-guide/additional-services/","title":"Additional Services","text":"<p>Compliant Kubernetes simplifies usage of a complex and diverse infrastructure. By exposing simple and uniform concepts, it allows you to focus on application development.</p> <p>However, your application needs more than just running stateless containers. At the very least, you will need a database -- such as PostgreSQL -- to persist data. More complex applications will require a distributed cache -- such as Redis -- to store session information or offload the database. Finally, background tasks are best handled by separate containers, connected to your user-facing backend code via a message queue -- such as RabbitMQ.</p> <p>These additional services need to be delivered as securely as the rest of the platform. Access control, business continuity, disaster recovery, security patching and maintenance need to be a core feature, not an afterthought.</p> <p>It turns out, the same simple and uniform concepts that benefit your application can also be used to simplify hosting additional services. And thanks to security-hardening included in Compliant Kubernetes, the burden of delivering additional services with the security you need is also reduced.</p> <p>Compliant Kubernetes is the \"hourglass waist\" of the platform. Think of it like HTTPS being the \"hourglass waist\" of the Internet: It unites the sprawl of wired and wireless network technologies to offer a uniform concept on which various web, gaming, chat and video streaming protocols can run.</p> <p>In the end, you win by having a feature-full platform to host your application. Not just VMs, but useful services. Administrators win by avoiding to re-invent the wheel and focus on the specifics of each additional service.</p> <p>This section of the user guide will help you benefit the most from the additional services hosted within Compliant Kubernetes.</p>","tags":["BSI IT-Grundschutz APP.4.4.A16"]},{"location":"user-guide/additional-services/argocd/","title":"Argo CD\u2122","text":"<p>This page will help you succeed in connecting to Argo CD application which meets your security and compliance requirements.</p>"},{"location":"user-guide/additional-services/argocd/#provision-a-new-argo-cd-cluster","title":"Provision a New Argo CD Cluster","text":"<p>Ask your service-specific administrator to install a Argo CD inside your Compliant Kubernetes environment.The service-specific administrator will ensure the Argo CD cluster complies with your security requirements, including:</p> <ul> <li>Business continuity: Due to its functionality, ArgoCD does not need high availability and Kubernetes's self-healing is sufficient for business continuity of ArgoCD. Hence, it is sufficient to run only one replica of ArgoCD provided at least two Nodes have capacity to host it.</li> <li>Disaster recovery: Your service-specific administrator will configure velero with regular backups.</li> <li>Capacity management: Your service-specific administrator will ensure Argo CD has enough capacity to meet your needs,as required to get the best performance.</li> <li>Incident management: Your administrator will set up the necessary probes, dashboards and alerts, to discover issues and resolve them, before they become a problem.</li> <li>Access control: Your administrator will provides Argo CD URL that has authentication using Dex.</li> </ul> <p>For installing ArgoCD, Compliant Kubernetes recommends the Argo CD Helm Chart.</p>"},{"location":"user-guide/additional-services/argocd/#getting-access","title":"Getting Access","text":"<p>Your administrator will set up the authentication inside Compliant Kubernetes, which will give you access to ArgoCD UI.</p>"},{"location":"user-guide/additional-services/argocd/#follow-the-go-live-checklist","title":"Follow the Go-Live Checklist","text":"<p>You should be all set. Before going into production, don't forget to go through the go-live checklist.</p>"},{"location":"user-guide/additional-services/argocd/#ck8s-argo-cd-release-notes","title":"CK8S Argo CD Release Notes","text":"<p>Check out the release notes for the Argo CD setup that runs in Compliant Kubernetes environments!</p>"},{"location":"user-guide/additional-services/argocd/#further-reading","title":"Further Reading","text":"<ul> <li>ArgoCD documentation</li> </ul>"},{"location":"user-guide/additional-services/jaeger/","title":"Jaeger\u00ae","text":"<p>This page will help you succeed in connecting your application to Jaeger tracing  which meets your security and compliance requirements.</p>"},{"location":"user-guide/additional-services/jaeger/#provision-a-new-jaeger-cluster","title":"Provision a New Jaeger Cluster","text":"<p>Ask your service-specific administrator to install a Jaeger cluster inside your Compliant Kubernetes environment. The service-specific administrator will ensure the Jaeger cluster complies with your security requirements, including:</p> <ul> <li>Business continuity: We recommend a highly available setup with 3 instances for Jaeger and Opensearch.</li> <li>Disaster recovery: Your service-specific administrator will configure Opensearch with regular backups.</li> <li>Capacity management: Your service-specific administrator will ensure Jaeger and Opensearch runs on dedicated (i.e., tainted) Kubernetes Nodes, as required to get the best performance.</li> <li>Incident management: Your administrator will set up the necessary probes, dashboards and alerts, to discover issues and resolve them, before they become a problem.</li> <li>Access control: Your administrator will set up a reverse proxy that provides authentication using Dex.</li> </ul> <p>Compliant Kubernetes recommends the Jaeger operator.</p>"},{"location":"user-guide/additional-services/jaeger/#getting-access","title":"Getting Access","text":"<p>Your administrator will set up the authentication reverse proxy inside Compliant Kubernetes, which will give you access to JaegerUI.</p>"},{"location":"user-guide/additional-services/jaeger/#prepare-your-application","title":"Prepare your application","text":"<p>The Jaeger agent is exposed as a DaemonSet. Your application needs to be told where the agent is located by setting the environment variable JAEGER_AGENT_HOST to the value of the Kubernetes node\u2019s IP:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nspec:\nselector:\n...\ntemplate:\n...\nspec:\ncontainers:\n- name: my-app\nimage: acme/my-app:my-version\nenv:\n- name: JAEGER_AGENT_HOST\nvalueFrom:\nfieldRef:\nfieldPath: status.hostIP\n</code></pre>"},{"location":"user-guide/additional-services/jaeger/#follow-the-go-live-checklist","title":"Follow the Go-Live Checklist","text":"<p>You should be all set. Before going into production, don't forget to go through the go-live checklist.</p>"},{"location":"user-guide/additional-services/jaeger/#ck8s-jaeger-release-notes","title":"CK8S Jaeger Release Notes","text":"<p>Check out the release notes for the Jaeger setup that runs in Compliant Kubernetes environments!</p>"},{"location":"user-guide/additional-services/jaeger/#further-reading","title":"Further Reading","text":"<ul> <li>Jaeger documentation</li> </ul>"},{"location":"user-guide/additional-services/postgresql/","title":"PostgreSQL\u00ae","text":"<p>This page will help you succeed in connecting your application to a primary relational database PostgreSQL which meets your security and compliance requirements.</p>"},{"location":"user-guide/additional-services/postgresql/#provision-a-new-postgresql-cluster","title":"Provision a New PostgreSQL Cluster","text":"<p>Ask your service-specific administrator to install a PostgreSQL cluster inside your Compliant Kubernetes environment. The service-specific administrator will ensure the PostgreSQL cluster complies with your security requirements, including:</p> <ul> <li>Business continuity: We recommend a highly available setup with at minimum a primary instance and a replica. Ideally, the PostgreSQL cluster should be configured with a primary and two replicas.</li> <li>Disaster recovery: Your service-specific administrator will configure the PostgreSQL cluster with physical backups, logical backups and Point-in-Time Recovery (PITR), as required to meet your Recovery Point Objectives.</li> <li>Capacity management: Your service-specific administrator will ensure PostgreSQL runs on dedicated (i.e., tainted) Kubernetes Nodes, as required to get the best performance.</li> <li>Incident management: Your administrator will set up the necessary Probes, dashboards and alerts, to discover issues and resolve them, before they become a problem.</li> <li>Access control: Your administrator will set up a \"root-like\" PostgreSQL account, which will allow you to create databases and PostgreSQL users, but not tamper will logging, business continuity or disaster recovery.</li> </ul> <p>Compliant Kubernetes recommends the Zalando PostgreSQL operator.</p>"},{"location":"user-guide/additional-services/postgresql/#install-prerequisites","title":"Install Prerequisites","text":"<p>Before continuing, make sure you have access to the Kubernetes API, as describe here.</p> <p>Make sure to install the PostgreSQL client on your workstation. On Ubuntu, this can be achieved as follows:</p> <pre><code>sudo apt-get install postgresql-client\n</code></pre>"},{"location":"user-guide/additional-services/postgresql/#getting-access","title":"Getting Access","text":"<p>Your administrator will set up a Secret inside Compliant Kubernetes, which contains all information you need to access your PostgreSQL cluster. The Secret has the following shape:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: $SECRET\nnamespace: $NAMESPACE\nstringData:\n# PGHOST represents a cluster-scoped DNS name or IP, which only makes sense inside the Kubernetes cluster.\n# E.g., postgresql1.postgres-system.svc.cluster.local\nPGHOST: $PGHOST\n\n# These fields map to the environment variables consumed by psql.\n# Ref https://www.postgresql.org/docs/13/libpq-envars.html\nPGUSER: $PGUSER\nPGPASSWORD: $PGPASSWORD\nPGSSLMODE: $PGSSLMODE\n\n# This is the Kubernetes Service name to which you can port-foward to in order to get access to the PostgreSQL cluster from outside the Kubernetes cluster.\n# Ref https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/\nUSER_ACCESS: $USER_ACCESS\n</code></pre> <p>Important</p> <p>The Secret is very precious! Prefer not to persist any information extracted from it, as shown below.</p> <p>To extract this information, proceed as follows:</p> <pre><code>export SECRET=            # Get this from your administrator\nexport NAMESPACE=         # Get this from your administrator\n\nexport PGHOST=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGHOST}' | base64 --decode)\nexport PGUSER=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGUSER}' | base64 --decode)\nexport PGPASSWORD=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGPASSWORD}' | base64 --decode)\nexport PGSSLMODE=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGSSLMODE}' | base64 --decode)\nexport USER_ACCESS=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.USER_ACCESS}' | base64 --decode)\n</code></pre> <p>Important</p> <p>Do not configure your application with the PostgreSQL admin username and password. Since the application will get too much permission, this will likely violate your access control policy.</p> <p>Important</p> <p>If you change the password for $PGUSER, you are responsible for keeping track of the new password.</p>"},{"location":"user-guide/additional-services/postgresql/#create-an-application-user","title":"Create an Application User","text":"<p>First, in one console, fetch the information from the access Secret as described above and port forward into the PostgreSQL master.</p> <pre><code>kubectl -n $NAMESPACE port-forward svc/$USER_ACCESS 5432\n</code></pre> <p>Important</p> <p>Since humans are bad at generating random passwords, we recommend using pwgen.</p> <p>Second, in another console, fetch the information from the access Secret again and run the PostgreSQL client to create the application database and user:</p> <pre><code>export APP_DATABASE=myapp\nexport APP_USERNAME=myapp\nexport APP_PASSWORD=$(pwgen 32)\n\ncat &lt;&lt;EOF | psql -d postgres -h 127.0.0.1 -U $PGUSER \\\n    --set=APP_DATABASE=$APP_DATABASE \\\n    --set=APP_USERNAME=$APP_USERNAME \\\n    --set=APP_PASSWORD=$APP_PASSWORD\ncreate database :APP_DATABASE;\ncreate user :APP_USERNAME with encrypted password ':APP_PASSWORD';\ngrant all privileges on database :APP_DATABASE to :APP_USERNAME;\nEOF\n</code></pre> <p>Continue with the second console in the next section to create a Secret with this information.</p>"},{"location":"user-guide/additional-services/postgresql/#create-an-application-secret","title":"Create an Application Secret","text":"<p>First, check that you are on the right Compliant Kubernetes cluster, in the right application namespace:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre> <p>Now, create a Kubernetes Secret in your application namespace to store the PostgreSQL application username and password. For consistency, prefer sticking to naming connection parameters as the environment variables consumed by psql.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n    name: app-postgresql-secret\ntype: Opaque\nstringData:\n    PGHOST: ${PGHOST}\n    PGPORT: '5432'\n    PGSSLMODE: ${PGSSLMODE}\n    PGUSER: ${APP_USERNAME}\n    PGPASSWORD: ${APP_PASSWORD}\n    PGDATABASE: ${APP_DATABASE}\nEOF\n</code></pre> <p>Warning</p> <p>Although most client libraries follow the <code>libpq</code> definition of these environment variables, some do not, and this will require changes to the application Secret.</p> <p>Notably <code>node-postgres</code> does not currently do so for <code>PGSSLMODE</code>. When this variable is set to <code>require</code>, it will do a full verification instead, requiring access to the PostgreSQL certificates to allow a connection. To get the intended mode for <code>require</code> set the variable to <code>no-verify</code> instead.</p>"},{"location":"user-guide/additional-services/postgresql/#expose-postgresql-credentials-to-your-application","title":"Expose PostgreSQL credentials to Your Application","text":"<p>To expose the PostgreSQL cluster credentials to your application, follow one of the following upstream documentation:</p> <ul> <li>Create a Pod that has access to the secret data through a Volume</li> <li>Define container environment variables using Secret data</li> </ul>"},{"location":"user-guide/additional-services/postgresql/#follow-the-go-live-checklist","title":"Follow the Go-Live Checklist","text":"<p>You should be all set. Before going into production, don't forget to go through the go-live checklist.</p>"},{"location":"user-guide/additional-services/postgresql/#ck8s-postgresql-release-notes","title":"CK8S PostgreSQL Release Notes","text":"<p>Check out the release notes for the PostgreSQL cluster that runs in Compliant Kubernetes environments!</p>"},{"location":"user-guide/additional-services/postgresql/#further-reading","title":"Further Reading","text":"<ul> <li>Creating users</li> <li>Creating databases</li> <li>Granting permissions</li> <li>Kubernetes Secrets</li> </ul>"},{"location":"user-guide/additional-services/rabbitmq/","title":"RabbitMQ\u00ae","text":"<p>This page will help you succeed in connecting your application to a RabbitMQ-based message queue which meets your security and compliance requirements.</p>"},{"location":"user-guide/additional-services/rabbitmq/#provision-a-new-rabbitmq-cluster","title":"Provision a New RabbitMQ Cluster","text":"<p>Ask your service-specific administrator to install a RabbitMQ cluster inside your Compliant Kubernetes environment. The service-specific administrator will ensure the RabbitMQ cluster complies with your security requirements, including:</p> <ul> <li>Business continuity: We recommend a highly available setup with three instances.</li> <li>Disaster recovery: Note that, because messages are often short-lived and possibly transient, backing them up from under a running node is highly discouraged and can lead to an inconsistent snapshot of the data. Generally, disaster recovery for message queue only makes sense for the dead letter exchange.</li> <li>Capacity management: Your service-specific administrator will ensure RabbitMQ runs on dedicated (i.e., tainted) Kubernetes Nodes, as required to get the best performance.</li> <li>Incident management: Your administrator will set up the necessary monitors, probes, dashboards and alerts, to discover issues and resolve them, before they become a problem.</li> <li>Access control: Your service-specific administrator will hand you the RabbitMQ administrator username and password. This will allow you to declare exchanges, queues, bindings, users, virtual hosts and user permissions, as required.</li> </ul> <p>Compliant Kubernetes recommends the RabbitMQ Cluster Operator for Kubernetes.</p>"},{"location":"user-guide/additional-services/rabbitmq/#accessing-a-rabbitmq-cluster","title":"Accessing a RabbitMQ Cluster","text":"<p>Before continuing, make sure you have access to the Kubernetes API, as describe here.</p> <p>Make sure to install the RabbitMQ client on your workstation. On Ubuntu, this can be achieved as follows:</p> <pre><code>sudo apt install rabbitmq-server\n</code></pre>"},{"location":"user-guide/additional-services/rabbitmq/#accessing-the-management-ui","title":"Accessing the Management UI","text":"<p>RabbitMQ provides the management plugin which allows control over everything within it, including messaging topology and access control.</p> <p>Your administrator will set up a Secret inside Compliant Kubernetes containing the credentials of the default admin user, which is all you need to access your RabbitMQ cluster. The Secret has the following shape:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: ${RABBITMQ_CLUSTER}-default-user\nnamespace: ${RABBITMQ_NAMESPACE}\nstringData:\nusername: ${RABBITMQ_ADMIN_USERNAME}\npassword: ${RABBITMQ_ADMIN_PASSWORD}\n</code></pre> <p>Danger</p> <p>The Secret is very precious! Prefer not to persist any information extracted from it, as shown below.</p> <p>To access the management UI, proceed as follows:</p> <ol> <li> <p>Retrieve the admin default username and password</p> <pre><code>export RABBITMQ_CLUSTER=     # Get this from your administrator\nexport RABBITMQ_NAMESPACE=   # Get this from your administrator\n\necho -n \"RabbitMQ admin username: \"\nkubectl -n rabbitmq-system get secret rabbitmq-cluster-default-user -o jsonpath=\"{.data.username}\" | base64 --decode &amp;&amp; echo\n\necho -n \"RabbitMQ admin password: \"\nkubectl -n rabbitmq-system get secret rabbitmq-cluster-default-user -o jsonpath=\"{.data.password}\" | base64 --decode &amp;&amp; echo\n</code></pre> <p>Danger</p> <p>Do not configure your application with the RabbitMQ default admin username and password. Since the application will get too much permission, this will likely violate your access control policy.</p> </li> <li> <p>Start the port-forwarding:</p> <pre><code>kubectl port-forward -n ${RABBITMQ_NAMESPACE} svc/${RABBITMQ_CLUSTER} 15672\n</code></pre> </li> <li> <p>Open the admin dashboard (at http://localhost:15672) and log in using the credentials retrieved in step 1.</p> </li> <li> <p>Create an application username, password and vhost, and store these in variables as named below:</p> <pre><code>APP_USER=\nAPP_PASS=\nAPP_VHOST=\n</code></pre> </li> </ol>"},{"location":"user-guide/additional-services/rabbitmq/#create-an-application-secret","title":"Create an Application Secret","text":"<p>First, check that you are on the right Compliant Kubernetes cluster, in the right application namespace:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre> <p>Now, create a Kubernetes Secret in your application namespace to store the AMPQ URL:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n    name: app-amqp-secret\ntype: Opaque\nstringData:\n    AMQP_URL: amqp://${APP_USER}:${APP_PASS}@${RABBITMQ_CLUSTER}.${RABBITMQ_NAMESPACE}/${APP_VHOST}\nEOF\n</code></pre>"},{"location":"user-guide/additional-services/rabbitmq/#expose-amqp-url-to-your-application","title":"Expose AMQP URL to Your Application","text":"<p>To expose the AMQP URL to your application, follow one of the following upstream documentation:</p> <ul> <li>Create a Pod that has access to the secret data through a Volume</li> <li>Define container environment variables using Secret data</li> </ul>"},{"location":"user-guide/additional-services/rabbitmq/#using-a-rabbitmq-cluster","title":"Using a RabbitMQ Cluster","text":"<p>Best practices for using RabbitMQ in Compliant Kubernetes.</p> <p>Note</p> <p>RabbitMQ is built as a plugin based system, by default only a limited set of plugins are enabled, contact your service-specific administrator about enabling the ones you require.</p>"},{"location":"user-guide/additional-services/rabbitmq/#cluster-division","title":"Cluster Division","text":"<p>RabbitMQ is a multi tenant system and has the concept of virtual host or vhost which provides logical separation of resources within it. Each vhost acts as their own RabbitMQ cluster and have their own connection, channels, exchanges, queues, and bindings.</p> <p>Messages cannot flow directly between exchanges and queues in different vhosts, instead if that is required there must be a client that consumes messages in one vhost and then publishes messages in another vhost. This can also be realised with two plugins:</p> <ul> <li> <p>Federation plugin - Provides federated exchanges and queue which connect to an upstream cluster or vhost, allowing consumers to access message from the upstream source.</p> </li> <li> <p>Shovel plugin - Provides a client which connect both to an upstream and downstream cluster or vhost, consuming and publishing messages between them.</p> </li> </ul>"},{"location":"user-guide/additional-services/rabbitmq/#connection-and-channel-management","title":"Connection and Channel Management","text":"<p>In RabbitMQ connections and channels are intended to and optimised to be long-lived. Therefore one should avoid connection and channel churn by using and reusing them for as long as possible.</p> <p>Each connection and channel consumes resources and should therefore be kept at a minimal number as required by the application. It is important to close unused connections and channels to free their resources.</p> <p>Consumers and publishers should use separate connections and channels to allow RabbitMQ to better manage clients when under high pressure, allowing consumers to catch up by restricting publishers.</p> <p>Recovering from connection failures should use automatic or recommended methods when provided by the client library, custom recovery methods need to be carefully made to not put too much pressure on RabbitMQ.</p>"},{"location":"user-guide/additional-services/rabbitmq/#reliable-messaging","title":"Reliable Messaging","text":"<p>The messaging protocol AMQP provide two mechanisms for client to manage reliable messaging:</p> <ul> <li> <p>Consumer Acknowledgements - Ensure that the client has properly received a message. RabbitMQ will redeliver message left unacknowledged.</p> </li> <li> <p>Publisher Confirms - Ensures that the server has properly received a message. RabbitMQ may delay confirms when under to high pressure to be able to handle the load.</p> </li> </ul> <p>Additionally RabbitMQ has different queue types and modes that affect the reliability of messaging as will be explained in the next section.</p>"},{"location":"user-guide/additional-services/rabbitmq/#queue-selection","title":"Queue Selection","text":"<p>RabbitMQ supports three different types of queues which all have different pro's and con's:</p> <ul> <li> <p>Classic Queues - Best for high performance with transient messages.</p> <p>This is the standard choice for transient queues and messages since they consume little resources and offer high performance.</p> <p>Danger</p> <p>Classic queues are not replicated by default and should not be used with replication since mirrored classic queues are deprecated.</p> <p>Durable classic queues are tied to the server they are created on and will become unavailable if that server is unavailable.</p> </li> <li> <p>Quorum Queues - Best for high availability with durable messages.</p> <p>This is the standard choice for durable queues and messages since they are replicated by default and offer high availability.</p> <p>Warning</p> <p>Similar to connections and channels quorum queues are intended to be long-lived, avoid scenarios where quorum queues are frequently removed and redeclared as it may lock internal resources within RabbitMQ.</p> <p>Keep the queues small and use multiple queues when possible for best performance.</p> </li> <li> <p>Stream Queues - Best for high volume with durable messages.</p> <p>This is the standard choice for high volume durable queues and messages since they store messages as a persistent replicated log. This allows for messages to persist after they are consumed to be recalled or replayed, or with the stream protocol efficiently processed in batches.</p> <p>Warning</p> <p>When using stream queues you must set reasonable retention to keep them from filling up RabbitMQ as it will prevent it from accepting new messages.</p> <p>Also note that messages in RabbitMQ are not backed up in the case of disaster.</p> </li> </ul>"},{"location":"user-guide/additional-services/rabbitmq/#follow-the-go-live-checklist","title":"Follow the Go-Live Checklist","text":"<p>You should be all set. Before going into production, don't forget to go through the go-live checklist.</p>"},{"location":"user-guide/additional-services/rabbitmq/#ck8s-rabbitmq-release-notes","title":"CK8S RabbitMQ Release Notes","text":"<p>Check out the release notes for the RabbitMQ cluster that runs in Compliant Kubernetes environments!</p>"},{"location":"user-guide/additional-services/rabbitmq/#further-reading","title":"Further Reading","text":"<ul> <li>RabbitMQ Management UI</li> <li>AMQP URL spec</li> <li>AMQP Clients</li> <li>Kubernetes Secrets</li> </ul>"},{"location":"user-guide/additional-services/redis/","title":"Redis\u2122","text":"<p>This page will help you succeed in connecting your application to a low-latency in-memory cache Redis which meets your security and compliance requirements.</p>"},{"location":"user-guide/additional-services/redis/#provision-a-new-redis-cluster","title":"Provision a New Redis Cluster","text":"<p>Ask your service-specific administrator to install a Redis cluster inside your Compliant Kubernetes environment. The service-specific administrator will ensure the Redis cluster complies with your security requirements, including:</p> <ul> <li>Business continuity: We recommend a highly available setup with at minimum three instances. The Redis client library that you use in your application needs to support Redis Sentinel. Notice that clients with Sentinel support need extra steps to discover the Redis primary.</li> <li>Capacity management: Your service-specific administrator will ensure Redis has enough capacity to meet your needs.</li> <li>Incident management: Your administrator will set up the necessary Probes, dashboards and alerts, to discover issues and resolve them, before they become a problem.</li> </ul> <p>Important: Improve Access Control with NetworkPolicies</p> <p>Please note the follow information about Redis access control from the upstream documentation:</p> <p>Redis is designed to be accessed by trusted clients inside trusted environments.</p> <p>For improved security, discuss with your service-specific administrator what Pods and/or Namespaces need access to the Redis cluster. They can then set up the necessary NetworkPolicies.</p> <p>Important: No Disaster Recovery</p> <p>We do not recommend using Redis as primary database. Redis should be used to store:</p> <ul> <li>Cached data: If this is lost, this data can be quickly retrieved from the primary database, such as the PostgreSQL cluster.</li> <li>Session state: If this is lost, the user experience might be impacted -- e.g., the user needs to re-login -- but no data should be lost.</li> </ul> <p>Compliant Kubernetes recommends the Spotahome operator.</p>"},{"location":"user-guide/additional-services/redis/#install-prerequisites","title":"Install Prerequisites","text":"<p>Before continuing, make sure you have access to the Kubernetes API, as describe here.</p> <p>Make sure to install the Redis client on your workstation. On Ubuntu, this can be achieved as follows:</p> <pre><code>sudo apt install redis-tools\n</code></pre>"},{"location":"user-guide/additional-services/redis/#getting-access","title":"Getting Access","text":"<p>Your administrator will set up a ConfigMap inside Compliant Kubernetes, which contains all information you need to access your Redis cluster. The ConfigMap has the following shape:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: $CONFIG_MAP\nnamespace: $NAMESPACE\ndata:\n# REDIS_SENTINEL_HOST represents a cluster-scoped Redis Sentinel host, which only makes sense inside the Kubernetes cluster.\n# E.g., rfs-redis-cluster.redis-system\nREDIS_SENTINEL_HOST: $REDIS_SENTINEL_HOST\n\n# REDIS_SENTINEL_PORT represents a cluster-scoped Redis Sentinel port, which only makes sense inside the Kubernetes cluster.\n# E.g., 26379\nREDIS_SENTINEL_PORT: \"$REDIS_SENTINEL_PORT\"\n</code></pre> <p>To extract this information, proceed as follows:</p> <pre><code>export CONFIG_MAP=            # Get this from your administrator\nexport NAMESPACE=         # Get this from your administrator\n\nexport REDIS_SENTINEL_HOST=$(kubectl -n $NAMESPACE get configmap $CONFIG_MAP -o 'jsonpath={.data.REDIS_SENTINEL_HOST}')\nexport REDIS_SENTINEL_PORT=$(kubectl -n $NAMESPACE get configmap $CONFIG_MAP -o 'jsonpath={.data.REDIS_SENTINEL_PORT}')\n</code></pre> <p>Important</p> <p>At the time of this writing, we do not recommend to use a Redis cluster in a multi-tenant fashion. One Redis cluster should have only one purpose.</p>"},{"location":"user-guide/additional-services/redis/#create-a-configmap","title":"Create a ConfigMap","text":"<p>First, check that you are on the right Compliant Kubernetes cluster, in the right application namespace:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre> <p>Now, create a Kubernetes ConfigMap in your application namespace to store the Redis Sentinel connection parameters:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n    name: app-redis-config\ndata:\n    REDIS_SENTINEL_HOST: $REDIS_SENTINEL_HOST\n    REDIS_SENTINEL_PORT: \"$REDIS_SENTINEL_PORT\"\nEOF\n</code></pre>"},{"location":"user-guide/additional-services/redis/#expose-redis-connection-parameters-to-your-application","title":"Expose Redis Connection Parameters to Your Application","text":"<p>To expose the Redis cluster to your application, follow one of the following upstream documentation:</p> <ul> <li>Create a Pod that has access to the ConfigMap data through a Volume</li> <li>Define container environment variables using ConfigMap data</li> </ul> <p>Important</p> <p>Make sure to use a Redis client library with Sentinel support. For example:</p> <ul> <li>Django-Redis Client that supports Sentinel Cluster HA     If the linked code example doesn't work, try <code>LOCATION: redis://mymaster/db</code>.</li> </ul> <p>If your library doesn't support sentinel you could use this project - Redis sentinel proxy     Note that the default configuration in this repository will not ensure HA for redis.     For this you'll either need to use multiple replicas or use it as a sidecar for your applications.</p>"},{"location":"user-guide/additional-services/redis/#follow-the-go-live-checklist","title":"Follow the Go-Live Checklist","text":"<p>You should be all set. Before going into production, don't forget to go through the go-live checklist.</p>"},{"location":"user-guide/additional-services/redis/#ck8s-redis-release-notes","title":"CK8S Redis Release Notes","text":"<p>Check out the release notes for the Redis cluster that runs in Compliant Kubernetes environments!</p>"},{"location":"user-guide/additional-services/redis/#further-reading","title":"Further Reading","text":"<ul> <li>Redis Sentinel</li> <li>Guidelines for Redis clients with support for Redis Sentinel</li> <li>Redis Commands</li> <li>Kubernetes ConfigMap</li> </ul>"},{"location":"user-guide/additional-services/timescaledb/","title":"TimescaleDB\u00ae","text":"<p>This page will help you succeed in connecting your application to a primary relational database TimescaleDB which meets your security and compliance requirements.</p> <p>TimescaleDB is an extension on top of our managed PostgreSQL. This means that your administrator will be setting up a complete PostgreSQL cluster for you and you just use it for TimescaleDB via the TimescaleDB extension.</p> <p>Note</p> <p>TimescaleDB is not a viable option for collecting all metrics from the Kubernetes cluster. The data is uncompressed and would take a lot of space to store and use a lot of resources to analyze, unless you want to use it with a very short retention period. This is not usually a problem for collecting application specific metrics, since they are not as many as the metrics that are generated from the Kubernetes cluster.</p> <p>Important</p> <p>Due to very different performance-tuning characteristics, Timescale and PostgreSQL databases should never run on the same PostgreSQL cluster. To comply with this, it is essential that every PostgreSQL database that gets created on the PostgreSQL cluster also has the Timescale extension created for it.</p> <p>If you want to use TimescaleDB on your Compliant Kubernetes cluster, ask your administrator to provision a new standard PostgreSQL cluster inside your Compliant Kubernetes environment. Then set up the TimescaleDB extension.</p>"},{"location":"user-guide/additional-services/timescaledb/#provision-a-new-postgresql-cluster","title":"Provision a New PostgreSQL Cluster","text":"<p>Ask your service-specific administrator to install a PostgreSQL cluster inside your Compliant Kubernetes environment. The service-specific administrator will ensure the PostgreSQL cluster complies with your security requirements, including:</p> <ul> <li>Business continuity: We recommend a highly available setup with at minimum a primary instance and a replica. Ideally, the PostgreSQL cluster should be configured with a primary and two replicas.</li> <li>Disaster recovery: Your service-specific administrator will configure the PostgreSQL cluster with physical backups, logical backups and Point-in-Time Recovery (PITR), as required to meet your Recovery Point Objectives.</li> <li>Capacity management: Your service-specific administrator will ensure PostgreSQL runs on dedicated (i.e., tainted) Kubernetes Nodes, as required to get the best performance.</li> <li>Incident management: Your administrator will set up the necessary Probes, dashboards and alerts, to discover issues and resolve them, before they become a problem.</li> <li>Access control: Your administrator will set up a \"root-like\" PostgreSQL account, which will allow you to create databases and PostgreSQL users, but not tamper will logging, business continuity or disaster recovery.</li> </ul> <p>Compliant Kubernetes recommends the Zalando PostgreSQL operator.</p>"},{"location":"user-guide/additional-services/timescaledb/#install-prerequisites","title":"Install Prerequisites","text":"<p>Before continuing, make sure you have access to the Kubernetes API, as describe here.</p> <p>Make sure to install the PostgreSQL client on your workstation. On Ubuntu, this can be achieved as follows:</p> <pre><code>sudo apt-get install postgresql-client\n</code></pre>"},{"location":"user-guide/additional-services/timescaledb/#getting-access","title":"Getting Access","text":"<p>Your administrator will set up a Secret inside Compliant Kubernetes, which contains all information you need to access your PostgreSQL cluster. The Secret has the following shape:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: $SECRET\nnamespace: $NAMESPACE\nstringData:\n# PGHOST represents a cluster-scoped DNS name or IP, which only makes sense inside the Kubernetes cluster.\n# E.g., postgresql1.postgres-system.svc.cluster.local\nPGHOST: $PGHOST\n\n# These fields map to the environment variables consumed by psql.\n# Ref https://www.postgresql.org/docs/13/libpq-envars.html\nPGUSER: $PGUSER\nPGPASSWORD: $PGPASSWORD\nPGSSLMODE: $PGSSLMODE\n\n# This is the Kubernetes Service name to which you can port-foward to in order to get access to the PostgreSQL cluster from outside the Kubernetes cluster.\n# Ref https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/\nUSER_ACCESS: $USER_ACCESS\n</code></pre> <p>Important</p> <p>The Secret is very precious! Prefer not to persist any information extracted from it, as shown below.</p> <p>To extract this information, proceed as follows:</p> <pre><code>export SECRET=            # Get this from your administrator\nexport NAMESPACE=         # Get this from your administrator\n\nexport PGHOST=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGHOST}' | base64 --decode)\nexport PGUSER=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGUSER}' | base64 --decode)\nexport PGPASSWORD=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGPASSWORD}' | base64 --decode)\nexport PGSSLMODE=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGSSLMODE}' | base64 --decode)\nexport USER_ACCESS=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.USER_ACCESS}' | base64 --decode)\n</code></pre> <p>Important</p> <p>Do not configure your application with the PostgreSQL admin username and password. Since the application will get too much permission, this will likely violate your access control policy.</p> <p>Important</p> <p>If you change the password for $PGUSER, you are responsible for keeping track of the new password.</p>"},{"location":"user-guide/additional-services/timescaledb/#create-an-application-user","title":"Create an Application User","text":"<p>First, in one console, fetch the information from the access Secret as described above and port forward into the PostgreSQL master.</p> <pre><code>kubectl -n $NAMESPACE port-forward svc/$USER_ACCESS 5432\n</code></pre> <p>Important</p> <p>Since humans are bad at generating random passwords, we recommend using pwgen.</p> <p>Second, in another console, fetch the information from the access Secret again and run the PostgreSQL client to create the application database and user:</p> <pre><code>export APP_DATABASE=myapp\nexport APP_USERNAME=myapp\nexport APP_PASSWORD=$(pwgen 32)\n\ncat &lt;&lt;EOF | psql -d postgres -h 127.0.0.1 -U $PGUSER \\\n    --set=APP_DATABASE=$APP_DATABASE \\\n    --set=APP_USERNAME=$APP_USERNAME \\\n    --set=APP_PASSWORD=$APP_PASSWORD\ncreate database :APP_DATABASE;\ncreate user :APP_USERNAME with encrypted password ':APP_PASSWORD';\ngrant all privileges on database :APP_DATABASE to :APP_USERNAME;\nEOF\n</code></pre> <p>Continue with the second console in the next section to create a Secret with this information.</p>"},{"location":"user-guide/additional-services/timescaledb/#create-an-application-secret","title":"Create an Application Secret","text":"<p>First, check that you are on the right Compliant Kubernetes cluster, in the right application namespace:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre> <p>Now, create a Kubernetes Secret in your application namespace to store the PostgreSQL application username and password. For consistency, prefer sticking to naming connection parameters as the environment variables consumed by psql.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n    name: app-postgresql-secret\ntype: Opaque\nstringData:\n    PGHOST: ${PGHOST}\n    PGPORT: '5432'\n    PGSSLMODE: ${PGSSLMODE}\n    PGUSER: ${APP_USERNAME}\n    PGPASSWORD: ${APP_PASSWORD}\n    PGDATABASE: ${APP_DATABASE}\nEOF\n</code></pre> <p>Warning</p> <p>Although most client libraries follow the <code>libpq</code> definition of these environment variables, some do not, and this will require changes to the application Secret.</p> <p>Notably <code>node-postgres</code> does not currently do so for <code>PGSSLMODE</code>. When this variable is set to <code>require</code>, it will do a full verification instead, requiring access to the PostgreSQL certificates to allow a connection. To get the intended mode for <code>require</code> set the variable to <code>no-verify</code> instead.</p>"},{"location":"user-guide/additional-services/timescaledb/#expose-postgresql-credentials-to-your-application","title":"Expose PostgreSQL credentials to Your Application","text":"<p>To expose the PostgreSQL cluster credentials to your application, follow one of the following upstream documentation:</p> <ul> <li>Create a Pod that has access to the secret data through a Volume</li> <li>Define container environment variables using Secret data</li> </ul>"},{"location":"user-guide/additional-services/timescaledb/#set-up-the-timescaledb-extension-on-postgresql","title":"Set up the TimescaleDB extension on PostgreSQL","text":"<ul> <li>Connect to the created database: <pre><code>\\c $APP_DATABASE\n</code></pre></li> <li>Add the TimescaleDB extension: <pre><code>CREATE EXTENSION IF NOT EXISTS timescaledb;\n</code></pre></li> </ul>"},{"location":"user-guide/additional-services/timescaledb/#follow-the-go-live-checklist","title":"Follow the Go-Live Checklist","text":"<p>You should be all set. Before going into production, don't forget to go through the go-live checklist.</p>"},{"location":"user-guide/additional-services/timescaledb/#ck8s-timescaledb-release-notes","title":"CK8S TimescaleDB Release Notes","text":"<p>Check out the release notes for the TimescaleDB/PostgreSQL cluster that runs in Compliant Kubernetes environments!</p>"},{"location":"user-guide/additional-services/timescaledb/#further-reading","title":"Further Reading","text":"<ul> <li>Getting started with Timescale</li> <li>Creating users</li> <li>Creating databases - Remember to create Timescale extension on the new databases.</li> <li>Granting permissions</li> <li>Kubernetes Secrets</li> </ul>"},{"location":"user-guide/safeguards/","title":"Safeguards","text":"<p>Important</p> <p>In 2021-01-27, the French Data Protection Authority (CNIL) imposed a fine on both the data controller and the data processor for failing to comply with their security obligations. For details, please read this article.</p> <p>Some of these safeguards might be \"inconvenient\" and \"easy to disable\". Faced with tight deadlines, it might be tempting to pressure administrators to disable some of these safeguards.</p> <p>Prefer to keep these safeguards to avoid costly fines. A safeguard should only be disabled if a risk assessment determined that the cost of implementation outweighs the risk to personal data.</p> <p>\"Det ska vara l\u00e4tt att g\u00f6ra r\u00e4tt.\" (English: \"It should be easy to do it right.\")</p> <p>We know you care about the security and uptime of your application. But all that effort goes wasted if the platform allows you to make trivial mistakes.</p> <p>That is why Compliant Kubernetes is built with various safeguards, to allow you to make security and reliability easy for you.</p>"},{"location":"user-guide/safeguards/#relevant-regulations","title":"Relevant Regulations","text":"<ul> <li> <p>GDPR Article 32:</p> <p>Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] a process for regularly testing, assessing and evaluating the effectiveness of technical and organisational measures for ensuring the security of the processing.</p> <p>In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added]</p> </li> </ul>"},{"location":"user-guide/safeguards/enforce-networkpolicies/","title":"Reduce blast radius: NetworkPolicies","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.13.1.1 Network Controls</li> <li>A.13.1.2 Security of Network Services</li> <li>A.13.1.3 Segregation in Networks</li> </ul> <p>Important</p> <ul> <li>This safeguard is enabled by default with the enforcement action <code>deny</code> since Compliant Kubernetes apps v0.19.0. As a result, resources that violate this policy will not be created.</li> <li>The default enforcement action for this safeguard has been changed to <code>warn</code> instead of <code>deny</code> since Compliant Kubernetes apps v0.29.0. As a result, resources that violate this policy will generate warning messages, but will still be created.</li> </ul> <p>NetworkPolicies are useful in two cases: segregating tenants hosted in the same environment and further segregating application components. Both help you achieve better data protection.</p>","tags":["ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18"]},{"location":"user-guide/safeguards/enforce-networkpolicies/#segregating-tenants-hosted-in-the-same-environment","title":"Segregating tenants hosted in the same environment","text":"<p>Say you want to host a separate instance of your application for each tenant. For example, your end-users may belong to different -- potentially competing -- organizations, and you promised them to take extra care of not mixing their data. Say you want to reduce complexity by hosting all tenants inside the same environment, but without compromising data protection.</p> <p>Each application instance could be installed as a separate Helm Release, perhaps even in its own Namespace. These instances should be segregated from other application instances using NetworkPolicies. This insures that network traffic from one application instance cannot reach another application instance. Besides reducing attack surface, it also prevents embarrassing mistakes, like connecting one application to the database of another.</p>","tags":["ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18"]},{"location":"user-guide/safeguards/enforce-networkpolicies/#further-segregation-of-application-components","title":"Further segregation of application components","text":"<p>If you run several applications -- e.g., frontend, backend, backoffice, database, message queue -- in a single Kubernetes cluster, it is a best practice to segregrate them. By segregating your applications and only allowing required ingress and egress network traffic, you further reduce blast radius in case of an attack.</p>","tags":["ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18"]},{"location":"user-guide/safeguards/enforce-networkpolicies/#compliant-kubernetes-helps-enforce-segregation","title":"Compliant Kubernetes helps enforce segregation","text":"<p>Compliant Kubernetes allows you to segregate applications by installing suitable NetworkPolicies. These are a bit like firewalls, but in the container world: Since containers are supposed to be deleted and recreated frequently, they change IP address a lot. Clearly the old \"allow/deny IP\" method does not scale. Therefore, NetworkPolicies select source and destination Pods based on labels or namespace labels.</p> <p>To make sure you don't forget to configure NetworkPolicies, the administrator can configure Compliant Kubernetes to deny creation of Pods with no matching NetworkPolicies.</p> <p>If you get the following error:</p> <pre><code>Error: admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-networkpolicy] No matching networkpolicy found\n</code></pre> <p>Then you are missing NetworkPolicies which select your Pods. The user demo gives a good example to get you started.</p> <p>If your administrator has not enforced this policy yet, you can view current violations of the policy by running <pre><code>kubectl get k8srequirenetworkpolicy.constraints.gatekeeper.sh require-networkpolicy -ojson | jq .status.violations\n</code></pre></p>","tags":["ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18"]},{"location":"user-guide/safeguards/enforce-networkpolicies/#further-reading","title":"Further Reading","text":"<ul> <li>NetworkPolicies</li> </ul>","tags":["ISO 27001 A.13.1 Network Security","BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18"]},{"location":"user-guide/safeguards/enforce-no-latest-tag/","title":"Avoid unexpected changes: disallowed tags","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.12.1.2 Change Management</li> <li>A.14.2.2 System Change Control Procedures</li> <li>A.14.2.4 Restrictions on Changes to Software Packages</li> </ul> <p>Important</p> <p>This safeguard is enabled by default with the enforcement action <code>deny</code> since Compliant Kubernetes apps v0.29.0. As a result, resources that violate this policy will not be created.</p> <p>Using the <code>:latest</code> tag can lead to inconsistent deployments, where it is difficult to rollback. In Compliant Kubernetes we suggest using explicit tags for your container images. This way you know that image version <code>v1.0.0</code> will be deployed if you are using the <code>:v1.0.0</code> tag.</p>","tags":["ISO 27001 A.12.1.2 Change Management","ISO 27001 A.14.2.2 System Change Control Procedures","ISO 27001 A.14.2.4 Restrictions on Changes to Software Packages"]},{"location":"user-guide/safeguards/enforce-no-latest-tag/#how-to-solve-container-image-must-not-have-disallowed-tags","title":"How to solve: [container-image-must-not-have-disallowed-tags]","text":"<p>You may encounter the following issue:</p> <pre><code>Error from server ([container-image-must-not-have-disallowed-tags] container &lt;example-container&gt; uses a disallowed tag &lt;harbor.$DOMAIN/$REGISTRY_PROJECT/example-container:latest&gt;; disallowed tags are [\"latest\"])\n</code></pre> <p>This means that you are not allowed to use the <code>:latest</code> tag on your images. If no tag is specified, Kubernetes assumes <code>:latest</code>, but that does not mean that the most recent version of the image will actually be used. <code>:latest</code> is just a tag and is not dynamically updated to the most recent version of the image. It also becomes difficult to track which version of the image was used if you were to do a rollback.</p> <p>To fix this, you have the following options:</p> <ul> <li>Use a meaningful tag for your images i.e. <code>v1.0.0</code>.</li> </ul> <p>If your administrator has not enforced this policy yet, you can view current violations of the policy by running <pre><code>kubectl get k8sdisallowedtags.constraints.gatekeeper.sh container-image-must-not-have-disallowed-tags -ojson | jq .status.violations\n</code></pre></p>","tags":["ISO 27001 A.12.1.2 Change Management","ISO 27001 A.14.2.2 System Change Control Procedures","ISO 27001 A.14.2.4 Restrictions on Changes to Software Packages"]},{"location":"user-guide/safeguards/enforce-no-latest-tag/#further-reading","title":"Further Reading","text":"<ul> <li>Images</li> </ul>","tags":["ISO 27001 A.12.1.2 Change Management","ISO 27001 A.14.2.2 System Change Control Procedures","ISO 27001 A.14.2.4 Restrictions on Changes to Software Packages"]},{"location":"user-guide/safeguards/enforce-no-root/","title":"Reduce blast radius: Preventing forgotten roots","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.9.4.4 Use of Privileged Utility Programmes</li> <li>A.12.6.1 Management of Technical Vulnerabilities</li> <li>A.14.2.5 Secure System Engineering Principles</li> </ul> <p>Many container runtimes and operating system vulnerabilities need code running as root to become a threat. To minimize this risk, application should only run as root when strictly necessary.</p> <p>Unfortunately, many Dockerfiles -- and container base images -- today are shipped running as root by default. This makes it easy to slip code running as root into production, exposing data to unnecessary risks.</p> <p>To reduce blast radius, Compliant Kubernetes will protect you from accidentally deploying application running as root.</p>","tags":["ISO 27001 A.9.4.4 Use of Privileged Utility Programmes","ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","ISO 27001 A.14.2.5 Secure System Engineering Principles"]},{"location":"user-guide/safeguards/enforce-no-root/#how-to-solve-createcontainerconfigerror","title":"How to solve: CreateContainerConfigError","text":"<p>You may encounter the following issue:</p> <pre><code>$ kubectl get pods\nNAME                                   READY   STATUS                       RESTARTS   AGE\nmyapp-ck8s-user-demo-564f8dd85-2bs8r   0/1     CreateContainerConfigError   0          84s\nmyapp-ck8s-user-demo-bfbf9c459-dmk4l   0/1     CreateContainerConfigError   0          13m\n$ kubectl describe pods myapp-ck8s-user-demo-564f8dd85-2bs8r\n[...]\nError: container has runAsNonRoot and image has non-numeric user (node), cannot verify user is non-root (pod: \"myapp-ck8s-user-demo-bfbf9c459-dmk4l_demo1(1b53b1a8-4845-4db5-aecf-6bebcc54e396)\", container: ck8s-user-demo)\n</code></pre> <p>This means that your Dockerfile uses a non-numeric user and Kubernetes cannot validate whether the image truly runs as non-root.</p> <p>Alternatively, you may get:</p> <pre><code>$ kubectl describe pods myapp-ck8s-user-demo-564f8dd85-2bs8r\n[...]\nError: container has runAsNonRoot and image will run as root (pod: \"myapp-ck8s-user-demo-564f8dd85-2bs8r_demo1(a55a25f3-7b77-4fae-9f92-11e264446ecc)\", container: ck8s-user-demo)\n</code></pre> <p>This means that your Dockerfile has no <code>USER</code> directive and your application would run as root.</p> <p>To ensure your application does not run as root, you have two options:</p> <ol> <li>Change the Dockerfile to <code>USER 1000</code> or whatever numeric ID corresponds to your user. This is what the user demo does.</li> <li>Add the following snippet to the <code>spec</code> of your Pod manifest:     <pre><code>securityContext:\nrunAsUser: 1000\n</code></pre></li> </ol> <p>If possible, prefer changing the Dockerfile, to ensure your application runs as non-root not only in production, but also during development and testing. The smaller the difference between development, testing and production, the fewer surprises down the time.</p>","tags":["ISO 27001 A.9.4.4 Use of Privileged Utility Programmes","ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","ISO 27001 A.14.2.5 Secure System Engineering Principles"]},{"location":"user-guide/safeguards/enforce-no-root/#further-reading","title":"Further Reading","text":"<ul> <li>Dockerfile USER</li> <li>SecurityContext</li> </ul>","tags":["ISO 27001 A.9.4.4 Use of Privileged Utility Programmes","ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","ISO 27001 A.14.2.5 Secure System Engineering Principles"]},{"location":"user-guide/safeguards/enforce-resources/","title":"Enforce Resources","text":"","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"user-guide/safeguards/enforce-resources/#avoid-downtime-with-resource-requests-and-limits","title":"Avoid downtime with Resource Requests and Limits","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.12.1.3 Capacity Management</li> </ul> <p>Important</p> <p>This safeguard is enabled by default with the enforcement action <code>deny</code> since Compliant Kubernetes apps v0.19.0. As a result, resources that violate this policy will not be created.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"user-guide/safeguards/enforce-resources/#problem","title":"Problem","text":"<p>A major source of application downtime is insufficient capacity. For example, if a Node reaches 100% CPU utilization, then application Pods hosted on it will run slow, leading to bad end-user experience. If a Node runs into memory pressure, the application will run slower, as less memory is available for the page cache. High memory pressure may lead to the Node triggering the infamous Out-of-Memory (OOM) Killer, killing a victim, either your application or a platform component.</p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"user-guide/safeguards/enforce-resources/#solution","title":"Solution","text":"<p>To avoid running into capacity issues, Kubernetes allows Pods to specify resource requests and limits for each of its containers. This achieves two benefits:</p> <ol> <li>It ensures that Pods are scheduled to Nodes that have the requested resources.</li> <li>It ensures that a Pod does not exceed its resource limits, hence limiting its blast radius and protecting other application or platform Pods.</li> </ol>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"user-guide/safeguards/enforce-resources/#how-does-compliant-kubernetes-help","title":"How Does Compliant Kubernetes Help?","text":"<p>To make sure you don't forget to configure resource requests and limits, the administrator can configure Compliant Kubernetes to deny creation of Pods without explicit resource specifications.</p> <p>If you get the following error:</p> <pre><code>Error: UPGRADE FAILED: failed to create resource: admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-resource-requests] Container \"ck8s-user-demo\" has no resource requests\n</code></pre> <p>Then you are missing resource requests for some containers of your Pods. The user demo gives a good example to get you started.</p> <p>If your administrator has not enforced this policy yet, you can view current violations of the policy by running <pre><code>kubectl get k8sresourcerequests.constraints.gatekeeper.sh require-resource-requests -ojson | jq .status.violations\n</code></pre></p>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"user-guide/safeguards/enforce-resources/#further-reading","title":"Further Reading","text":"<ul> <li>Managing Resources for Containers</li> </ul>","tags":["ISO 27001 A.12.1.3 Capacity Management"]},{"location":"user-guide/safeguards/enforce-trusted-registries/","title":"Enforce Trusted Registries","text":"","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"user-guide/safeguards/enforce-trusted-registries/#avoid-vulnerable-container-images","title":"Avoid vulnerable container images","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.12.6.1 Management of Technical Vulnerabilities</li> </ul> <p>Important</p> <ul> <li>This safeguard is enabled by default with the enforcement action <code>deny</code> since Compliant Kubernetes apps v0.19.0. As a result, resources that violate this policy will not be created.</li> <li>The default enforcement action for this safeguard has been changed to <code>warn</code> instead of <code>deny</code> since Compliant Kubernetes apps v0.29.0. As a result, resources that violate this policy will generate warning messages, but will still be created.</li> </ul>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"user-guide/safeguards/enforce-trusted-registries/#problem","title":"Problem","text":"<p>A healthy security posture requires you to ensure your code has no known vulnerabilities. Compliant Kubernetes comes with a registry which includes vulnerability scanning of container images. It can even be configured to prevent the Kubernetes cluster from pulling images with vulnerabilities above a set criticality. This is a per-project setting, so you could, for example, have a stricter policy for publicly facing application components -- e.g., the front office -- and a less strict policy for internal application components -- e.g., the back office.</p> <p>Public container registry, such as Docker Hub and Quay, might not stick to the vulnerability management you require, perhaps being at times too strict or too loose.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"user-guide/safeguards/enforce-trusted-registries/#solution","title":"Solution","text":"<p>You can designate a set of registries, a project within a registry or specific container images as trusted. By this you declared that you did a risk analysis and determined that they fulfill your security requirements.</p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"user-guide/safeguards/enforce-trusted-registries/#how-does-compliant-kubernetes-help","title":"How Does Compliant Kubernetes Help?","text":"<p>Your administrator can configure Compliant Kubernetes to technically enforce a set of trusted container registries. This means that if you accidentally reference an image in an untrusted registry, you will get the following error:</p> <pre><code>Error: admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-harbor-repo] container \"ck8s-user-demo\" has an invalid image repo \"harbor.example.com/demo/ck8s-user-demo:1.16.0\", allowed repos are [\"harbor.cksc.a1ck.io\"]\n</code></pre> <p>The resolution is rather simple. You have two options:</p> <ol> <li>Change the container image to point to a trusted registry.</li> <li>Get in touch with your administrator and discuss augmenting the set of trusted registries.</li> </ol> <p>Important</p> <p>Instead of adding a not-really-trusted registry to the set of trusted registries, prefer mirroring some public images in your Compliant Kubernetes registry.</p> <p>If your administrator has not enforced this policy yet, you can view current violations of the policy by running <pre><code>kubectl get k8sallowedrepos.constraints.gatekeeper.sh require-harbor-repo -ojson | jq .status.violations\n</code></pre></p>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"user-guide/safeguards/enforce-trusted-registries/#further-reading","title":"Further Reading","text":"<ul> <li>Container Images</li> <li>Harbor Vulnerability Scanning</li> </ul>","tags":["ISO 27001 A.12.6.1 Management of Technical Vulnerabilities"]},{"location":"ciso-guide/controls/bsi-it-grundschutz/","title":"BSI IT-Grundschutz Controls","text":"<p>Click on the links below to navigate the documentation by control.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a2","title":"BSI IT-Grundschutz APP.4.4.A2","text":"<ul> <li>CI/CD</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a3","title":"BSI IT-Grundschutz APP.4.4.A3","text":"<ul> <li>How to Delegate</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a5","title":"BSI IT-Grundschutz APP.4.4.A5","text":"<ul> <li>Backup Dashboard</li> <li>Backups</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a7","title":"BSI IT-Grundschutz APP.4.4.A7","text":"<ul> <li>Network Model</li> <li>Enforce NetworkPolicies</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a10","title":"BSI IT-Grundschutz APP.4.4.A10","text":"<ul> <li>CI/CD</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a14","title":"BSI IT-Grundschutz APP.4.4.A14","text":"<ul> <li>Use Dedicated Nodes for Additional Services</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a16","title":"BSI IT-Grundschutz APP.4.4.A16","text":"<ul> <li>Overview</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a18","title":"BSI IT-Grundschutz APP.4.4.A18","text":"<ul> <li>Network Model</li> <li>Enforce NetworkPolicies</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a21","title":"BSI IT-Grundschutz APP.4.4.A21","text":"<ul> <li>Maintenance</li> <li>Prepare Your Application</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#other-it-grundschutz-controls","title":"Other IT-Grundschutz Controls","text":""},{"location":"ciso-guide/controls/bsi-it-grundschutz/#app44a17-attestierung-von-nodes-h","title":"APP.4.4.A17 Attestierung von Nodes (H)","text":"<p>The Kubespray layer in Compliant Kubernetes ensures that Data Plane Nodes and Control Plane Nodes are mutually authenticated via mutual TLS.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-controls-outside-the-scope-of-compliant-kubernetes","title":"BSI IT-Grundschutz Controls outside the scope of Compliant Kubernetes","text":"<p>Pending official translation into English, the controls are written in German.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#app44a1-planung-der-separierung-der-anwendungen-b","title":"APP.4.4.A1 Planung der Separierung der Anwendungen (B)","text":"<p>Compliant Kubernetes recommends to setting up at least two separate environments: one for testing and one for production.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#app44a6-initialisierung-von-pods-s","title":"APP.4.4.A6 Initialisierung von Pods (S)","text":"<p>Application developers must make sure that initialization happens in init containers.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#app44a11-uberwachung-der-container-s","title":"APP.4.4.A11 \u00dcberwachung der Container (S)","text":"<p>Application developers must ensure that their application has a liveliness and readiness probe, which are configured in the Deployment. This is illustrated by our user demo.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#app44a12-absicherung-der-infrastruktur-anwendungen-s","title":"APP.4.4.A12 Absicherung der Infrastruktur-Anwendungen (S)","text":"<p>This requirement essentially states that the Compliant Kubernetes environments are only as secure as the infrastructure around them. Make sure you have a proper IT policy in place. Regularly review the systems where you store backups and configuration of Compliant Kubernetes.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#app44a13-automatisierte-auditierung-der-konfiguration-h","title":"APP.4.4.A13 Automatisierte Auditierung der Konfiguration (H)","text":"<p>Compliant Kubernetes administrators must regularly audit the configuration of their environments. We recommend doing this on a quarterly basis.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#app44a20-verschlusselte-datenhaltung-bei-pods-h","title":"APP.4.4.A20 Verschl\u00fcsselte Datenhaltung bei Pods (H)","text":"<p>Compliant Kubernetes recommends disk encryption to be provided at the infrastructure level. If you have this requirement, check for full-disk encryption via the provider audit.</p>"},{"location":"ciso-guide/controls/gdpr/","title":"GDPR (Regulation (EU) 2016/679)","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p> <p>Note</p> <p>Fully implementing GDPR entails a lot of work, like:</p> <ul> <li>Assigning a DPO;</li> <li>Documenting Records of Processing Activities;</li> <li>Writing Privacy Policies;</li> <li>Signing Data Protection Agreements with your suppliers.</li> </ul> <p>This page only points you to the GDPR concerns relevant for Compliant Kubernetes.</p> <p>If you process personal data in the EU/EEA, you need to follow GDPR.</p>"},{"location":"ciso-guide/controls/gdpr/#gdpr-art-32-security-of-processing","title":"GDPR Art. 32 Security of Processing","text":"<p>When it comes to security, GDPR is rather broad and non-prescriptive. Pretty much everything we do in Compliant Kubernetes is done to secure data. This includes, for instance, that we perform vulnerability scanning both at rest and at runtime, process logs in a separate cluster controlled with restrictive access controls to make them tamper-proof from hacked applications, and that we put safeguards in place to make developers enforce network segregation per application component. And much more. In fact, we could pretty much link every single page to GDPR Art. 32, but that would be rather noisy!</p> <p>Hence, if you need a more precise understanding on how Compliant Kubernetes protects personal data as required by GDPR Art. 32, please look at our ISO 27001 Controls, which links to both more technical controls, and continuous confidentiality, integrity, availability and resilience of processing processes, such as our go-live checklist.</p>"},{"location":"ciso-guide/controls/gdpr/#gdpr-art-17-right-to-erasure-right-to-be-forgotten","title":"GDPR Art. 17 Right to erasure (\"right to be forgotten\")","text":"<ul> <li>Backups</li> <li>Long-term Log Retention</li> </ul>"},{"location":"ciso-guide/controls/gdpr/#gdpr-art-28-processor","title":"GDPR Art. 28 Processor","text":"<ul> <li>Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/gdpr/#further-reading","title":"Further reading","text":"<ul> <li>What is personal data?</li> <li>Art. 28 GDPR Processor</li> <li>Art. 17 GDPR Right to erasure (\"right to be forgotten\")</li> <li>Art. 32 GDPR Security of processing</li> </ul>"},{"location":"ciso-guide/controls/hipaa/","title":"HIPAA Controls","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p> <p>Click on the links below to navigate the documentation by control.</p>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s5-security-management-process-information-system-activity-review-164308a1iid","title":"HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","text":"<ul> <li>Log Review</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s12-information-access-management-isolating-healthcare-clearinghouse-functions-164308a4iia","title":"HIPAA S12 - Information Access Management - Isolating Healthcare Clearinghouse Functions - \u00a7 164.308(a)(4)(ii)(A)","text":"<ul> <li>How many environments?</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s13-information-access-management-access-authorization-164308a4iib","title":"HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","text":"<ul> <li>Access Control</li> <li>How to Delegate</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s14-information-access-management-access-establishment-and-modification-164308a4iic","title":"HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","text":"<ul> <li>Access Control</li> <li>How to Delegate</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s16-security-awareness-and-training-security-reminders-164308a5iia","title":"HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","text":"<ul> <li>Vulnerability Dashboard</li> <li>Maintenance</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s17-security-awareness-training-and-tools-protection-from-malicious-software-164308a5iib","title":"HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","text":"<ul> <li>Vulnerability Dashboard</li> <li>Container registry</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s18-security-awareness-training-and-tools-log-in-monitoring-164308a5iic","title":"HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","text":"<ul> <li>Audit Logs</li> <li>Log Review</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s20-security-incident-procedures-164308a6","title":"HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","text":"<ul> <li>Intrusion Detection Dashboard</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s23-contingency-plan-data-backup-plan-164308a7iia","title":"HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","text":"<ul> <li>Backup Dashboard</li> <li>Backups</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s24-contingency-plan-disaster-recovery-plan-164308a7iib","title":"HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","text":"<ul> <li>Disaster Recovery</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s26-contingency-plan-testing-and-revision-procedure-164308a7iid","title":"HIPAA S26 - Contingency Plan - Testing and Revision Procedure - \u00a7 164.308(a)(7)(ii)(D)","text":"<ul> <li>Go-live Checklist</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s29-business-associate-contracts-and-other-arrangements-164308b1","title":"HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","text":"<ul> <li>Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s31-facility-access-controls-164310a1","title":"HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","text":"<ul> <li>Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s32-facility-access-controls-contingency-operations-164310a2i","title":"HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","text":"<ul> <li>Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s33-facility-access-controls-facility-security-plan-164310a2ii","title":"HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","text":"<ul> <li>Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s34-facility-access-controls-access-control-and-validation-procedures-164310a2iii","title":"HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","text":"<ul> <li>Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s35-facility-access-controls-maintain-maintenance-records-164310a2iv","title":"HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","text":"<ul> <li>Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s39-device-and-media-controls-disposal-164310d2i","title":"HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","text":"<ul> <li>Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s43-access-control-164312a1","title":"HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","text":"<ul> <li>Access Control</li> <li>How to Delegate</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s44-access-control-unique-user-identification-164312a2i","title":"HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","text":"<ul> <li>Access Control</li> <li>Use of Credentials</li> <li>CI/CD</li> <li>How to Delegate</li> <li>Kubernetes API</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s45-access-control-emergency-access-procedure-164312a2ii","title":"HIPAA S45 - Access Control - Emergency Access Procedure - \u00a7 164.312(a)(2)(ii)","text":"<ul> <li>Breaking the Glass</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s47-access-control-encryption-and-decryption-164312a2iv","title":"HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","text":"<ul> <li>Use of Cryptography</li> <li>Provider Audit</li> <li>FAQ</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s48-audit-controls-164312b","title":"HIPAA S48 - Audit Controls - \u00a7 164.312(b)","text":"<ul> <li>Audit Logs</li> <li>Long-term Log Retention</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s52-transmission-164312e1","title":"HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s53-transmission-security-integrity-controls-164312e2i","title":"HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s54-transmission-security-encryption-164312e2ii","title":"HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#other-hipaa-controls","title":"Other HIPAA Controls","text":"<p>HIPAA controls are taken from these documents:</p> <ul> <li>HIPAA Security Series - Security Standards: Administrative Safeguards</li> <li>HIPAA Security Series - Security Standards: Physical Safeguards</li> <li>HIPAA Security Series - Security Standards: Technical Safeguards</li> </ul> <p>The following controls are outside the scope of Compliant Kubernetes and need to be implemented by the organization operating Compliant Kubernetes. ISO-27001-certified Compliant Kubernetes operators, such as Elastisys already have the right processes in place.</p> <ul> <li>S1 - Security Management Process - \u00a7 164.308(a)(1)</li> <li>S2 - Security Management Process - Risk Analysis - \u00a7 164.308(a)(1)(ii)(A)</li> <li>S3 - Security Management Process - Risk Management - \u00a7 164.308(a)(1)(ii)(B)</li> <li>S4 - Security Management Process - Sanction Policy - \u00a7 164.308(a)(1)(ii)(C)</li> <li>S6 - Assigned Security Responsibility - \u00a7 164.308(a)(2)</li> <li>S7 - Workforce Security - \u00a7 164.308(a)(3)</li> <li>S8 - Workforce security - Authorization and/or Supervision - \u00a7 164.308(a)(3)(ii)(A)</li> <li>S9 - Workforce security - Workforce Clearance Procedure - \u00a7 164.308(a)(3)(ii)(B)</li> <li>S10 - Workforce security - Establish Termination Procedures - \u00a7 164.308(a)(3)(ii)(C)</li> <li>S11 - Information Access Management - \u00a7 164.308(a)(4)</li> <li>S15 - Security Awareness and Training - \u00a7 164.308(a)(5)</li> <li>S19 - Security Awareness, Training, and Tools - Password Management - \u00a7 164.308(a)(5)(ii)(D)</li> <li>S21 - Security Incident Procedures - Response and Reporting - \u00a7 164.308(a)(6)</li> <li>S22 - Contingency Plan - \u00a7 164.308(a)(7)</li> <li>S25 - Contingency Plan - Emergency Mode Operation Plan - \u00a7 164.308(a)(7)(ii)(C)</li> <li>S27 - Contingency Plan - Application and Data Criticality Analysis - \u00a7 164.308(a)(7)(ii)(E)</li> <li>S28 - Evaluation - \u00a7 164.308(a)(8)</li> <li>S30 - Business Associate Contracts and Other Arrangements - Written Contract or Other Arrangement - \u00a7 164.308(b)(4)</li> <li>S36 - Workstation Use - \u00a7 164.310(b)</li> <li>S37 - Workstation Security - \u00a7 164.310(c)</li> <li>S38 - Device and Media Controls - \u00a7 164.310(d)(1)</li> <li>S40 - Device and Media Controls - Media Re-use - \u00a7 164.310(d)(2)(ii)</li> <li>S41 - Device and Media Controls - Accountability - \u00a7 164.310(d)(2)(iii)</li> <li>S42 - Device and Media Controls - Data Backup and Storage Procedures - \u00a7 164.310(d)(2)(iv)</li> <li> <p>S46 - Access Control - Automatic Logoff - \u00a7 164.312(a)(2)(iii)</p> <p>Important</p> <p>Compliant Kubernetes API access is configured so as to require a new OpenID flow every 12 hours.</p> </li> <li> <p>S49 - Integrity - \u00a7 164.312(c)(1)</p> </li> <li>S50 - Integrity - Mechanism to Authenticate ePHI - \u00a7 164.312(c)(2)</li> <li>S51 - Person or Entity Authentication - \u00a7 164.312(d)</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/","title":"Swedish Patient Data Act (HSLF-FS 2016:40)","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p> <p>Note</p> <p>Controls not covered below are controls which cannot be fulfilled by Compliant Kubernetes. These include requirements such as:</p> <ul> <li>Your management team needs to regularly perform various risk analysis.</li> <li>You need to do background checks when recruiting.</li> <li>You need to activate multi-factor authentication in your Identity Provider.</li> <li>You need to have a policy on how to safely use USB sticks.</li> <li>Requirements which fall under the scope of the application.</li> </ul> <p>If you are a Swedish healthcare provider, you likely process patient data. Patient data includes GDPR personal data and patient records. HSLF-FS 2016:40 recommends following ISO 27001.</p> <p>Please look at the ISO 27001 controls to understand how Compliant Kubernetes helps you keep patient data private and secure.</p>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-3-kap-9-upphandling-och-utveckling","title":"HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","text":"<ul> <li>Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-3-kap-10-upphandling-och-utveckling","title":"HSLF-FS 2016:40 3 kap. 10 \u00a7 Upphandling och utveckling","text":"<ul> <li>How many environments?</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-3-kap-12-sakerhetskopiering","title":"HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","text":"<ul> <li>Backup Dashboard</li> <li>Backups</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-3-kap-13-sakerhetskopiering","title":"HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","text":"<ul> <li>Disaster Recovery</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-3-kap-14-fysiskt-skydd-av-informationssystem","title":"HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","text":"<ul> <li>Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-3-kap-15-behandling-av-personuppgifter-i-oppna-nat","title":"HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","text":"<ul> <li>Cryptography Dashboard</li> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-4-kap-2-styrning-av-behorigheter","title":"HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","text":"<ul> <li>Use of Credentials</li> <li>Kubernetes API</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-4-kap-3-styrning-av-behorigheter","title":"HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","text":"<ul> <li>Access Control</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-4-kap-9-kontroll-av-atkomst-till-uppgifter","title":"HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","text":"<ul> <li>Audit Logs</li> <li>Log Review</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#further-reading","title":"Further reading","text":"<ul> <li>HSLF-FS 2016:40 Socialstyrelsens f\u00f6reskrifter och allm\u00e4nna r\u00e5d om journalf\u00f6ring och behandling av personuppgifter i h\u00e4lso- och sjukv\u00e5rden</li> <li>IMY Care providers' protection of patient information</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/","title":"ISO 27001 Controls","text":"<p>Note</p> <p>Controls not covered below are controls which cannot be fulfilled by Compliant Kubernetes. These include requirements such as:</p> <ul> <li>Your management team needs to regularly perform various risk analysis.</li> <li>You need to do background checks when recruiting.</li> <li>You need to activate multi-factor authentication in your Identity Provider.</li> <li>You need to have a policy on how to safely use USB sticks.</li> <li>Requirements which fall under the scope of the application.</li> </ul> <p>Click on the links below to navigate the documentation by control.</p>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a941-information-access-restriction","title":"ISO 27001 A.9.4.1 Information Access Restriction","text":"<ul> <li>How to Delegate</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a944-use-of-privileged-utility-programmes","title":"ISO 27001 A.9.4.4 Use of Privileged Utility Programmes","text":"<ul> <li>Enforce No Root</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a10-cryptography","title":"ISO 27001 A.10 Cryptography","text":"<ul> <li>Use of Cryptography</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1012-key-management","title":"ISO 27001 A.10.1.2 Key Management","text":"<ul> <li>Cryptography Dashboard</li> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1212-change-management","title":"ISO 27001 A.12.1.2 Change Management","text":"<ul> <li>Enforce No Latest Tag</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1213-capacity-management","title":"ISO 27001 A.12.1.3 Capacity Management","text":"<ul> <li>Capacity Management (Kubernetes Status) Dashboard</li> <li>Capacity Management</li> <li>Metrics</li> <li>Enforce Resources</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1214-separation-of-development-testing-operational-environments","title":"ISO 27001 A.12.1.4 Separation of Development, Testing &amp; Operational Environments","text":"<ul> <li>How many environments?</li> <li>Namespaces</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1221-controls-against-malware","title":"ISO 27001 A.12.2.1 Controls Against Malware","text":"<ul> <li>Intrusion Detection Dashboard</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1231-information-backup","title":"ISO 27001 A.12.3.1 Information Backup","text":"<ul> <li>Backup Dashboard</li> <li>Disaster Recovery</li> <li>Backups</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1241-event-logging","title":"ISO 27001 A.12.4.1 Event Logging","text":"<ul> <li>Log Review</li> <li>Logs</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1243-administrator-operator-logs","title":"ISO 27001 A.12.4.3 Administrator &amp; Operator Logs","text":"<ul> <li>Audit Logs</li> <li>Log Review</li> <li>Logs</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1244-clock-synchronization","title":"ISO 27001 A.12.4.4 Clock Synchronization","text":"<ul> <li>Clock Synchronization</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1261-management-of-technical-vulnerabilities","title":"ISO 27001 A.12.6.1 Management of Technical Vulnerabilities","text":"<ul> <li>Intrusion Detection Dashboard</li> <li>Vulnerability Dashboard</li> <li>Overview</li> <li>Maintenance</li> <li>Prepare Your Application</li> <li>Container registry</li> <li>Enforce No Root</li> <li>Enforce Trusted Registries</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a13-network-security","title":"ISO 27001 A.13 Network Security","text":"<ul> <li>Network Security Dashboard</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a131-network-security","title":"ISO 27001 A.13.1 Network Security","text":"<ul> <li>Network Model</li> <li>Enforce NetworkPolicies</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1411-information-security-requirements-analysis-specification","title":"ISO 27001 A.14.1.1 Information Security Requirements Analysis &amp; Specification","text":"<ul> <li>Architectural Decision Log</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1422-system-change-control-procedures","title":"ISO 27001 A.14.2.2 System Change Control Procedures","text":"<ul> <li>Enforce No Latest Tag</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1424-restrictions-on-changes-to-software-packages","title":"ISO 27001 A.14.2.4 Restrictions on Changes to Software Packages","text":"<ul> <li>Architectural Decision Log</li> <li>Enforce No Latest Tag</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1425-secure-system-engineering-principles","title":"ISO 27001 A.14.2.5 Secure System Engineering Principles","text":"<ul> <li>Namespaces</li> <li>Enforce No Root</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1429-system-acceptance-testing","title":"ISO 27001 A.14.2.9 System Acceptance Testing","text":"<ul> <li>QA</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a15-supplier-relationships","title":"ISO 27001 A.15 Supplier Relationships","text":"<ul> <li>Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a16-information-security-incident-management","title":"ISO 27001 A.16 Information Security Incident Management","text":"<ul> <li>Intrusion Detection Dashboard</li> <li>Metric Alerts</li> <li>Log-based Alerts</li> <li>Logs</li> <li>Metrics</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1711-planning-information-security-continuity","title":"ISO 27001 A.17.1.1 Planning Information Security Continuity","text":"<ul> <li>We believe in community-driven open source</li> <li>Backup Dashboard</li> <li>Disaster Recovery</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1713-verify-review-evaluate-information-security-continuity","title":"ISO 27001 A.17.1.3 Verify, Review &amp; Evaluate Information Security Continuity","text":"<ul> <li>Go-live Checklist</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1812-intellectual-property-rights","title":"ISO 27001 A.18.1.2 Intellectual Property Rights","text":"<ul> <li>FAQ</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1822-compliance-with-security-policies-standards","title":"ISO 27001 A.18.2.2 Compliance with Security Policies &amp; Standards","text":"<ul> <li>Policy-as-Code Dashboard</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-a1823-technical-compliance-review","title":"ISO 27001 A.18.2.3 Technical Compliance Review","text":"<ul> <li>Policy-as-Code Dashboard</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/","title":"MSBFS 2020:7 Controls","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p> <p>Note</p> <p>Controls not covered below are controls which cannot be fulfilled by Compliant Kubernetes. These include requirements such as:</p> <ul> <li>Your management team needs to regularly perform various risk analysis.</li> <li>You need to do background checks when recruiting.</li> <li>You need to activate multi-factor authentication in your Identity Provider.</li> <li>You need to have a policy on how to safely use USB sticks.</li> <li>Requirements which fall under the scope of the application.</li> </ul> <p>If you are a Swedish government agency or a supplier you likely need to comply with MSBFS 2020:7.</p> <p>Click on the links below to navigate the documentation by control.</p>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-2-kap-4","title":"MSBFS 2020:7 2 kap. 4 \u00a7","text":"<ul> <li>Architecture</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-3-kap-1","title":"MSBFS 2020:7 3 kap. 1 \u00a7","text":"<ul> <li>Provider Audit</li> <li>How many environments?</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-3-kap-2","title":"MSBFS 2020:7 3 kap. 2 \u00a7","text":"<ul> <li>Provider Audit</li> <li>How many environments?</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-1","title":"MSBFS 2020:7 4 kap. 1 \u00a7","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-2","title":"MSBFS 2020:7 4 kap. 2 \u00a7","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-3","title":"MSBFS 2020:7 4 kap. 3 \u00a7","text":"<ul> <li>Access Control</li> <li>How to Delegate</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-4","title":"MSBFS 2020:7 4 kap. 4 \u00a7","text":"<ul> <li>Access Control</li> <li>How to Delegate</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-7","title":"MSBFS 2020:7 4 kap. 7 \u00a7","text":"<ul> <li>FAQ</li> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-9","title":"MSBFS 2020:7 4 kap. 9 \u00a7","text":"<ul> <li>Use of Cryptography</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-12","title":"MSBFS 2020:7 4 kap. 12 \u00a7","text":"<ul> <li>Maintenance</li> <li>Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-13","title":"MSBFS 2020:7 4 kap. 13 \u00a7","text":"<ul> <li>Clock Synchronization</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-14","title":"MSBFS 2020:7 4 kap. 14 \u00a7","text":"<ul> <li>Backup Dashboard</li> <li>Disaster Recovery</li> <li>Backups</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-15","title":"MSBFS 2020:7 4 kap. 15 \u00a7","text":"<ul> <li>Backup Dashboard</li> <li>Disaster Recovery</li> <li>Backups</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-16","title":"MSBFS 2020:7 4 kap. 16 \u00a7","text":"<ul> <li>Audit Logs</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-17","title":"MSBFS 2020:7 4 kap. 17 \u00a7","text":"<ul> <li>Log Review</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-18","title":"MSBFS 2020:7 4 kap. 18 \u00a7","text":"<ul> <li>Intrusion Detection Dashboard</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-20","title":"MSBFS 2020:7 4 kap. 20 \u00a7","text":"<ul> <li>Vulnerability Dashboard</li> <li>Container registry</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-21","title":"MSBFS 2020:7 4 kap. 21 \u00a7","text":"<ul> <li>Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-22","title":"MSBFS 2020:7 4 kap. 22 \u00a7","text":"<ul> <li>Disaster Recovery</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#further-reading","title":"Further Reading","text":"<ul> <li>Myndigheten f\u00f6r samh\u00e4llsskydd och beredskaps f\u00f6reskrifter om s\u00e4kerhets\u00e5tg\u00e4rder i informationssystem f\u00f6r statliga myndigheter</li> </ul>"}]}